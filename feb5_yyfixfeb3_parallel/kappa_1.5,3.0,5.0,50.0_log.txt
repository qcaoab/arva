Starting at: 
06-02-23_11:14

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 50000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 5000, 'itbound_SGD_algorithms': 50000, 'nit_IterateAveragingStart': 45000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1514.779448990284
Current xi:  [97.67873]
objective value function right now is: -1514.779448990284
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1542.6051315014568
Current xi:  [98.94706]
objective value function right now is: -1542.6051315014568
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1542.9663043085884
Current xi:  [103.45147]
objective value function right now is: -1542.9663043085884
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [106.96053]
objective value function right now is: -1540.2825450221362
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1546.0086966620736
Current xi:  [109.791]
objective value function right now is: -1546.0086966620736
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.755035]
objective value function right now is: -1545.0363087511985
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [111.97609]
objective value function right now is: -1543.2368904440702
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1549.548669878029
Current xi:  [112.23094]
objective value function right now is: -1549.548669878029
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [113.33058]
objective value function right now is: -1545.987920880897
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [114.22364]
objective value function right now is: -1548.4439663556843
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1550.250939895231
Current xi:  [114.73178]
objective value function right now is: -1550.250939895231
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.64896]
objective value function right now is: -1547.8231301802384
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [101.86522]
objective value function right now is: -1502.3853707241417
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [100.45179]
objective value function right now is: -1543.175446904666
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [104.81696]
objective value function right now is: -1546.1582791750802
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [108.01647]
objective value function right now is: -1541.3863654506804
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.404335]
objective value function right now is: -1544.808818802824
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [111.90346]
objective value function right now is: -1548.7720857697564
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [114.30136]
objective value function right now is: -1547.7463764082916
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1551.0306437533116
Current xi:  [115.60657]
objective value function right now is: -1551.0306437533116
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.66512]
objective value function right now is: -1549.2483038654486
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.5673]
objective value function right now is: -1547.1826284654053
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.5450233967267
Current xi:  [117.50647]
objective value function right now is: -1552.5450233967267
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.38002]
objective value function right now is: -1547.915901406779
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.873314]
objective value function right now is: -1551.9102481566254
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.603905]
objective value function right now is: -1547.3022098670285
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1553.2420800304694
Current xi:  [117.190605]
objective value function right now is: -1553.2420800304694
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [117.55598]
objective value function right now is: -1548.166302940155
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [118.27374]
objective value function right now is: -1552.0683854717824
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.309204]
objective value function right now is: -1550.6001086791396
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.97407]
objective value function right now is: -1548.9508160884666
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.5889]
objective value function right now is: -1550.6352607815788
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.847046]
objective value function right now is: -1548.520319252408
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.53655]
objective value function right now is: -1549.6779161832249
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.79822]
objective value function right now is: -1552.061326963512
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.6849236637079
Current xi:  [117.73072]
objective value function right now is: -1554.6849236637079
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.43282]
objective value function right now is: -1554.5698176482892
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1554.937872445111
Current xi:  [117.463165]
objective value function right now is: -1554.937872445111
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.95499]
objective value function right now is: -1554.874837632855
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.0492508787338
Current xi:  [117.63354]
objective value function right now is: -1555.0492508787338
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.4299271638117
Current xi:  [117.77922]
objective value function right now is: -1555.4299271638117
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.57641]
objective value function right now is: -1554.6480159493321
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.235085]
objective value function right now is: -1555.257315653218
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.96233]
objective value function right now is: -1555.2297987559223
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.13717]
objective value function right now is: -1555.3451768964944
new min fval from sgd:  -1555.5018452217057
new min fval from sgd:  -1555.5105118521556
new min fval from sgd:  -1555.5736875369435
new min fval from sgd:  -1555.5917733679757
new min fval from sgd:  -1555.6275081720253
new min fval from sgd:  -1555.6851720714326
new min fval from sgd:  -1555.739105547426
new min fval from sgd:  -1555.7548865279616
new min fval from sgd:  -1555.758648039498
new min fval from sgd:  -1555.772844737184
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.27574]
objective value function right now is: -1554.9702220937345
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [117.14115]
objective value function right now is: -1552.3208465974178
new min fval from sgd:  -1555.7764898354123
new min fval from sgd:  -1555.7808396882604
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.787056]
objective value function right now is: -1555.011534704266
new min fval from sgd:  -1555.7824365644847
new min fval from sgd:  -1555.7925929981413
new min fval from sgd:  -1555.7950396495366
new min fval from sgd:  -1555.8007706206765
new min fval from sgd:  -1555.8114906790063
new min fval from sgd:  -1555.8166512093037
new min fval from sgd:  -1555.8321159461718
new min fval from sgd:  -1555.840868783276
new min fval from sgd:  -1555.8500753742692
new min fval from sgd:  -1555.8611887108543
new min fval from sgd:  -1555.8772357171442
new min fval from sgd:  -1555.8892417463453
new min fval from sgd:  -1555.8916867378884
new min fval from sgd:  -1555.8961108853061
new min fval from sgd:  -1555.8983673308849
new min fval from sgd:  -1555.9026328465418
new min fval from sgd:  -1555.9051635416502
new min fval from sgd:  -1555.913590943316
new min fval from sgd:  -1555.9176473213308
new min fval from sgd:  -1555.9201873121349
new min fval from sgd:  -1555.9229888667496
new min fval from sgd:  -1555.9244828910103
new min fval from sgd:  -1555.933745275068
new min fval from sgd:  -1555.943657141157
new min fval from sgd:  -1555.955391384411
new min fval from sgd:  -1555.9680672991444
new min fval from sgd:  -1555.9739001849925
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.97287]
objective value function right now is: -1555.750821047638
new min fval from sgd:  -1555.9844739870384
new min fval from sgd:  -1555.9960874889293
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [116.993675]
objective value function right now is: -1555.7824043864432
min fval:  -1555.9960874889293
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-43.8585,  -9.5178],
        [ -3.8818, -10.5659],
        [ -1.0550,   0.7146],
        [ -1.0550,   0.7146],
        [ 11.7937,  -0.8662],
        [ -1.0272,   0.7006],
        [-11.6943,   0.5175],
        [ -1.0550,   0.7146],
        [  6.8685,  -8.9523],
        [ 10.8996,  -3.0510]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-9.3809, -8.5067, -3.0417, -3.0417, -9.9223, -3.0542,  9.6205, -3.0417,
        -8.1929, -9.9192], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.3535, -0.0794, -0.0257, -0.0257, -0.7843, -0.0232, -0.6715, -0.0257,
          0.2922, -0.2031],
        [10.7147, 13.0219, -0.1457, -0.1458,  9.3650, -0.0692, -7.9569, -0.1457,
         14.2019,  7.5296],
        [-0.3530, -0.0785, -0.0257, -0.0257, -0.7831, -0.0233, -0.6736, -0.0257,
          0.2924, -0.2028],
        [12.3386, 13.4658,  0.1613,  0.1613,  7.5661,  0.0550, -8.4245,  0.1612,
         13.3908,  4.9916],
        [-0.3535, -0.0794, -0.0257, -0.0257, -0.7843, -0.0232, -0.6715, -0.0257,
          0.2922, -0.2031],
        [-0.3535, -0.0794, -0.0257, -0.0257, -0.7843, -0.0232, -0.6715, -0.0257,
          0.2922, -0.2031],
        [-0.3535, -0.0794, -0.0257, -0.0257, -0.7843, -0.0232, -0.6715, -0.0257,
          0.2922, -0.2031],
        [-0.3535, -0.0794, -0.0257, -0.0257, -0.7843, -0.0232, -0.6715, -0.0257,
          0.2922, -0.2031],
        [ 4.2080,  8.9946,  0.4849,  0.4849,  1.0343,  0.4861, -6.9814,  0.4849,
         10.2168, -0.1775],
        [-0.3535, -0.0794, -0.0257, -0.0257, -0.7843, -0.0232, -0.6715, -0.0257,
          0.2922, -0.2031]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.5231, -4.6142, -1.5274, -4.5547, -1.5231, -1.5231, -1.5231, -1.5231,
        -2.1432, -1.5231], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.5037, -10.7837,  -0.5033, -11.4946,  -0.5037,  -0.5037,  -0.5037,
          -0.5037,  -4.5835,  -0.5037]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 13.6600,   3.6831],
        [-14.0656, -10.3325],
        [  3.7557,  12.9551],
        [ -3.2848,  12.7585],
        [  1.5318,  -1.6061],
        [  9.2218,   4.0024],
        [ 13.7830,   0.0485],
        [-10.8082,   8.4917],
        [ -1.2541,   6.7622],
        [-14.1156,  -4.5331]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -1.1964,  -7.6544,  10.3214,   8.8152,   3.3703,  -4.1295, -11.6465,
          8.1591,  -5.1723,  -3.2545], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-5.6461e+00,  1.0122e+01, -9.6431e+00,  1.0011e-01, -9.7023e-01,
         -1.4750e-01, -5.2127e+00, -5.2267e-01, -5.5034e-03,  2.0114e+00],
        [-2.4933e+00, -4.9249e+00,  2.3890e+00, -6.2104e+00,  1.8661e+00,
         -1.4902e+00, -4.9744e+00, -7.3062e+00,  9.7691e-03, -8.3723e-01],
        [-1.0393e+00,  1.1341e+00, -4.2513e-01, -1.1686e+00, -2.4871e+00,
         -1.4267e+00, -1.5486e-01, -1.0902e+00, -1.6857e-01,  7.5399e-02],
        [-1.8488e+00,  1.0984e+00, -9.2028e-01, -5.3598e-01, -2.0150e+00,
         -9.2797e-01, -5.3068e-02, -5.1728e-01, -1.0394e-01, -3.9105e-02],
        [-3.6950e+00,  2.3199e+00,  2.1310e+00, -3.0729e+00,  2.3316e+00,
         -1.2892e+00,  4.1244e+00, -3.1316e+00,  1.2580e-01,  8.0162e+00],
        [-7.5593e+00,  1.0035e+01, -1.8951e+01, -1.3842e-01,  1.7788e+00,
          4.1420e-02, -6.3014e+00,  1.5550e+00, -4.6310e-03,  3.5275e+00],
        [-2.5017e+00,  8.1071e+00, -1.5129e+01, -1.1252e+01,  3.5719e+00,
         -5.7630e+00, -1.3333e+01, -3.2977e+00,  7.0089e-03,  6.4941e+00],
        [-1.1067e+00, -8.7890e+00, -1.0619e+00,  4.8890e+00,  4.3935e-01,
         -6.7376e+00, -1.1528e+01,  8.2850e+00,  5.1941e+00, -4.9004e+00],
        [-1.8161e+00,  1.1118e+00, -9.3036e-01, -5.3097e-01, -1.9610e+00,
         -9.1178e-01, -8.0054e-02, -5.1639e-01, -8.6176e-02, -3.3119e-02],
        [-9.3285e-01, -2.7678e+01,  7.4599e+00, -5.2766e+00,  3.8462e+00,
         -1.1470e+00,  4.4814e+00,  1.5865e+01, -2.4409e-02,  1.0028e+01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-6.9045, -5.1487, -2.9732, -3.4207, -5.9577, -9.0717, -4.0023, -3.4511,
        -3.4993, -4.4467], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  7.1483,   3.8779,   1.0022,   0.5060,   1.3949,  12.4415,  -8.7637,
           0.3810,   0.5012,   0.3357],
        [ -7.1912,  -3.8740,  -1.0021,  -0.5060,  -1.3609, -12.4015,   8.7545,
          -0.4394,  -0.5011,  -0.3833]], device='cuda:0'))])
xi:  [116.99283]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 508.7631454545288
W_T_median: 292.9649304942617
W_T_pctile_5: 117.00625211627712
W_T_CVAR_5_pct: -8.745197153340252
Average q (qsum/M+1):  50.61658108618952
Optimal xi:  [116.99283]
Observed VAR:  292.9649304942617
Expected(across Rb) median(across samples) p_equity:  0.2680995990832647
obj fun:  tensor(-1555.9961, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:158: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1405.9504120201739
Current xi:  [104.78939]
objective value function right now is: -1405.9504120201739
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1474.0954353009447
Current xi:  [110.36222]
objective value function right now is: -1474.0954353009447
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1497.660791296518
Current xi:  [120.62289]
objective value function right now is: -1497.660791296518
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1509.2250901179518
Current xi:  [134.09958]
objective value function right now is: -1509.2250901179518
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1515.398014846512
Current xi:  [142.88766]
objective value function right now is: -1515.398014846512
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1523.8905003947661
Current xi:  [149.41676]
objective value function right now is: -1523.8905003947661
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [152.28455]
objective value function right now is: -1510.3809684553808
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1526.1644635971995
Current xi:  [155.50009]
objective value function right now is: -1526.1644635971995
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.88457]
objective value function right now is: -1522.7978246999835
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [158.54555]
objective value function right now is: -1525.7435834834157
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [159.56346]
objective value function right now is: -1524.0646046046224
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [160.87357]
objective value function right now is: -1507.3437686872794
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1530.2661264736764
Current xi:  [160.47398]
objective value function right now is: -1530.2661264736764
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1541.6642320466747
Current xi:  [159.79283]
objective value function right now is: -1541.6642320466747
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1555.2938214723756
Current xi:  [161.6975]
objective value function right now is: -1555.2938214723756
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [162.99596]
objective value function right now is: -1554.013605332484
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.49992]
objective value function right now is: -1554.2952759564794
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1557.023897780493
Current xi:  [169.2935]
objective value function right now is: -1557.023897780493
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.99426]
objective value function right now is: -1549.5584166151903
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.47859]
objective value function right now is: -1556.784669822277
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.40146]
objective value function right now is: -1552.3429378347062
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.26779]
objective value function right now is: -1540.3503534289446
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [163.96898]
objective value function right now is: -1549.6228052799177
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [164.01443]
objective value function right now is: -1550.8036074608308
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.4266138720293
Current xi:  [165.50813]
objective value function right now is: -1560.4266138720293
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.16933]
objective value function right now is: -1547.5792001855066
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.57721]
objective value function right now is: -1546.898336184003
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [167.03171]
objective value function right now is: -1555.5398124828569
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1562.241033450468
Current xi:  [169.11943]
objective value function right now is: -1562.241033450468
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.93663]
objective value function right now is: -1555.5307380274367
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [167.26038]
objective value function right now is: -1557.7953839661996
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.07426]
objective value function right now is: -1554.7536851013606
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.48991]
objective value function right now is: -1552.6053219450403
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [166.1173]
objective value function right now is: -1559.706827340899
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.21414]
objective value function right now is: -1549.3258239383408
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.0656720675124
Current xi:  [167.82274]
objective value function right now is: -1564.0656720675124
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.351850617838
Current xi:  [167.83313]
objective value function right now is: -1564.351850617838
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.456980732298
Current xi:  [167.99727]
objective value function right now is: -1564.456980732298
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.65648]
objective value function right now is: -1564.0483566110008
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [168.96347]
objective value function right now is: -1563.2743079450076
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.29213]
objective value function right now is: -1564.3746935554682
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.7117351505258
Current xi:  [169.45622]
objective value function right now is: -1564.7117351505258
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.323]
objective value function right now is: -1563.4168718854228
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.7593778298005
Current xi:  [169.23239]
objective value function right now is: -1564.7593778298005
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.35797]
objective value function right now is: -1564.0129222133187
new min fval from sgd:  -1564.946876901668
new min fval from sgd:  -1565.0329420046874
new min fval from sgd:  -1565.043371249876
new min fval from sgd:  -1565.0541107759113
new min fval from sgd:  -1565.062691416262
new min fval from sgd:  -1565.0748009995832
new min fval from sgd:  -1565.15288919155
new min fval from sgd:  -1565.2023816357819
new min fval from sgd:  -1565.2038900418356
new min fval from sgd:  -1565.214155112261
new min fval from sgd:  -1565.3725952043656
new min fval from sgd:  -1565.4514783587776
new min fval from sgd:  -1565.5140727010091
new min fval from sgd:  -1565.5416732672709
new min fval from sgd:  -1565.615000673514
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [169.72684]
objective value function right now is: -1565.0765372480626
new min fval from sgd:  -1565.6499048487306
new min fval from sgd:  -1565.6931596677412
new min fval from sgd:  -1565.7132034243405
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.00722]
objective value function right now is: -1565.4523030292462
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.07861]
objective value function right now is: -1564.4950948813832
new min fval from sgd:  -1565.7259125492958
new min fval from sgd:  -1565.731987723514
new min fval from sgd:  -1565.7627949235437
new min fval from sgd:  -1565.7910908509136
new min fval from sgd:  -1565.8098076847393
new min fval from sgd:  -1565.8225086397201
new min fval from sgd:  -1565.8314924448425
new min fval from sgd:  -1565.8408095198486
new min fval from sgd:  -1565.8444658042192
new min fval from sgd:  -1565.848820762413
new min fval from sgd:  -1565.8491991329768
new min fval from sgd:  -1565.8510733824783
new min fval from sgd:  -1565.8528854979008
new min fval from sgd:  -1565.8607235729676
new min fval from sgd:  -1565.8771239727785
new min fval from sgd:  -1565.8861973134894
new min fval from sgd:  -1565.9138564417028
new min fval from sgd:  -1565.9311267721032
new min fval from sgd:  -1565.9377881955295
new min fval from sgd:  -1565.9399186900503
new min fval from sgd:  -1565.9404523071275
new min fval from sgd:  -1565.9417175783083
new min fval from sgd:  -1565.9461187302345
new min fval from sgd:  -1565.946804149882
new min fval from sgd:  -1565.9497817774773
new min fval from sgd:  -1565.968169330059
new min fval from sgd:  -1565.9841561520072
new min fval from sgd:  -1565.9987613794904
new min fval from sgd:  -1566.0033198064004
new min fval from sgd:  -1566.0055690898148
new min fval from sgd:  -1566.0073643255903
new min fval from sgd:  -1566.0346261398872
new min fval from sgd:  -1566.0422847000466
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.67006]
objective value function right now is: -1565.8456264916342
new min fval from sgd:  -1566.0463244683506
new min fval from sgd:  -1566.0526786531573
new min fval from sgd:  -1566.057392060527
new min fval from sgd:  -1566.0590045591857
new min fval from sgd:  -1566.061749363672
new min fval from sgd:  -1566.0643344294208
new min fval from sgd:  -1566.0658499832116
new min fval from sgd:  -1566.067071332194
new min fval from sgd:  -1566.0686022408265
new min fval from sgd:  -1566.0699876501328
new min fval from sgd:  -1566.0734160727452
new min fval from sgd:  -1566.076274805889
new min fval from sgd:  -1566.0800319825448
new min fval from sgd:  -1566.0806747897068
new min fval from sgd:  -1566.1112772652134
new min fval from sgd:  -1566.1345428115815
new min fval from sgd:  -1566.1528042444313
new min fval from sgd:  -1566.1678793853573
new min fval from sgd:  -1566.1722731563752
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.82025]
objective value function right now is: -1565.8975738645945
min fval:  -1566.1722731563752
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ 12.6126,  -0.3209],
        [ -3.9501, -12.4718],
        [ 10.7086,  -8.7911],
        [  7.6255, -10.6805],
        [-37.6352, -11.0954],
        [ -1.1721,   0.4429],
        [ -1.1797,   0.4434],
        [  3.4173,  -0.8429],
        [ 13.8942,  -0.8953],
        [ 11.8873,  -3.7455]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-11.1191,  -9.0319,  -7.9533,  -8.9389, -10.0557,  -3.2347,  -3.2431,
         -7.2984, -10.8318,  -8.7884], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.9510e-01, -3.6452e-01, -5.7696e-01, -4.1456e-01,  8.3439e-03,
          5.2052e-03,  5.2417e-03,  2.3371e-03, -3.6963e-01, -2.5100e-01],
        [-1.9510e-01, -3.6452e-01, -5.7696e-01, -4.1456e-01,  8.3439e-03,
          5.2052e-03,  5.2417e-03,  2.3371e-03, -3.6963e-01, -2.5100e-01],
        [ 7.2613e+00,  1.0666e+01,  5.9737e+00,  7.0703e+00,  1.1514e+01,
          1.0785e-01,  1.1538e-01,  1.6191e-01,  1.1045e+01,  4.3603e+00],
        [-1.9510e-01, -3.6452e-01, -5.7696e-01, -4.1456e-01,  8.3439e-03,
          5.2052e-03,  5.2417e-03,  2.3371e-03, -3.6963e-01, -2.5100e-01],
        [-1.9510e-01, -3.6452e-01, -5.7696e-01, -4.1456e-01,  8.3439e-03,
          5.2052e-03,  5.2417e-03,  2.3371e-03, -3.6963e-01, -2.5100e-01],
        [ 7.0371e+00,  1.0722e+01,  7.9933e+00,  6.7065e+00,  7.5607e+00,
         -1.3557e-01, -1.4256e-01,  4.3721e-01,  1.1552e+01,  5.7043e+00],
        [-1.9510e-01, -3.6452e-01, -5.7696e-01, -4.1456e-01,  8.3439e-03,
          5.2052e-03,  5.2417e-03,  2.3371e-03, -3.6963e-01, -2.5100e-01],
        [ 3.0307e-01,  9.0174e+00,  4.5606e+00,  6.1137e+00,  7.7921e+00,
         -1.8835e-02, -1.9262e-02, -7.6275e-02,  1.8917e+00,  1.7731e+00],
        [-1.9510e-01, -3.6452e-01, -5.7696e-01, -4.1456e-01,  8.3439e-03,
          5.2052e-03,  5.2417e-03,  2.3371e-03, -3.6963e-01, -2.5100e-01],
        [-1.9510e-01, -3.6452e-01, -5.7696e-01, -4.1456e-01,  8.3439e-03,
          5.2052e-03,  5.2417e-03,  2.3371e-03, -3.6963e-01, -2.5100e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ -1.9469,  -1.9469, -13.5458,  -1.9469,  -1.9469, -12.9282,  -1.9469,
        -11.4029,  -1.9469,  -1.9469], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0295,   0.0295, -12.2648,   0.0295,   0.0295,  -9.2563,   0.0295,
          -6.4111,   0.0295,   0.0295]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-27.7783,  -1.7519],
        [-26.8147, -17.2787],
        [  1.3224,   4.3199],
        [  0.5674,  15.5531],
        [-15.3391,  -7.7757],
        [  6.6043, -14.3885],
        [ 12.3542,  -0.9094],
        [ 14.2887,   5.5388],
        [  2.6823,  11.3007],
        [-21.2597,   1.1710]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([  0.1440, -14.9830,  -3.7285,  11.5675,  -3.3130,  -8.6514, -13.0888,
         -3.2818,   8.2676,  11.7765], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.5486e+00,  1.2412e+01,  6.5329e-02, -8.0437e+00,  4.0040e+00,
         -7.1394e+00, -1.1630e+01, -1.2369e+00, -1.7347e+01,  9.3581e+00],
        [ 3.1929e-02, -4.0677e-01, -1.3636e-01, -1.3248e+00, -4.1884e-01,
         -2.7575e+00, -4.5239e-01, -1.4791e+00, -1.2136e+00, -5.4532e-01],
        [ 3.8780e+00, -1.3165e+01, -1.3557e+00, -2.8350e-01,  2.8002e+00,
         -3.4116e+00, -2.1563e+00, -1.2218e+00,  2.2167e+00,  4.0628e+00],
        [-4.3561e-02, -1.3638e+00,  9.8545e-01, -9.0407e-02,  1.2473e+00,
         -2.6869e+00, -2.8190e+00, -4.2992e+00, -4.6647e-01, -2.7510e+00],
        [-4.0140e-02,  2.4926e-02, -1.0281e-01, -8.3498e-01, -1.2094e-01,
         -2.6349e+00, -2.2584e-01, -1.7088e+00, -1.0956e+00, -4.6881e-01],
        [ 6.5692e+00, -4.1496e+00,  2.6231e-02, -2.9351e+01,  1.1092e+01,
          7.1279e-01, -1.1842e+01, -4.8602e+00, -1.5376e+01, -1.3232e+00],
        [-1.4187e+00,  3.7513e+00, -1.2232e+00, -5.0124e+00,  4.0526e+00,
         -1.7907e+00, -7.5426e+00,  2.6014e+00,  3.0824e+00, -3.7394e+00],
        [ 2.8772e-02, -3.8105e-01, -1.3117e-01, -1.3034e+00, -4.0245e-01,
         -2.7615e+00, -4.4442e-01, -1.4866e+00, -1.2082e+00, -5.3240e-01],
        [ 3.3130e-02, -4.1755e-01, -1.3962e-01, -1.3342e+00, -4.2542e-01,
         -2.7581e+00, -4.5595e-01, -1.4759e+00, -1.2173e+00, -5.5080e-01],
        [ 6.0332e+00, -1.7380e+01,  2.3267e-01,  2.7262e+01, -9.3743e+00,
         -4.2716e+00,  1.7054e+00,  2.5346e+00,  4.2395e+00,  2.6169e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-8.6941, -3.9667, -4.0079, -4.4471, -4.0368, -3.0633, -4.5134, -3.9518,
        -3.9703,  2.0299], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 16.3238,  -0.0283,   0.4232,  -0.8565,   0.0370,  -6.8187,  -2.8284,
          -0.0235,  -0.0324,   0.5312],
        [-16.3386,   0.0297,  -0.4043,   0.8092,  -0.0373,   7.1053,   2.8037,
           0.0219,   0.0312,  -0.3687]], device='cuda:0'))])
xi:  [170.72917]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 610.0200275791095
W_T_median: 382.1781240631517
W_T_pctile_5: 170.75881888970133
W_T_CVAR_5_pct: 16.16457271875391
Average q (qsum/M+1):  48.95735808341734
Optimal xi:  [170.72917]
Observed VAR:  382.1781240631517
Expected(across Rb) median(across samples) p_equity:  0.2617484852671623
obj fun:  tensor(-1566.1723, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:196: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:158: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -981.821292310687
Current xi:  [105.49276]
objective value function right now is: -981.821292310687
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1511.8874168115615
Current xi:  [121.75009]
objective value function right now is: -1511.8874168115615
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1539.6584759247787
Current xi:  [141.53386]
objective value function right now is: -1539.6584759247787
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1587.2486023731478
Current xi:  [159.34258]
objective value function right now is: -1587.2486023731478
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [173.26257]
objective value function right now is: -1570.50547230811
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.27937]
objective value function right now is: -1576.8197149817604
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [185.23653]
objective value function right now is: -1579.7299964241022
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.65486]
objective value function right now is: -1584.7543299642675
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [178.45297]
objective value function right now is: -1478.349572635721
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.01299]
objective value function right now is: -1555.6969094489218
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [181.56636]
objective value function right now is: -1583.2228716598765
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.74243]
objective value function right now is: -1583.7777324375927
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1589.7483520058613
Current xi:  [184.33086]
objective value function right now is: -1589.7483520058613
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [183.60559]
objective value function right now is: -1587.8559732733186
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1590.1969437911353
Current xi:  [184.09518]
objective value function right now is: -1590.1969437911353
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1596.9466976650838
Current xi:  [186.77295]
objective value function right now is: -1596.9466976650838
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.97697]
objective value function right now is: -1560.1697274857975
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.83798]
objective value function right now is: -1594.1407121614097
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.791]
objective value function right now is: -1589.8389202721726
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1596.9657138126759
Current xi:  [185.18909]
objective value function right now is: -1596.9657138126759
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.70728]
objective value function right now is: -1583.294908763393
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.38438]
objective value function right now is: -1564.826014126645
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1597.5806848436614
Current xi:  [186.03088]
objective value function right now is: -1597.5806848436614
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.54066]
objective value function right now is: -1580.0765871112537
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.07306]
objective value function right now is: -1587.8312249462704
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.28256]
objective value function right now is: -1589.740165456837
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.75381]
objective value function right now is: -1585.2394713699032
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [186.48126]
objective value function right now is: -1581.8163363498124
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [184.9745]
objective value function right now is: -1588.6755520022623
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.943288047296
Current xi:  [186.42941]
objective value function right now is: -1601.943288047296
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.70651]
objective value function right now is: -1509.752655029055
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.39]
objective value function right now is: -1580.6937014632956
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [185.86388]
objective value function right now is: -1588.7525875775996
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.2155]
objective value function right now is: -1577.0746685106988
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [184.99178]
objective value function right now is: -1580.2580562840105
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1603.474555065715
Current xi:  [185.54755]
objective value function right now is: -1603.474555065715
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.58401]
objective value function right now is: -1602.2859742590724
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.2389408817078
Current xi:  [186.56216]
objective value function right now is: -1605.2389408817078
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.9689]
objective value function right now is: -1604.2318645655416
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.24066]
objective value function right now is: -1605.0949028130212
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [187.53302]
objective value function right now is: -1602.1395016640517
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1605.5691910626665
Current xi:  [188.23355]
objective value function right now is: -1605.5691910626665
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.11697]
objective value function right now is: -1605.0787086197995
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.28152]
objective value function right now is: -1604.0085607887781
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.64726]
objective value function right now is: -1601.9179013906062
new min fval from sgd:  -1605.6924729118016
new min fval from sgd:  -1605.8279900060509
new min fval from sgd:  -1605.9525763247134
new min fval from sgd:  -1605.9608924846673
new min fval from sgd:  -1606.0193256777993
new min fval from sgd:  -1606.0796242565589
new min fval from sgd:  -1606.1481121064828
new min fval from sgd:  -1606.1912952124003
new min fval from sgd:  -1606.2756248342769
new min fval from sgd:  -1606.3431021686317
new min fval from sgd:  -1606.440077279616
new min fval from sgd:  -1606.4836683268757
new min fval from sgd:  -1606.50508062472
new min fval from sgd:  -1606.512708781132
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.85219]
objective value function right now is: -1606.1232950356589
new min fval from sgd:  -1606.5159539085646
new min fval from sgd:  -1606.6970966303206
new min fval from sgd:  -1606.748564275682
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.97397]
objective value function right now is: -1606.1075156621225
new min fval from sgd:  -1606.8305318168043
new min fval from sgd:  -1606.8492982074677
new min fval from sgd:  -1606.8871822568187
new min fval from sgd:  -1606.9298377909693
new min fval from sgd:  -1606.936493561843
new min fval from sgd:  -1606.9696602109323
new min fval from sgd:  -1606.9703443678782
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [189.0365]
objective value function right now is: -1606.477293058222
new min fval from sgd:  -1606.989648212749
new min fval from sgd:  -1607.04054815816
new min fval from sgd:  -1607.0461978754795
new min fval from sgd:  -1607.0679594323942
new min fval from sgd:  -1607.112218674739
new min fval from sgd:  -1607.1426630678627
new min fval from sgd:  -1607.1564732150193
new min fval from sgd:  -1607.1797633943231
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.80731]
objective value function right now is: -1606.9890830989718
new min fval from sgd:  -1607.180835382169
new min fval from sgd:  -1607.2121272395905
new min fval from sgd:  -1607.2374396206826
new min fval from sgd:  -1607.2566497546577
new min fval from sgd:  -1607.2746252282673
new min fval from sgd:  -1607.2874291698704
new min fval from sgd:  -1607.3063231252695
new min fval from sgd:  -1607.3232788982968
new min fval from sgd:  -1607.3340381103862
new min fval from sgd:  -1607.338789655329
new min fval from sgd:  -1607.3482981870575
new min fval from sgd:  -1607.3571452422302
new min fval from sgd:  -1607.367755354607
new min fval from sgd:  -1607.3821181783937
new min fval from sgd:  -1607.3925350265952
new min fval from sgd:  -1607.3997095173534
new min fval from sgd:  -1607.4064357025438
new min fval from sgd:  -1607.411858750668
new min fval from sgd:  -1607.4151456382222
new min fval from sgd:  -1607.4195708811908
new min fval from sgd:  -1607.4254790652815
new min fval from sgd:  -1607.4376964451938
new min fval from sgd:  -1607.4440731841441
new min fval from sgd:  -1607.4501265801896
new min fval from sgd:  -1607.4526879015855
new min fval from sgd:  -1607.4540994457584
new min fval from sgd:  -1607.4578848332633
new min fval from sgd:  -1607.489592703435
new min fval from sgd:  -1607.512338180447
new min fval from sgd:  -1607.525496297365
new min fval from sgd:  -1607.5454849095236
new min fval from sgd:  -1607.5639566524635
new min fval from sgd:  -1607.5832502156272
new min fval from sgd:  -1607.6133717360462
new min fval from sgd:  -1607.6180289467407
new min fval from sgd:  -1607.6221116049733
new min fval from sgd:  -1607.6389281835231
new min fval from sgd:  -1607.650041371252
new min fval from sgd:  -1607.6567757287921
new min fval from sgd:  -1607.6741539779446
new min fval from sgd:  -1607.695731276608
new min fval from sgd:  -1607.7057309179422
new min fval from sgd:  -1607.7065302891397
new min fval from sgd:  -1607.7209386704537
new min fval from sgd:  -1607.7294690794536
new min fval from sgd:  -1607.737817196453
new min fval from sgd:  -1607.7417016139236
new min fval from sgd:  -1607.7508056468205
new min fval from sgd:  -1607.7583057178529
new min fval from sgd:  -1607.7663907884667
new min fval from sgd:  -1607.7680504165362
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.93126]
objective value function right now is: -1607.6131027717493
min fval:  -1607.7680504165362
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -1.1570,   0.4903],
        [-35.5395, -10.8950],
        [ 13.8606,  -0.0639],
        [ -2.0691,   3.8144],
        [ -2.9186,   2.5055],
        [ 12.2876,  -1.6684],
        [ 13.2848,  -4.6688],
        [  3.9986, -13.4180],
        [ 11.6718,  -7.1991],
        [  1.3208,  -0.5523]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.3743,  -9.8970, -12.1618,  -4.9512,  -6.1980,  -9.1276,  -8.6709,
        -10.1044,  -9.6439,  -5.7255], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 6.9929e-02,  8.9457e+00,  9.8304e+00,  5.2534e-01, -4.7516e-01,
          4.5926e+00,  4.2903e+00,  1.6135e+01,  3.1956e+00, -1.0825e-01],
        [ 5.8804e-03, -1.2842e-02, -1.2272e-01,  1.5499e-03,  1.8897e-02,
         -3.2232e-01, -4.7678e-01, -4.1125e-01, -1.3515e-01,  5.2762e-03],
        [ 1.7304e-01, -1.1938e+00,  9.5005e-03,  5.1615e-02, -4.2080e-03,
         -1.1227e-01, -3.9819e-01,  9.3986e+00, -1.4167e-01,  1.1094e-01],
        [-6.0483e-03,  1.0450e+01,  1.1218e+01,  5.5840e-01, -8.8916e-01,
          5.2727e+00,  5.4568e+00,  1.6213e+01,  5.6168e+00, -2.9330e-01],
        [ 5.8804e-03, -1.2842e-02, -1.2272e-01,  1.5500e-03,  1.8897e-02,
         -3.2232e-01, -4.7678e-01, -4.1125e-01, -1.3515e-01,  5.2762e-03],
        [ 5.8805e-03, -1.2842e-02, -1.2272e-01,  1.5501e-03,  1.8897e-02,
         -3.2232e-01, -4.7678e-01, -4.1125e-01, -1.3515e-01,  5.2763e-03],
        [ 5.8805e-03, -1.2842e-02, -1.2272e-01,  1.5502e-03,  1.8897e-02,
         -3.2232e-01, -4.7678e-01, -4.1125e-01, -1.3515e-01,  5.2763e-03],
        [ 4.0016e-02,  7.7392e+00,  1.3927e+01,  7.3208e-01, -2.0902e+00,
          5.3614e+00,  7.2375e+00,  1.7545e+01,  1.2011e+01,  3.2595e-01],
        [ 5.8804e-03, -1.2842e-02, -1.2272e-01,  1.5499e-03,  1.8897e-02,
         -3.2232e-01, -4.7678e-01, -4.1125e-01, -1.3515e-01,  5.2762e-03],
        [ 5.8804e-03, -1.2842e-02, -1.2272e-01,  1.5499e-03,  1.8897e-02,
         -3.2232e-01, -4.7678e-01, -4.1125e-01, -1.3515e-01,  5.2762e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-11.1006,  -2.1158,  -7.6770, -11.4345,  -2.1158,  -2.1158,  -2.1158,
        -11.5893,  -2.1158,  -2.1158], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -9.5691,   0.0189,  -3.1520, -10.5756,   0.0189,   0.0189,   0.0189,
         -12.2805,   0.0189,   0.0189]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -1.7283,   2.9731],
        [ -0.6459,   9.4208],
        [ 12.3579,   5.0153],
        [ -1.7283,   2.9731],
        [ -3.8461,  10.7883],
        [ 13.0074,  -0.8851],
        [ -1.9323,   3.2183],
        [-10.5748,  -6.6210],
        [-13.9002,  -9.1451],
        [  1.4726,  12.1123]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.6604,   2.0696,   3.7739,  -4.6604,   7.5919, -12.5533,  -4.3385,
         -2.8498,  -6.0696,   9.3289], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.6370e-01,  1.3845e+00,  1.0770e+00,  1.6371e-01,  1.3740e+01,
          1.5619e+00,  1.8567e-01, -1.1359e+01,  1.3545e+00,  1.3711e+01],
        [ 1.4530e-01, -6.1675e-03, -3.4732e+00,  1.4529e-01, -2.1435e-01,
         -4.8941e-01,  1.2023e-01, -3.7148e-01, -4.8658e-02, -9.6010e-01],
        [ 4.7880e-02, -3.5183e-02, -3.7819e+00,  4.7880e-02, -2.9446e-01,
         -4.0072e-01,  4.8466e-02, -3.3752e-01, -5.5341e-02, -9.4198e-01],
        [ 2.3039e-01, -1.5476e-01, -3.2961e+00,  2.3039e-01, -2.8705e-01,
         -3.8505e-01,  2.1323e-01, -1.0501e+00, -2.8021e-01, -1.4193e+00],
        [-7.2794e-01, -2.3029e+00, -3.3308e+00, -7.2794e-01, -5.2230e+00,
         -2.5368e+01, -6.6463e-01,  2.7859e+00, -7.7593e-01,  4.9823e+00],
        [ 3.4334e+00, -1.8889e+00, -1.1577e+00,  3.4334e+00, -9.8787e+00,
         -1.6489e+00,  3.7981e+00, -1.2050e+01,  5.3749e+00, -3.6154e-01],
        [-5.9557e-01, -8.6827e-01, -1.3283e+01, -5.9559e-01, -4.0897e+00,
         -2.5291e+01, -6.2271e-01,  7.1083e+00,  1.0360e+01, -4.2582e+01],
        [-1.1157e-01, -2.2612e+00, -6.9193e+00, -1.1159e-01, -8.2132e+00,
         -1.4195e+01, -1.1921e-01,  6.3442e+00,  7.0728e+00, -2.0490e+01],
        [-1.3247e+00,  4.9226e+00, -8.9723e+00, -1.3247e+00,  1.2927e+00,
          1.3427e+00, -5.8272e-01,  2.8053e+00,  2.0838e+00, -5.0788e-01],
        [ 2.6932e+00,  1.3195e+00,  3.3087e-01,  2.6933e+00, -9.9668e+00,
         -7.8849e+00,  2.2818e+00, -1.7906e+00,  5.0914e+00, -3.7446e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-0.7271, -4.3374, -3.8793, -5.2128, -2.8286, -2.9330, -4.6101,  1.5972,
        -1.4842, -3.9782], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ 4.9991e-01,  2.3916e-02,  3.0650e-02, -2.7768e-01,  2.3297e+00,
         -6.1817e+00,  1.9032e+01, -9.6529e+00,  3.0818e+00, -5.2202e+00],
        [-4.4815e-01, -4.7343e-03, -2.5640e-02,  2.9726e-01, -2.3916e+00,
          5.8370e+00, -1.9229e+01,  9.4217e+00, -3.0864e+00,  5.2689e+00]],
       device='cuda:0'))])
xi:  [188.92459]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 689.7006449860389
W_T_median: 428.8877099119345
W_T_pctile_5: 189.13928889497174
W_T_CVAR_5_pct: 23.229573835987022
Average q (qsum/M+1):  48.11683310231855
Optimal xi:  [188.92459]
Observed VAR:  428.8877099119345
Expected(across Rb) median(across samples) p_equity:  0.2714685551822186
obj fun:  tensor(-1607.7681, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:196: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:158: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
