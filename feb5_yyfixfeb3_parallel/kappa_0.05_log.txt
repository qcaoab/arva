Starting at: 
05-02-23_19:30

 Random seed:  2  

Key parameters-------
paths: 256000
iterations: 50000
batchsize: 1000
remove neg:  False


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
-----------------------------------------------
No need to read market data.
-----------------------------------------------
3.3333333333333335% of MC simulations done.
6.666666666666667% of MC simulations done.
10.0% of MC simulations done.
13.333333333333334% of MC simulations done.
16.666666666666664% of MC simulations done.
20.0% of MC simulations done.
23.333333333333332% of MC simulations done.
26.666666666666668% of MC simulations done.
30.0% of MC simulations done.
33.33333333333333% of MC simulations done.
36.666666666666664% of MC simulations done.
40.0% of MC simulations done.
43.333333333333336% of MC simulations done.
46.666666666666664% of MC simulations done.
50.0% of MC simulations done.
53.333333333333336% of MC simulations done.
56.666666666666664% of MC simulations done.
60.0% of MC simulations done.
63.33333333333333% of MC simulations done.
66.66666666666666% of MC simulations done.
70.0% of MC simulations done.
73.33333333333333% of MC simulations done.
76.66666666666667% of MC simulations done.
80.0% of MC simulations done.
83.33333333333334% of MC simulations done.
86.66666666666667% of MC simulations done.
90.0% of MC simulations done.
93.33333333333333% of MC simulations done.
96.66666666666667% of MC simulations done.
100.0% of MC simulations done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 5000, 'itbound_SGD_algorithms': 50000, 'nit_IterateAveragingStart': 45000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1576.8182150775267
W_T_median: 1136.7449346433505
W_T_pctile_5: -127.64084552288806
W_T_CVAR_5_pct: -295.72244238283764
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1692.39681308817
Current xi:  [53.6344]
objective value function right now is: -1692.39681308817
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1705.2802822335836
Current xi:  [6.202024]
objective value function right now is: -1705.2802822335836
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1712.6575122114168
Current xi:  [-35.81122]
objective value function right now is: -1712.6575122114168
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1717.6649902475256
Current xi:  [-76.78422]
objective value function right now is: -1717.6649902475256
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1721.911701570584
Current xi:  [-117.892654]
objective value function right now is: -1721.911701570584
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1727.0031145462185
Current xi:  [-158.94334]
objective value function right now is: -1727.0031145462185
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1730.3652022286901
Current xi:  [-199.58315]
objective value function right now is: -1730.3652022286901
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1733.754355344318
Current xi:  [-239.67522]
objective value function right now is: -1733.754355344318
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1734.9888596592104
Current xi:  [-278.36908]
objective value function right now is: -1734.9888596592104
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1737.7087476095492
Current xi:  [-315.40967]
objective value function right now is: -1737.7087476095492
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1738.8168840788967
Current xi:  [-351.24896]
objective value function right now is: -1738.8168840788967
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1739.8344985759256
Current xi:  [-384.37518]
objective value function right now is: -1739.8344985759256
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.7281896697828
Current xi:  [-414.94052]
objective value function right now is: -1740.7281896697828
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1741.1529974650111
Current xi:  [-441.7979]
objective value function right now is: -1741.1529974650111
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.170635596966
Current xi:  [-463.73322]
objective value function right now is: -1741.170635596966
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-479.0397]
objective value function right now is: -1741.1337893029722
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.3302403579737
Current xi:  [-485.13495]
objective value function right now is: -1741.3302403579737
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-490.47876]
objective value function right now is: -1741.0174812551602
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.78137]
objective value function right now is: -1739.8078963180162
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.6993]
objective value function right now is: -1740.7640766567677
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-494.52972]
objective value function right now is: -1741.1247554332704
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.506171438583
Current xi:  [-495.89444]
objective value function right now is: -1741.506171438583
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-495.53543]
objective value function right now is: -1740.9387805938452
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.26608]
objective value function right now is: -1740.9369567717995
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.73877]
objective value function right now is: -1741.2729971541146
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-496.96014]
objective value function right now is: -1740.5746225913224
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.05472]
objective value function right now is: -1740.861240071758
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-492.82315]
objective value function right now is: -1741.295785237919
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-491.86572]
objective value function right now is: -1741.0356314903038
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-495.44644]
objective value function right now is: -1740.508755111839
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.40173]
objective value function right now is: -1740.9368906412208
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.52866]
objective value function right now is: -1741.2259395780486
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-494.768]
objective value function right now is: -1741.0442939070022
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.0034]
objective value function right now is: -1741.1092426523478
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-494.23553]
objective value function right now is: -1740.6672668857393
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.82632]
objective value function right now is: -1741.3179537959868
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.5812060560195
Current xi:  [-493.52716]
objective value function right now is: -1741.5812060560195
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.02887]
objective value function right now is: -1741.5069131995826
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.05707]
objective value function right now is: -1741.3816848564763
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.70966]
objective value function right now is: -1740.9746954410714
82.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.6076099344593
Current xi:  [-491.90036]
objective value function right now is: -1741.6076099344593
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.623533288396
Current xi:  [-492.91754]
objective value function right now is: -1741.623533288396
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.636979805235
Current xi:  [-494.1527]
objective value function right now is: -1741.636979805235
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-493.70868]
objective value function right now is: -1741.534908435202
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.9006]
objective value function right now is: -1741.534521397978
new min fval from sgd:  -1741.6495156017659
new min fval from sgd:  -1741.6520181444869
new min fval from sgd:  -1741.652063883725
new min fval from sgd:  -1741.6538921438619
new min fval from sgd:  -1741.6603004510016
new min fval from sgd:  -1741.6610675909976
new min fval from sgd:  -1741.6634801309165
new min fval from sgd:  -1741.6665924409415
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-492.05844]
objective value function right now is: -1741.4425970893055
new min fval from sgd:  -1741.6682871055837
new min fval from sgd:  -1741.6692920341861
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.6507]
objective value function right now is: -1741.516185262417
new min fval from sgd:  -1741.6712705320722
new min fval from sgd:  -1741.6736092519584
new min fval from sgd:  -1741.6739918138476
new min fval from sgd:  -1741.6744751030806
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.42407]
objective value function right now is: -1741.586665100981
new min fval from sgd:  -1741.6745576446829
new min fval from sgd:  -1741.675238640966
new min fval from sgd:  -1741.6754026297672
new min fval from sgd:  -1741.67579414705
new min fval from sgd:  -1741.6760327370089
new min fval from sgd:  -1741.6764778571712
new min fval from sgd:  -1741.6769772652303
new min fval from sgd:  -1741.6775380058048
new min fval from sgd:  -1741.6779818067482
new min fval from sgd:  -1741.678385857179
new min fval from sgd:  -1741.6791604551227
new min fval from sgd:  -1741.6798438776445
new min fval from sgd:  -1741.6798557145548
new min fval from sgd:  -1741.6800742473974
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.0952]
objective value function right now is: -1741.6706153217046
new min fval from sgd:  -1741.6805970407531
new min fval from sgd:  -1741.6808276564182
new min fval from sgd:  -1741.681669257213
new min fval from sgd:  -1741.682584943129
new min fval from sgd:  -1741.6828046522614
new min fval from sgd:  -1741.6843558703881
new min fval from sgd:  -1741.6856809252777
new min fval from sgd:  -1741.686768309522
new min fval from sgd:  -1741.687552357205
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-491.2611]
objective value function right now is: -1741.6690407165913
min fval:  -1741.687552357205
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.5357,  1.3663],
        [-0.5357,  1.3663],
        [16.1731,  2.4886],
        [-0.5357,  1.3663],
        [-0.5357,  1.3663],
        [-0.5357,  1.3663],
        [ 5.7240, -7.8680],
        [-0.5357,  1.3663],
        [-0.5357,  1.3663],
        [-0.5357,  1.3663]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -0.8336,  -0.8336,  -8.5492,  -0.8336,  -0.8336,  -0.8336, -15.0996,
         -0.8336,  -0.8336,  -0.8336], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 3.3673e-03,  3.3674e-03,  8.5165e+00,  3.3678e-03,  3.3673e-03,
          3.3673e-03, -9.6538e+00,  3.3674e-03,  3.3673e-03,  3.3673e-03],
        [-4.3569e-03, -4.3569e-03, -2.4052e-01, -4.3569e-03, -4.3569e-03,
         -4.3569e-03, -4.8705e-01, -4.3569e-03, -4.3569e-03, -4.3569e-03],
        [-4.3569e-03, -4.3569e-03, -2.4052e-01, -4.3569e-03, -4.3569e-03,
         -4.3569e-03, -4.8705e-01, -4.3569e-03, -4.3569e-03, -4.3569e-03],
        [-4.3569e-03, -4.3569e-03, -2.4052e-01, -4.3569e-03, -4.3569e-03,
         -4.3569e-03, -4.8705e-01, -4.3569e-03, -4.3569e-03, -4.3569e-03],
        [ 2.7570e-02,  2.7570e-02, -3.7084e+00,  2.7570e-02,  2.7570e-02,
          2.7570e-02,  4.2857e+00,  2.7570e-02,  2.7570e-02,  2.7570e-02],
        [ 2.5130e-02,  2.5130e-02, -4.8863e+00,  2.5130e-02,  2.5130e-02,
          2.5130e-02,  5.5269e+00,  2.5130e-02,  2.5130e-02,  2.5130e-02],
        [-2.2138e-02, -2.2138e-02, -8.7215e-01, -2.2138e-02, -2.2138e-02,
         -2.2138e-02,  6.2120e-01, -2.2138e-02, -2.2138e-02, -2.2138e-02],
        [-6.0403e-02, -6.0403e-02, -5.3368e+00, -6.0403e-02, -6.0403e-02,
         -6.0403e-02,  6.8947e+00, -6.0403e-02, -6.0403e-02, -6.0403e-02],
        [-6.1263e-02, -6.1263e-02, -5.4417e+00, -6.1263e-02, -6.1263e-02,
         -6.1263e-02,  6.9998e+00, -6.1263e-02, -6.1263e-02, -6.1263e-02],
        [-4.3569e-03, -4.3569e-03, -2.4052e-01, -4.3569e-03, -4.3569e-03,
         -4.3569e-03, -4.8705e-01, -4.3569e-03, -4.3569e-03, -4.3569e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 3.4880, -0.7387, -0.7387, -0.7387, -2.1483, -2.1954, -1.8186, -2.4304,
        -2.4731, -0.7387], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 8.5225,  0.0112,  0.0112,  0.0112, -2.8047, -4.0018, -0.5164, -5.6554,
         -5.8264,  0.0112]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-14.3173,  -9.8005],
        [ -4.5059,  12.3783],
        [  4.0075,  11.2424],
        [-10.7275,  -0.1684],
        [ -0.9541,   3.3921],
        [ -1.4703,   0.6435],
        [  7.5357,  -2.8885],
        [ 15.9095,   3.9685],
        [-12.1494,  -1.3344],
        [  1.9512,  -0.8170]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-9.9395,  9.9218,  8.0814, 10.9133, -2.4820, -2.5288, -9.3459, -6.4657,
        -0.6547,  4.0123], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.7372e+01, -8.0764e+00, -1.2543e+01,  6.1272e+00,  1.0051e-01,
          3.5989e-01,  5.7130e-01, -5.1168e+00,  9.0419e+00, -3.5706e+00],
        [-1.1763e-01,  3.8961e-01, -5.9934e-01, -1.1394e+00,  1.7530e-01,
          7.6557e-02, -8.2610e-01, -1.2249e+00, -1.1847e-01, -1.3206e+00],
        [-6.6884e+00, -9.5679e+00, -1.6031e+00,  3.0496e+00, -4.1722e-02,
         -6.7963e-02, -8.8737e-01, -2.8653e-01,  4.2580e-02, -1.6582e+00],
        [-2.2399e-01, -3.0328e-02, -6.8609e-01, -1.2540e+00,  4.8544e-02,
          1.7397e-02, -6.8926e-01, -9.7152e-01, -2.1730e-01, -1.3873e+00],
        [ 2.4608e+00, -4.6921e-01,  4.9677e+00, -5.1619e+00, -4.6275e-02,
         -1.0756e-01, -6.5983e+00,  1.1267e+01, -2.9249e+00,  4.5143e+00],
        [-7.0181e-02,  5.9386e-01, -5.4803e-01, -1.0788e+00,  2.5169e-01,
          1.0989e-01, -8.7534e-01, -1.3176e+00, -6.8307e-02, -1.2817e+00],
        [-1.8972e-01,  1.0018e-01, -6.6370e-01, -1.2191e+00,  8.0026e-02,
          3.0949e-02, -7.3405e-01, -1.0605e+00, -1.8675e-01, -1.3685e+00],
        [-1.6391e-01,  2.0003e-01, -6.4320e-01, -1.1919e+00,  1.1033e-01,
          4.5534e-02, -7.6713e-01, -1.1219e+00, -1.6307e-01, -1.3528e+00],
        [-1.2832e-01,  3.4461e-01, -6.1018e-01, -1.1520e+00,  1.5919e-01,
          6.9193e-02, -8.1284e-01, -1.2020e+00, -1.2909e-01, -1.3285e+00],
        [-1.7467e+01, -1.3046e+00,  3.0792e-01, -3.1242e+00, -7.1639e-01,
         -3.7915e-04,  1.5394e+01,  7.2513e-01, -5.5098e+00,  4.8504e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-1.5093, -1.3219, -0.2662, -1.3893,  2.5057, -1.2828, -1.3703, -1.3544,
        -1.3299,  0.6828], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-9.9128,  0.4105, -1.2300,  0.1747,  4.8174,  0.5432,  0.2409,  0.2961,
          0.3824, -4.8214],
        [ 9.9303, -0.4105,  0.8952, -0.1747, -4.8519, -0.5432, -0.2410, -0.2961,
         -0.3824,  4.7004]], device='cuda:0'))])
xi:  [-491.0825]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 237.98940957817857
W_T_median: 104.27223572446881
W_T_pctile_5: -492.1400775155102
W_T_CVAR_5_pct: -601.3195898621115
Average q (qsum/M+1):  57.153340001260084
Optimal xi:  [-491.0825]
Observed VAR:  104.27223572446881
Expected(across Rb) median(across samples) p_equity:  0.27849886173692845
obj fun:  tensor(-1741.6876, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
[0.0, 100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0, 1100.0, 1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0]
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:158: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
