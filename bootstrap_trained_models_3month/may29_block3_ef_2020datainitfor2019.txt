Starting at: 
29-05-23_14:24

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 10000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
               CPI  CPI_nom_ret  ...  VWD_real_ret  EWD_real_ret
date                             ...                            
192512  100.000000          NaN  ...           NaN           NaN
192601  100.000000     0.000000  ...      0.000561      0.023174
192602  100.000000     0.000000  ...     -0.033046     -0.053510
192603   99.441303    -0.005587  ...     -0.058743     -0.091750
192604  100.000000     0.005618  ...      0.031235      0.027204

[5 rows x 12 columns]
                CPI  CPI_nom_ret  ...  VWD_real_ret  EWD_real_ret
date                              ...                            
202008  1452.055956     0.003153  ...      0.065084      0.035618
202009  1454.078149     0.001393  ...     -0.036399     -0.028709
202010  1454.681696     0.000415  ...     -0.020584      0.000169
202011  1453.793232    -0.000611  ...      0.124393      0.175130
202012  1455.162018     0.000942  ...      0.044065      0.071843

[5 rows x 12 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_nom_ret', 'VWD_nom_ret']
############# End: defined asset  basket #################
        CPI_nom_ret  B10_nom_ret  VWD_nom_ret
date                                         
192601     0.000000     0.004350     0.000561
192602     0.000000     0.004338    -0.033046
192603    -0.005587     0.004327    -0.064002
192604     0.005618     0.004316     0.037029
192605    -0.005587     0.004304     0.012095
        CPI_nom_ret  B10_nom_ret  VWD_nom_ret
date                                         
202008     0.003153    -0.014709     0.068443
202009     0.001393     0.002560    -0.035057
202010     0.000415    -0.017311    -0.020178
202011    -0.000611     0.004016     0.123706
202012     0.000942    -0.004965     0.045048
Indices constructed with column names:
Index(['date_for_plt', 'CPI_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
B10_real_ret    0.001986
VWD_real_ret    0.006974
dtype: float64


timeseries_basket['data_df_stdev'] = 
B10_real_ret    0.018939
VWD_real_ret    0.053569
dtype: float64


timeseries_basket['data_df_corr'] = 
              B10_real_ret  VWD_real_ret
B10_real_ret      1.000000      0.075353
VWD_real_ret      0.075353      1.000000


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192601
End: 202012
-----------------------------------------------
Bootstrap block size: 3
-----------------------------------------------
Dates USED bootstrapping:
Start: 192601
End: 201912
-----------------------------------------------
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 1000, 'itbound_SGD_algorithms': 10000, 'nit_IterateAveragingStart': 9000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.3890,  1.2705],
        [-0.3890,  1.2705],
        [-3.1769,  6.0819],
        [13.5058,  1.4187],
        [-0.3899,  1.2758],
        [-0.3890,  1.2704],
        [-7.1279,  3.6605],
        [-0.3890,  1.2705],
        [-2.7274,  6.0897],
        [-7.4749, -4.6178]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.7172, -0.7172, 10.7230, -8.1522, -0.7211, -0.7172,  8.5092, -0.7172,
        10.6407,  0.0170], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01],
        [-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01],
        [ 1.2597e-01,  1.2597e-01, -7.3658e+00, -9.7405e+00,  8.8391e-02,
          1.2638e-01, -4.9376e+00,  1.2597e-01, -6.9440e+00,  1.7873e+00],
        [ 8.3322e-02,  8.3323e-02,  2.1481e+00,  3.8260e+00,  7.0418e-02,
          8.3492e-02,  1.7870e+00,  8.3323e-02,  2.0981e+00, -7.7238e-02],
        [-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01],
        [-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01],
        [ 4.2223e-02,  4.2225e-02,  3.4672e+00,  5.0553e+00,  6.1523e-03,
          4.2634e-02,  2.6714e+00,  4.2224e-02,  3.3334e+00, -1.9996e-01],
        [ 2.2983e-02,  2.2984e-02,  3.9047e+00,  5.5445e+00, -1.6427e-02,
          2.3378e-02,  2.9206e+00,  2.2984e-02,  3.7580e+00, -3.6710e-01],
        [-5.6500e-03, -5.6500e-03, -3.3708e-02, -1.4784e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6500e-03, -4.0836e-02, -4.7452e-01],
        [-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6068e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5222, -0.5222,  4.8970, -3.0995, -0.5222, -0.5222, -3.3636, -3.4951,
        -0.5221, -0.5222], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.4894e-03,  1.4894e-03, -1.3247e+01,  3.5944e+00,  1.4894e-03,
          1.4894e-03,  5.2154e+00,  5.9972e+00,  1.4895e-03,  1.4894e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 12.8302,   3.0636],
        [ -4.1623,  -9.0103],
        [  2.6807,  -7.5523],
        [-16.5897,  -5.9841],
        [-15.5150,  -2.9124],
        [-10.2233,   0.7646],
        [ -2.3654,  -4.4019],
        [-10.1674,  -9.3383],
        [ -8.7847,   0.8757],
        [  7.9507,   8.6118]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -2.6631, -11.6716,  -7.2738,  -3.6322,   1.6722,  11.6587,  -1.8508,
         -7.5078,   5.2666,   7.2578], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5515e+00,  1.5561e+00,  2.6742e+00, -3.0320e+00, -4.6687e+00,
         -9.0387e-01,  2.2213e+00,  5.3288e-01,  1.4450e+00, -4.0020e+00],
        [-2.4323e-01, -8.3041e-01, -1.0577e+00, -6.7899e-01, -5.0301e-01,
         -9.9127e-01, -1.5997e+00, -9.8904e-01, -3.1749e+00, -3.1361e-01],
        [-1.0866e+00, -4.0812e-01, -5.3434e+00,  9.1229e-01,  5.7108e+00,
         -1.3832e+00, -7.6576e+00, -2.6274e+00,  5.6125e+00, -8.4968e-01],
        [-4.1727e+00,  1.6408e-01,  2.5270e-02,  6.8018e+00,  9.1168e-03,
         -5.8992e+00, -3.1741e-01,  4.0828e-01, -3.6580e+00,  2.4430e-02],
        [-1.0131e+00, -3.2826e-01, -3.7271e+00, -1.5399e+00,  3.6249e+00,
         -1.3771e+00, -4.3775e+00, -2.7616e+00,  4.7750e+00, -7.7867e-01],
        [-1.5592e+00,  8.8254e+00, -9.7969e-01,  3.2892e+00,  8.9069e+00,
          4.6782e-03, -4.5292e+00, -2.0707e+00,  1.4748e+00, -2.4894e+00],
        [-4.3279e+00, -1.2782e+01,  4.2846e+00, -2.9663e+00, -4.8306e+00,
          6.2756e+00,  1.9816e-01,  7.3166e+00, -3.8854e+00, -7.5519e+00],
        [-4.0843e-01, -7.9911e-01, -1.1385e+00, -6.5685e-01, -4.2991e-01,
         -1.1014e+00, -1.6045e+00, -9.1947e-01, -2.6564e+00, -4.4088e-01],
        [-2.9625e+00,  1.0796e+00, -2.9583e-01,  5.3380e+00,  1.4345e+00,
         -4.7627e+00, -8.6064e-01, -3.2371e-01, -3.5119e+00,  1.2628e+00],
        [-8.1147e-01,  9.6900e+00, -5.2043e+00,  7.9194e+00,  1.0944e+00,
          1.1123e+00, -3.7378e+00, -4.9453e+00,  7.7430e+00,  1.2268e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.1657, -1.2410, -1.9436, -0.8159, -1.8813, -6.2588, -1.2579, -1.3573,
        -0.8754, -1.5403], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-0.3370, -1.0365, -4.3117, -4.1120, -2.4344,  5.4253, -7.8169, -0.8955,
         -1.9441,  0.7044],
        [ 0.1068,  1.0365,  4.3118,  4.1068,  2.4344, -5.3462,  7.8905,  0.8955,
          2.2557, -0.8519]], device='cuda:0'))])
loaded xi:  -458.1955
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1742.4629955042024
Current xi:  [-461.01572]
objective value function right now is: -1742.4629955042024
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-463.62164]
objective value function right now is: -1742.301335309125
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1742.577705202874
Current xi:  [-465.3742]
objective value function right now is: -1742.577705202874
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-466.13348]
objective value function right now is: -1742.2892824848122
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-468.30994]
objective value function right now is: -1742.193554717658
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-468.3731]
objective value function right now is: -1741.4133990299356
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-470.986]
objective value function right now is: -1742.2913570750716
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-472.25134]
objective value function right now is: -1742.408945855694
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1742.6733410534296
Current xi:  [-472.108]
objective value function right now is: -1742.6733410534296
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-472.5255]
objective value function right now is: -1742.4925465682545
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-473.18134]
objective value function right now is: -1741.870389784872
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1742.7474456535251
Current xi:  [-473.83215]
objective value function right now is: -1742.7474456535251
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1743.0328579350712
Current xi:  [-474.61823]
objective value function right now is: -1743.0328579350712
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-474.69998]
objective value function right now is: -1741.1151173613055
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.64926]
objective value function right now is: -1742.0117573980183
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.49652]
objective value function right now is: -1742.704133596982
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.53372]
objective value function right now is: -1742.3336915920934
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.54263]
objective value function right now is: -1742.7781724790466
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.052]
objective value function right now is: -1742.7396808338644
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.97144]
objective value function right now is: -1742.7248468415132
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.55243]
objective value function right now is: -1742.5928570311962
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.7841]
objective value function right now is: -1742.0135550509613
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-476.50586]
objective value function right now is: -1742.8631656205828
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-476.01846]
objective value function right now is: -1742.8353472988333
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-476.2102]
objective value function right now is: -1742.583579698035
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.44687]
objective value function right now is: -1742.3324050102642
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.56735]
objective value function right now is: -1742.0734458845745
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-474.36655]
objective value function right now is: -1742.6592716986934
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-474.75293]
objective value function right now is: -1742.783575426327
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-476.6124]
objective value function right now is: -1742.5282743028752
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-477.24222]
objective value function right now is: -1742.7431696787444
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-476.78094]
objective value function right now is: -1742.744220089621
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-476.25577]
objective value function right now is: -1742.729899079946
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-476.28046]
objective value function right now is: -1742.840104842162
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-476.06226]
objective value function right now is: -1742.6282095839954
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-476.1986]
objective value function right now is: -1743.0018757226292
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1743.0608635031817
Current xi:  [-476.00168]
objective value function right now is: -1743.0608635031817
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-475.72913]
objective value function right now is: -1742.994638967601
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1743.0621582751266
Current xi:  [-475.50546]
objective value function right now is: -1743.0621582751266
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1743.105993357595
Current xi:  [-475.00412]
objective value function right now is: -1743.105993357595
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.7928]
objective value function right now is: -1743.0779122733911
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.8539]
objective value function right now is: -1743.0677344809064
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.6622]
objective value function right now is: -1743.0629571695174
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.73938]
objective value function right now is: -1743.0491847722485
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.6802]
objective value function right now is: -1743.054107090599
new min fval from sgd:  -1743.1111051536031
new min fval from sgd:  -1743.1115221500654
new min fval from sgd:  -1743.1121639748283
new min fval from sgd:  -1743.118641369891
new min fval from sgd:  -1743.121770477963
new min fval from sgd:  -1743.1260709776068
new min fval from sgd:  -1743.132932373431
new min fval from sgd:  -1743.1336246659284
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.5952]
objective value function right now is: -1743.027387598063
new min fval from sgd:  -1743.1429921378497
new min fval from sgd:  -1743.1440834832752
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.38934]
objective value function right now is: -1743.0063327696412
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.50174]
objective value function right now is: -1743.0309594476298
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.23386]
objective value function right now is: -1743.13909955416
new min fval from sgd:  -1743.1441960691877
new min fval from sgd:  -1743.1461260549918
new min fval from sgd:  -1743.1474543648974
new min fval from sgd:  -1743.1478294303658
new min fval from sgd:  -1743.1481388025118
new min fval from sgd:  -1743.1495282186163
new min fval from sgd:  -1743.1501954812668
new min fval from sgd:  -1743.1502050325303
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-474.12747]
objective value function right now is: -1743.1433786991427
min fval:  -1743.1502050325303
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.4116,  1.3199],
        [-0.4116,  1.3199],
        [-2.8881,  6.7106],
        [14.9030,  1.8236],
        [-0.4116,  1.3199],
        [-0.4116,  1.3199],
        [-7.3815,  3.9500],
        [-0.4116,  1.3199],
        [-2.2671,  6.7956],
        [-7.1607, -2.9320]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.6831, -0.6831, 11.5805, -9.1002, -0.6831, -0.6831,  8.4358, -0.6831,
        11.4530, -1.4539], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.7876e-02, -1.7876e-02, -8.6863e-02, -2.6355e-01, -1.7876e-02,
         -1.7876e-02, -5.3778e-02, -1.7876e-02, -9.0438e-02, -1.9438e-01],
        [-1.7876e-02, -1.7876e-02, -8.6863e-02, -2.6355e-01, -1.7876e-02,
         -1.7876e-02, -5.3778e-02, -1.7876e-02, -9.0438e-02, -1.9438e-01],
        [ 1.2096e-01,  1.2096e-01, -7.4791e+00, -1.0507e+01,  1.2096e-01,
          1.2096e-01, -3.9778e+00,  1.2096e-01, -7.2422e+00,  6.5532e-01],
        [ 2.1593e-01,  2.1593e-01,  1.2645e+00, -1.2186e-02,  2.1593e-01,
          2.1593e-01,  1.2058e+00,  2.1593e-01,  1.2207e+00, -8.7088e-01],
        [-1.7876e-02, -1.7876e-02, -8.6863e-02, -2.6355e-01, -1.7876e-02,
         -1.7876e-02, -5.3778e-02, -1.7876e-02, -9.0438e-02, -1.9438e-01],
        [-1.7876e-02, -1.7876e-02, -8.6863e-02, -2.6355e-01, -1.7876e-02,
         -1.7876e-02, -5.3778e-02, -1.7876e-02, -9.0438e-02, -1.9438e-01],
        [ 3.3022e-02,  3.3022e-02,  3.7849e+00,  5.2999e+00,  3.3022e-02,
          3.3022e-02,  1.8996e+00,  3.3022e-02,  3.7527e+00,  6.6934e-01],
        [-6.9860e-03, -6.9859e-03,  4.3408e+00,  6.0624e+00, -6.9859e-03,
         -6.9859e-03,  2.1862e+00, -6.9860e-03,  4.2987e+00,  5.3325e-01],
        [-1.7876e-02, -1.7876e-02, -8.6863e-02, -2.6355e-01, -1.7876e-02,
         -1.7876e-02, -5.3778e-02, -1.7876e-02, -9.0438e-02, -1.9438e-01],
        [-1.7876e-02, -1.7876e-02, -8.6863e-02, -2.6355e-01, -1.7876e-02,
         -1.7876e-02, -5.3778e-02, -1.7876e-02, -9.0438e-02, -1.9438e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7982, -0.7982,  5.3043, -2.2768, -0.7982, -0.7982, -3.4286, -3.6425,
        -0.7982, -0.7982], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0303,  -0.0303, -12.8805,   1.8007,  -0.0303,  -0.0303,   5.8044,
           6.9833,  -0.0303,  -0.0303]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 13.0398,   2.9332],
        [ -4.2506,  -9.6273],
        [  3.7264,  -7.9721],
        [-16.4685,  -6.4120],
        [-14.5486,  -2.5781],
        [-12.3933,   0.2663],
        [ -5.5573,  -6.1077],
        [-10.1669,  -9.6245],
        [ -8.4640,   1.3361],
        [  7.9075,   9.2857]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -3.8641, -12.0384,  -8.0521,  -4.2529,   1.8889,  12.1311,  -3.1131,
         -7.5913,   5.7351,   7.3481], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[  1.4363,  -0.7948,   3.7646,  -4.8710,  -5.5298,   1.7145,   3.3426,
           2.1426,  -0.7843,  -4.1346],
        [ -0.5591,  -1.1772,  -0.0351,  -0.4769,  -0.5398,  -1.3934,  -3.4742,
          -0.7858,  -3.8795,  -0.3909],
        [ -1.1882,  -0.4769,  -0.7346,  -0.4938,  -0.4959,  -0.8933,  -0.5026,
          -0.4793,  -0.2802,  -1.2512],
        [ -3.9416,   0.1736,  -0.7707,   8.5852,   0.3627,  -5.7661,  -0.5687,
           0.2845,  -3.7352,   0.1033],
        [ -1.1892,  -0.4769,  -0.7356,  -0.4932,  -0.4954,  -0.8925,  -0.5028,
          -0.4792,  -0.2804,  -1.2508],
        [ -0.9845,  10.4842,  -3.5289,   4.7125,   8.9020,   0.7348,  -4.2300,
          -2.9615,   0.3569,  -0.5315],
        [ -3.8781, -11.6431,   4.7153,  -2.7397,  -4.7000,   6.3348,  -0.6087,
           7.0057,  -2.6747,  -8.3752],
        [ -1.1912,  -0.4794,  -0.7289,  -0.4937,  -0.4961,  -0.8988,  -0.5029,
          -0.4812,  -0.2794,  -1.2450],
        [ -3.1889,   0.6671,  -0.9896,   6.7605,   1.8848,  -4.8835,  -0.8292,
           0.1301,  -3.1318,  -0.3236],
        [ -1.2607,   8.0956,  -5.6303,   8.9911,   1.4931,   2.6997,  -2.0809,
          -5.0753,   4.6687,  -0.3588]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-2.9338, -1.4592, -1.8484, -0.8441, -1.8465, -6.2203, -2.3888, -1.8587,
        -1.1750, -2.1765], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-0.5337, -2.0400,  0.0156, -4.1127,  0.0156,  5.5426, -7.6339,  0.0155,
         -1.9083,  0.5455],
        [ 0.3100,  2.0400, -0.0156,  4.1075, -0.0155, -5.4649,  7.6748, -0.0155,
          2.2147, -0.6907]], device='cuda:0'))])
xi:  [-474.13513]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 258.66451123132225
W_T_median: 116.50817840705481
W_T_pctile_5: -469.7570318512789
W_T_CVAR_5_pct: -565.0307412204601
Average q (qsum/M+1):  57.142144972278224
Optimal xi:  [-474.13513]
Expected(across Rb) median(across samples) p_equity:  0.308783899092911
obj fun:  tensor(-1743.1502, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.8181,  1.2655],
        [-0.8181,  1.2655],
        [-6.7043,  8.6399],
        [-0.8181,  1.2655],
        [-0.8181,  1.2655],
        [-0.8181,  1.2655],
        [-0.8319,  1.2686],
        [-0.8181,  1.2655],
        [ 4.0510,  9.2020],
        [-0.8181,  1.2655]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.9350, -0.9350, 12.2100, -0.9350, -0.9350, -0.9350, -0.9740, -0.9350,
         9.9181, -0.9350], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-9.6010e-03, -9.6010e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6010e-03, -5.0271e-01, -9.6008e-03],
        [-9.6010e-03, -9.6011e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6011e-03, -5.0271e-01, -9.6008e-03],
        [ 5.9245e-01,  5.9245e-01, -8.2989e+00,  5.9239e-01,  5.9240e-01,
          5.9246e-01,  4.7607e-01,  5.9245e-01, -9.4766e+00,  5.9239e-01],
        [-1.6111e-01, -1.6111e-01,  4.4591e+00, -1.6113e-01, -1.6113e-01,
         -1.6111e-01, -2.3334e-01, -1.6111e-01,  4.1745e+00, -1.6113e-01],
        [-9.6010e-03, -9.6010e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6010e-03, -5.0271e-01, -9.6008e-03],
        [-9.6011e-03, -9.6011e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6011e-03, -5.0271e-01, -9.6008e-03],
        [-1.7597e-01, -1.7597e-01,  5.2062e+00, -1.7602e-01, -1.7601e-01,
         -1.7597e-01, -2.4063e-01, -1.7597e-01,  4.7205e+00, -1.7602e-01],
        [-2.0765e-01, -2.0765e-01,  5.9716e+00, -2.0773e-01, -2.0771e-01,
         -2.0764e-01, -2.7815e-01, -2.0765e-01,  5.3062e+00, -2.0773e-01],
        [-9.6010e-03, -9.6011e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6011e-03, -5.0271e-01, -9.6008e-03],
        [-9.6010e-03, -9.6010e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6011e-03, -5.0271e-01, -9.6008e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7303, -0.7303,  8.0176, -4.1127, -0.7303, -0.7303, -4.6390, -5.1923,
        -0.7303, -0.7303], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0248,  -0.0248, -14.4336,   4.4326,  -0.0248,  -0.0248,   5.3747,
           6.5926,  -0.0248,  -0.0248]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 14.9886,   2.4124],
        [ -3.9607,  -6.8647],
        [  2.8421, -15.0439],
        [-15.0104,  -1.3554],
        [-17.8803,  -2.1979],
        [-13.6356,   0.1386],
        [  1.5865,  -2.9758],
        [ -9.1459,  -9.2862],
        [-12.0836,   1.7614],
        [ 12.6914,  10.9896]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.8115, -13.3717, -15.6698,   0.1876,   3.0066,  12.2546,  -2.6500,
         -8.0082,   6.1204,  11.2100], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-9.9379e-01, -4.8549e+00,  6.0099e+00, -1.0764e+01,  1.1539e+00,
          1.3655e+00, -1.6340e+00, -1.3220e+00,  3.1557e+00, -2.2392e+00],
        [ 3.0425e-01, -2.2314e+00, -1.1031e+00, -4.3235e-01, -1.0041e+00,
          4.4433e-01,  1.8019e+00, -1.6744e+00, -6.2733e+00, -1.5991e+00],
        [-1.5442e+00, -9.9958e-03, -6.8393e+00,  4.0706e+00,  9.5080e-01,
          4.8177e+00, -9.3012e+00, -5.4022e+00,  3.5393e+00, -2.0912e+00],
        [-3.6220e+00,  2.5934e+00,  1.8413e-03,  5.1178e+00,  3.6488e+00,
         -4.4076e+00, -1.4818e+00, -1.1371e+00, -3.7062e+00, -2.1213e+00],
        [-1.1637e+00, -3.8776e-01, -7.0991e-01, -3.7050e-01, -3.8767e-01,
         -8.3575e-01, -1.0543e+00, -4.7849e-01, -2.9402e-02, -1.2324e+00],
        [-8.0890e+00,  4.4759e-01,  5.8985e+00,  8.0189e-01,  9.5517e+00,
          2.0382e+00, -3.0454e+00,  1.2842e+00,  1.8420e+00, -2.0685e+01],
        [-9.6121e+00,  4.2774e-01,  8.8690e+00, -1.0471e+01,  5.8744e+00,
          8.2530e+00, -4.9189e+00,  2.4137e+00,  8.7083e+00, -1.4855e+01],
        [-1.1633e+00, -3.8767e-01, -7.0976e-01, -3.7032e-01, -3.8750e-01,
         -8.3573e-01, -1.0541e+00, -4.7844e-01, -2.9403e-02, -1.2321e+00],
        [-3.3022e+00,  2.4695e+00,  7.7086e-02,  4.9005e+00,  3.4609e+00,
         -4.0536e+00, -1.4189e+00, -1.1454e+00, -3.4640e+00, -2.0260e+00],
        [-2.0113e-01,  1.0013e+01, -4.8859e+00,  6.8310e+00,  1.8534e+00,
          3.4384e+00, -3.1067e+00, -4.9847e+00,  1.8256e+00, -5.1503e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.5917, -3.1860, -3.7571, -2.1880, -1.7545, -6.7424, -7.9975, -1.7556,
        -2.1591, -1.7526], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-2.2944e+00, -2.0514e+00, -4.9324e+00, -5.4339e+00, -4.6270e-03,
          7.9311e+00, -3.4188e+00, -4.6358e-03, -3.2780e+00,  8.6686e-01],
        [ 2.0845e+00,  2.0514e+00,  4.9324e+00,  5.4288e+00,  4.6146e-03,
         -7.8544e+00,  3.4369e+00,  4.6236e-03,  3.5806e+00, -1.0103e+00]],
       device='cuda:0'))])
loaded xi:  -204.86673
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1677.487552068026
Current xi:  [-206.69061]
objective value function right now is: -1677.487552068026
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1677.898259616474
Current xi:  [-208.50201]
objective value function right now is: -1677.898259616474
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.0508151282595
Current xi:  [-209.5085]
objective value function right now is: -1678.0508151282595
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.0631065688704
Current xi:  [-211.00871]
objective value function right now is: -1678.0631065688704
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-212.44655]
objective value function right now is: -1677.7298834028625
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.1320707817595
Current xi:  [-213.75122]
objective value function right now is: -1678.1320707817595
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1678.3489498506062
Current xi:  [-214.68333]
objective value function right now is: -1678.3489498506062
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-216.03558]
objective value function right now is: -1678.230935240761
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-217.2383]
objective value function right now is: -1677.3778966505222
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-219.04582]
objective value function right now is: -1678.1712145273668
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-220.64331]
objective value function right now is: -1677.9732223177934
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-222.43077]
objective value function right now is: -1677.340804761124
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [-224.56343]
objective value function right now is: -1677.5978686256462
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-226.07988]
objective value function right now is: -1678.329647605348
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-228.2825]
objective value function right now is: -1677.7955034765048
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-230.16699]
objective value function right now is: -1677.7052508290437
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-232.52615]
objective value function right now is: -1678.0527367073778
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-234.65309]
objective value function right now is: -1677.9316299814839
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-236.83386]
objective value function right now is: -1677.9683363781762
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.5888380829101
Current xi:  [-238.09074]
objective value function right now is: -1678.5888380829101
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-239.39145]
objective value function right now is: -1676.8841457647188
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-240.00133]
objective value function right now is: -1677.83935555676
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-240.15181]
objective value function right now is: -1678.0638382798281
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-240.63145]
objective value function right now is: -1678.4912133144278
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.09303]
objective value function right now is: -1678.328520319477
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-240.79306]
objective value function right now is: -1678.2324462099798
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.5468]
objective value function right now is: -1678.5521995588103
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-241.5862]
objective value function right now is: -1677.676297275162
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-241.36494]
objective value function right now is: -1678.4510119945446
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.75365]
objective value function right now is: -1678.5296896837924
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.4337]
objective value function right now is: -1678.4501721674712
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.10509]
objective value function right now is: -1678.3364846502639
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.43166]
objective value function right now is: -1678.1864552535474
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.61673]
objective value function right now is: -1678.0867946168407
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.56189]
objective value function right now is: -1678.1411586055815
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.7666837952877
Current xi:  [-241.62112]
objective value function right now is: -1678.7666837952877
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.64755]
objective value function right now is: -1678.697385708932
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.780999924014
Current xi:  [-241.48703]
objective value function right now is: -1678.780999924014
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.7945540341066
Current xi:  [-241.44463]
objective value function right now is: -1678.7945540341066
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.30962]
objective value function right now is: -1678.5444136763083
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.18721]
objective value function right now is: -1678.7068511288594
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.26361]
objective value function right now is: -1678.7515898521049
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.2038]
objective value function right now is: -1678.742607083872
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.806700211028
Current xi:  [-241.27315]
objective value function right now is: -1678.806700211028
new min fval from sgd:  -1678.8380638385108
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.31198]
objective value function right now is: -1678.8380638385108
new min fval from sgd:  -1678.8382235387564
new min fval from sgd:  -1678.8437644865212
new min fval from sgd:  -1678.845971634739
new min fval from sgd:  -1678.8541966230991
new min fval from sgd:  -1678.8597402354612
new min fval from sgd:  -1678.8623854276916
new min fval from sgd:  -1678.8786605741382
new min fval from sgd:  -1678.880703007557
new min fval from sgd:  -1678.8843945708825
new min fval from sgd:  -1678.8888544673891
new min fval from sgd:  -1678.8917724923185
new min fval from sgd:  -1678.8918595731536
new min fval from sgd:  -1678.89517740958
new min fval from sgd:  -1678.899152843161
new min fval from sgd:  -1678.8999793103044
new min fval from sgd:  -1678.9008187243705
new min fval from sgd:  -1678.9019015804643
new min fval from sgd:  -1678.9019618951613
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.48378]
objective value function right now is: -1678.807779924469
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.26593]
objective value function right now is: -1678.7612914897243
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.40977]
objective value function right now is: -1678.8570946639134
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.45912]
objective value function right now is: -1678.8798394620117
new min fval from sgd:  -1678.903054206704
new min fval from sgd:  -1678.9044357214655
new min fval from sgd:  -1678.9049036566034
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-241.47005]
objective value function right now is: -1678.8993844383815
min fval:  -1678.9049036566034
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.7547,  1.5011],
        [-0.7547,  1.5011],
        [-9.0234,  8.4485],
        [-0.7547,  1.5011],
        [-0.7547,  1.5011],
        [-0.7547,  1.5011],
        [-0.7547,  1.5011],
        [-0.7547,  1.5011],
        [ 5.0028,  9.4979],
        [-0.7547,  1.5011]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-1.1278, -1.1278, 13.6225, -1.1278, -1.1278, -1.1278, -1.1278, -1.1278,
         9.7794, -1.1278], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-8.7072e-03, -8.7072e-03, -3.4632e-01, -8.7072e-03, -8.7072e-03,
         -8.7072e-03, -8.7070e-03, -8.7072e-03, -5.4617e-01, -8.7072e-03],
        [-8.7072e-03, -8.7072e-03, -3.4632e-01, -8.7071e-03, -8.7072e-03,
         -8.7072e-03, -8.7070e-03, -8.7072e-03, -5.4617e-01, -8.7072e-03],
        [ 2.8803e-01,  2.8803e-01, -7.9615e+00,  2.8803e-01,  2.8803e-01,
          2.8803e-01,  2.8800e-01,  2.8803e-01, -9.9360e+00,  2.8803e-01],
        [-4.4986e-02, -4.4986e-02,  3.8023e+00, -4.4986e-02, -4.4986e-02,
         -4.4986e-02, -4.4992e-02, -4.4986e-02,  4.5611e+00, -4.4986e-02],
        [-8.7072e-03, -8.7072e-03, -3.4632e-01, -8.7071e-03, -8.7072e-03,
         -8.7072e-03, -8.7070e-03, -8.7072e-03, -5.4617e-01, -8.7072e-03],
        [-8.7071e-03, -8.7072e-03, -3.4632e-01, -8.7071e-03, -8.7072e-03,
         -8.7072e-03, -8.7070e-03, -8.7072e-03, -5.4617e-01, -8.7072e-03],
        [-1.0119e-01, -1.0119e-01,  4.4717e+00, -1.0119e-01, -1.0119e-01,
         -1.0119e-01, -1.0120e-01, -1.0119e-01,  5.4129e+00, -1.0119e-01],
        [-1.2789e-01, -1.2789e-01,  5.1497e+00, -1.2789e-01, -1.2789e-01,
         -1.2789e-01, -1.2792e-01, -1.2789e-01,  6.1610e+00, -1.2789e-01],
        [-8.7071e-03, -8.7071e-03, -3.4632e-01, -8.7071e-03, -8.7071e-03,
         -8.7071e-03, -8.7070e-03, -8.7071e-03, -5.4617e-01, -8.7071e-03],
        [-8.7071e-03, -8.7072e-03, -3.4632e-01, -8.7071e-03, -8.7071e-03,
         -8.7071e-03, -8.7070e-03, -8.7071e-03, -5.4617e-01, -8.7071e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9162, -0.9162,  8.6895, -4.2167, -0.9162, -0.9162, -4.8302, -5.5045,
        -0.9162, -0.9162], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-3.3262e-03, -3.3262e-03, -1.4671e+01,  3.9956e+00, -3.3262e-03,
         -3.3262e-03,  5.4179e+00,  6.9562e+00, -3.3263e-03, -3.3262e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 15.8188,   2.1498],
        [ -2.1930,  -0.8806],
        [  1.9334, -16.0462],
        [-11.2783,  -1.4117],
        [-15.4302,  -2.7401],
        [-13.6265,   0.2803],
        [  1.9037,  -4.0017],
        [ -8.3565, -10.3499],
        [-13.0412,   2.8452],
        [ 13.4601,  11.3857]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.6437,  -5.3035, -16.1619,  -1.3299,   1.5421,  12.6302,  -3.0552,
         -9.7924,   8.2395,  11.7640], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ -1.5385,  -0.0969,   6.2621,  -4.7904,  -1.0882,   1.9835,  -1.5789,
          -1.9011,   1.8599,  -1.5028],
        [  0.1791,  -1.1077,  -0.8867,  -1.2336,  -1.0022,   0.4038,   1.8233,
          -0.5600,  -7.1505,  -0.7749],
        [ -1.2830,  -0.3512,  -0.7157,  -0.3515,  -0.3543,  -0.7129,  -1.0564,
          -0.5923,  -0.1328,  -1.1836],
        [ -2.7969,   4.7109,  -0.4599,   4.7826,   2.5341,  -3.2774,  -0.9907,
          -0.3494,  -2.8717,  -1.6164],
        [ -1.2829,  -0.3513,  -0.7157,  -0.3516,  -0.3544,  -0.7128,  -1.0564,
          -0.5922,  -0.1328,  -1.1840],
        [ -6.6539,   0.8123,   5.0930,  -0.3637,  10.1919,  -0.5864,  -2.5546,
           2.7646,   4.3220, -17.4497],
        [ -8.8141,  -1.9533,  12.6181,  -5.9548,   1.5214,   7.8471,  -4.1297,
          -1.3183,   8.9123, -12.5343],
        [ -1.2829,  -0.3513,  -0.7157,  -0.3516,  -0.3544,  -0.7128,  -1.0564,
          -0.5922,  -0.1328,  -1.1840],
        [ -2.6325,   4.5201,  -0.4851,   4.4594,   2.3966,  -3.1292,  -0.9731,
          -0.1783,  -2.6728,  -1.4606],
        [ -0.4053,   9.9173, -10.5833,   6.5345,   3.4628,   3.4959,  -2.0609,
          -0.9235,   1.0925,  -0.9137]], device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-5.1949, -4.0676, -1.8470, -2.4480, -1.8470, -6.6241, -8.7442, -1.8470,
        -2.3663, -1.7761], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.9342e+00, -2.3213e+00,  3.7472e-03, -5.6905e+00,  3.7421e-03,
          8.4818e+00, -3.8592e+00,  3.7422e-03, -3.5517e+00,  7.9993e-01],
        [ 1.7372e+00,  2.3213e+00, -3.7527e-03,  5.6855e+00, -3.7471e-03,
         -8.4062e+00,  3.8673e+00, -3.7471e-03,  3.8502e+00, -9.4156e-01]],
       device='cuda:0'))])
xi:  [-241.46672]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 280.49502670204953
W_T_median: 106.15907683454044
W_T_pctile_5: -241.7547373443734
W_T_CVAR_5_pct: -323.5850657422348
Average q (qsum/M+1):  56.245869298135084
Optimal xi:  [-241.46672]
Expected(across Rb) median(across samples) p_equity:  0.3339695433775584
obj fun:  tensor(-1678.9049, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-2.2247,  4.3209],
        [-2.2240,  4.3268],
        [-6.1086,  9.7295],
        [-2.2554,  4.0741],
        [-2.2525,  4.0967],
        [-2.2227,  4.3381],
        [ 6.7954,  2.8002],
        [-2.2250,  4.3185],
        [ 5.3083, 11.2098],
        [-2.2544,  4.0817]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 0.3963,  0.3968, 10.7253,  0.3753,  0.3772,  0.3978, -5.0732,  0.3960,
        11.5013,  0.3759], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [ -0.5656,  -0.5647,  -8.7616,  -0.6079,  -0.6036,  -0.5630,   7.2620,
          -0.5660, -10.7544,  -0.6064],
        [  0.7350,   0.7343,   5.9283,   0.7708,   0.7672,   0.7328,  -7.4856,
           0.7354,   4.0658,   0.7696],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [  2.2900,   2.2910,   6.5259,   2.2522,   2.2556,   2.2928,  -8.2870,
           2.2896,   5.1739,   2.2533],
        [  3.2348,   3.2364,   6.9482,   3.1717,   3.1774,   3.2393,  -8.8705,
           3.2342,   5.9754,   3.1736],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.4110, -1.4110, 10.7223, -6.0868, -1.4110, -1.4110, -6.8565, -7.6155,
        -1.4110, -1.4110], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0706,   0.0706, -20.7368,   6.3760,   0.0706,   0.0706,   8.9201,
          11.4583,   0.0706,   0.0706]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.5218,   1.7324],
        [ -2.1461,  -2.0257],
        [ -6.8512, -17.2638],
        [-14.5313,   1.7647],
        [-13.4452,  -3.1005],
        [-15.7318,   0.2365],
        [  3.6337,  -5.3431],
        [-11.6388, -11.4033],
        [-10.0472,   1.7281],
        [ 14.6455,  11.8789]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.0086,  -6.5859, -17.8332,   2.9281,  -0.8009,  14.0234,  -7.3603,
        -10.6435,   6.5159,  11.3695], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.0960e+00, -2.9719e+00,  9.1244e+00, -5.7432e+00, -3.2062e+00,
          6.8884e+00, -2.3659e+00,  1.3669e+00, -1.6958e-01, -4.1299e+00],
        [-2.9595e-01, -1.6511e+00,  3.7282e-02, -1.3465e-02, -1.7968e+00,
         -2.0560e+00, -4.6125e-01,  2.2892e-01, -5.2864e-01, -1.5267e+00],
        [ 4.0496e-02, -6.9984e-02,  5.6310e-02,  6.3262e+00, -5.6710e+00,
          4.8562e+00, -8.0886e+00, -3.3964e+00,  7.7433e+00, -5.1245e+00],
        [-4.1321e+00,  7.9852e+00, -8.7048e+00, -4.2872e+00,  5.4975e+00,
         -2.1416e+00, -7.9463e-03,  3.2576e+00,  9.6131e-02,  2.8003e-01],
        [-8.4917e-01, -6.3890e+00, -1.7492e-01, -9.8749e-01,  2.9528e+00,
         -4.2911e-01,  2.6289e+00, -1.3506e+00,  9.9220e-01, -8.4796e-01],
        [-6.9732e+00,  3.8222e-01,  8.0288e+00,  5.5360e-01,  1.0899e+01,
         -7.9086e-03, -1.3856e+00,  5.3801e-01,  1.4334e+00, -2.1538e+01],
        [-1.3112e+01, -8.6230e+00,  1.4464e+01, -7.7538e+00, -9.8681e-01,
          8.7489e+00, -1.5331e-01,  1.6045e+00,  6.5969e+00, -1.6779e+01],
        [-1.8426e-01, -6.0389e+00,  6.9451e-01, -2.1815e+00,  2.7693e+00,
         -2.0308e+00,  1.2657e+00,  4.2083e-01,  3.4479e+00, -1.4204e+00],
        [-3.7185e+00,  5.8697e+00, -1.1276e+00, -1.6690e-01,  4.5050e+00,
         -3.7879e+00, -7.8347e-01,  2.7785e-01, -2.4582e+00, -2.2583e+00],
        [-1.5464e+00,  2.1880e-01, -1.9021e+01,  8.7624e+00,  7.3152e+00,
          2.0446e+00, -9.2952e+00, -1.1962e+01,  1.4433e+00, -4.6725e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -7.2575,  -2.8650,  -7.9039,  -0.6463,  -3.3122,  -8.3409, -12.6841,
         -4.2731,  -1.7224,  -1.6095], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-0.6682,  0.8723, -5.8822, -5.2736,  3.3221,  8.6715, -4.6380,  2.5104,
         -3.1895,  1.8983],
        [ 0.4710, -0.8723,  5.8822,  5.2685, -3.3221, -8.5960,  4.6501, -2.5104,
          3.4879, -2.0341]], device='cuda:0'))])
loaded xi:  -34.98182
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1613.7643594787237
Current xi:  [-34.9913]
objective value function right now is: -1613.7643594787237
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.958847]
objective value function right now is: -1613.4891508009293
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.9975]
objective value function right now is: -1613.3466628167262
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.989597]
objective value function right now is: -1613.0767478134824
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.147396]
objective value function right now is: -1612.0477242593681
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.964073]
objective value function right now is: -1613.2298881337822
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [-34.985645]
objective value function right now is: -1613.4749381460902
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.977985]
objective value function right now is: -1612.9198638359248
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.989727]
objective value function right now is: -1613.3127443931073
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.99453]
objective value function right now is: -1612.8822779386874
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.056023]
objective value function right now is: -1613.7451182989944
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1613.8294236892036
Current xi:  [-35.024612]
objective value function right now is: -1613.8294236892036
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1613.851959090762
Current xi:  [-35.000347]
objective value function right now is: -1613.851959090762
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-34.99271]
objective value function right now is: -1613.020854845088
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1613.94034727064
Current xi:  [-35.018215]
objective value function right now is: -1613.94034727064
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.085323]
objective value function right now is: -1613.8939252394832
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.09247]
objective value function right now is: -1613.4414891792724
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.010864]
objective value function right now is: -1612.563298695742
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1614.4480521747312
Current xi:  [-34.977573]
objective value function right now is: -1614.4480521747312
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.8698]
objective value function right now is: -1613.9786301273468
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.094543]
objective value function right now is: -1612.743451795661
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.986256]
objective value function right now is: -1611.9393629229605
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.931423]
objective value function right now is: -1613.2475814530137
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.295265]
objective value function right now is: -1611.034026002734
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-33.721226]
objective value function right now is: -1611.2700015838384
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.673985]
objective value function right now is: -1605.7760632491027
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-36.160023]
objective value function right now is: -1610.4614591792883
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-34.92718]
objective value function right now is: -1613.4743024106149
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-35.00464]
objective value function right now is: -1612.851175852146
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.943016]
objective value function right now is: -1613.8378785806858
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.974995]
objective value function right now is: -1613.7214963869437
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.979824]
objective value function right now is: -1613.345924257836
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.960518]
objective value function right now is: -1613.1194537784459
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.92607]
objective value function right now is: -1613.97924139543
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.914528]
objective value function right now is: -1612.535533607489
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1614.638903651595
Current xi:  [-34.941788]
objective value function right now is: -1614.638903651595
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1614.7957850419418
Current xi:  [-34.947502]
objective value function right now is: -1614.7957850419418
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.918724]
objective value function right now is: -1614.5535268620247
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.96178]
objective value function right now is: -1614.561219439489
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.95492]
objective value function right now is: -1614.750005839945
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.95983]
objective value function right now is: -1614.6153642742775
84.0% of gradient descent iterations done. Method = Adam
new min fval:  -1614.9087553425343
Current xi:  [-34.93915]
objective value function right now is: -1614.9087553425343
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.92926]
objective value function right now is: -1614.7426290098042
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.963882]
objective value function right now is: -1614.8142521577772
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.904793]
objective value function right now is: -1614.3164744901512
new min fval from sgd:  -1614.9140049828236
new min fval from sgd:  -1614.9214553377672
new min fval from sgd:  -1614.9233272854756
new min fval from sgd:  -1614.9395670585534
new min fval from sgd:  -1614.9510717822775
new min fval from sgd:  -1614.956292260617
new min fval from sgd:  -1614.9670364864853
new min fval from sgd:  -1614.9694267524599
new min fval from sgd:  -1614.9862974315115
new min fval from sgd:  -1614.9950496200404
new min fval from sgd:  -1615.0142952557399
new min fval from sgd:  -1615.0313048457733
new min fval from sgd:  -1615.037058964193
new min fval from sgd:  -1615.0417380119147
new min fval from sgd:  -1615.0423554236113
new min fval from sgd:  -1615.0562194447039
new min fval from sgd:  -1615.0585667305863
new min fval from sgd:  -1615.0631334432792
new min fval from sgd:  -1615.0646337685678
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.967674]
objective value function right now is: -1612.7733167219824
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.92596]
objective value function right now is: -1614.9859694653892
new min fval from sgd:  -1615.0707697540975
new min fval from sgd:  -1615.074991191112
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.9916]
objective value function right now is: -1614.4750998753118
new min fval from sgd:  -1615.0756867189505
new min fval from sgd:  -1615.0800955259997
new min fval from sgd:  -1615.0851950561498
new min fval from sgd:  -1615.088707758217
new min fval from sgd:  -1615.0915210258213
new min fval from sgd:  -1615.095289088257
new min fval from sgd:  -1615.0974966189815
new min fval from sgd:  -1615.0986284819455
new min fval from sgd:  -1615.099716782739
new min fval from sgd:  -1615.1005890407619
new min fval from sgd:  -1615.10138616047
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.968876]
objective value function right now is: -1615.0158907188531
new min fval from sgd:  -1615.1045694416077
new min fval from sgd:  -1615.1150889927312
new min fval from sgd:  -1615.1243327119892
new min fval from sgd:  -1615.129162588259
new min fval from sgd:  -1615.1313390676664
new min fval from sgd:  -1615.1349471058595
new min fval from sgd:  -1615.1360229264715
new min fval from sgd:  -1615.1374275336743
new min fval from sgd:  -1615.1386689057886
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.963036]
objective value function right now is: -1615.1062948775475
min fval:  -1615.1386689057886
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-3.6733,  6.9760],
        [-3.6687,  6.9892],
        [-6.8499,  9.7034],
        [-3.8797,  6.3713],
        [-3.8604,  6.4294],
        [-3.6600,  7.0143],
        [ 7.7546,  2.2440],
        [-3.6752,  6.9705],
        [ 6.6046, 12.0485],
        [-3.8732,  6.3907]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 1.0666,  1.0668, 10.5463,  1.0453,  1.0476,  1.0671, -6.0731,  1.0665,
        11.4637,  1.0460], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -1.5697,  -1.5701,  -2.0700,  -1.5459,  -1.5483,  -1.5709,  -1.9548,
          -1.5695,  -0.3746,  -1.5467],
        [ -1.5689,  -1.5693,  -2.0695,  -1.5451,  -1.5475,  -1.5700,  -1.9542,
          -1.5687,  -0.3741,  -1.5459],
        [ -2.3660,  -2.3694, -10.3529,  -2.2057,  -2.2209,  -2.3758,   7.4783,
          -2.3646, -10.3319,  -2.2108],
        [  1.4280,   1.4276,   7.7337,   1.4454,   1.4436,   1.4268,  -6.7570,
           1.4282,   4.7555,   1.4447],
        [ -1.5699,  -1.5704,  -2.0702,  -1.5462,  -1.5486,  -1.5711,  -1.9549,
          -1.5698,  -0.3747,  -1.5470],
        [ -1.5695,  -1.5699,  -2.0699,  -1.5458,  -1.5481,  -1.5707,  -1.9547,
          -1.5694,  -0.3745,  -1.5465],
        [  2.8677,   2.8748,   8.9455,   2.5766,   2.6023,   2.8886,  -7.6571,
           2.8647,   6.6454,   2.5851],
        [  3.9667,   3.9690,   6.4122,   3.8902,   3.8957,   3.9735, -11.8420,
           3.9657,   4.6778,   3.8920],
        [ -1.5688,  -1.5692,  -2.0694,  -1.5450,  -1.5474,  -1.5700,  -1.9542,
          -1.5687,  -0.3741,  -1.5458],
        [ -1.5693,  -1.5697,  -2.0698,  -1.5456,  -1.5479,  -1.5705,  -1.9545,
          -1.5692,  -0.3744,  -1.5463]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([ 1.3356,  1.3362, 10.4226, -5.7861,  1.3354,  1.3357, -5.5098, -8.0120,
         1.3362,  1.3359], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -2.3452,  -2.3456, -19.4387,   8.1020,  -2.3451,  -2.3453,  16.1817,
          11.1697,  -2.3456,  -2.3454]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.2902,   1.7101],
        [ -3.1460,  -1.9136],
        [ -5.9346, -18.1745],
        [-16.1503,   1.4928],
        [-12.4021,  -3.2926],
        [-15.9970,   0.9176],
        [  5.4149,  -5.1802],
        [-11.8666, -12.1471],
        [-11.7408,   1.3859],
        [ 16.1628,  11.8068]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -9.6558,  -7.6127, -18.5862,   3.2371,  -1.6008,  14.8368,  -8.1537,
        -10.8318,   6.1822,  10.9351], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-5.2466e+00, -4.2380e+00,  8.8434e+00, -2.3455e+00, -4.0574e+00,
          6.3640e+00, -2.1175e+00,  3.7623e+00, -2.1605e+00, -5.8732e+00],
        [-1.1707e+00, -6.0117e-01, -7.3373e-01, -2.0926e-02, -6.1992e-01,
         -7.5004e-01, -1.3146e+00, -6.2872e-01, -2.3091e-01, -1.7920e+00],
        [-3.9256e-01, -1.1678e-02, -1.8375e+00,  3.4202e-01, -1.0244e+00,
          1.0815e+00, -9.2474e-01, -8.9409e-01,  2.2363e+00, -2.6489e+00],
        [-3.2209e+00,  9.7909e+00, -1.0707e+01, -4.7115e+00,  4.3637e+00,
         -1.7062e+00, -6.7447e-01,  5.2267e+00,  1.4767e+00, -9.9246e-01],
        [-2.9183e-01, -7.3615e+00,  1.2518e-01, -3.7788e-01,  3.8886e+00,
         -1.1126e-01,  2.2823e+00, -1.5048e+00,  1.5487e+00, -8.2963e-01],
        [-8.8281e+00,  6.5036e-02,  8.2468e+00, -3.1044e+00,  1.1304e+01,
          7.5966e-01, -1.4886e+00,  7.4186e-01,  5.2504e+00, -2.2893e+01],
        [-1.2361e+01, -1.1373e+01,  1.4851e+01, -6.9268e+00,  3.5528e-01,
          9.4411e+00,  2.3187e-01,  2.6563e+00,  5.6496e+00, -1.6891e+01],
        [ 1.3734e-01, -4.6360e+00,  1.2221e+00,  6.9939e-01, -2.3584e-01,
         -4.0178e+00,  2.4895e-01,  1.1870e+00,  2.5994e+00,  1.4895e-01],
        [-3.6682e+00,  5.5311e+00, -1.7486e+00,  1.8802e+00,  6.1052e+00,
         -3.4410e+00, -5.8522e-01,  1.0816e-01, -2.1400e+00, -2.1753e+00],
        [-1.0921e+00,  1.2784e-01, -2.4690e+01,  5.7431e+00,  9.1492e+00,
          3.5896e+00, -9.5146e+00, -1.1114e+01,  1.4887e+00, -9.0314e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -8.7256,  -2.6431,  -4.8203,  -0.4532,  -3.8030,  -9.5792, -13.6000,
         -5.0348,  -2.0937,  -2.3085], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-1.0235, -0.0307, -0.5608, -5.2392,  2.9252,  8.7375, -4.6830,  1.9898,
         -3.2985,  1.3110],
        [ 0.8317,  0.0307,  0.5607,  5.2341, -2.9252, -8.6624,  4.6933, -1.9900,
          3.5954, -1.4414]], device='cuda:0'))])
xi:  [-34.958042]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 294.29711469122515
W_T_median: 127.02849058252404
W_T_pctile_5: -34.96718173669499
W_T_CVAR_5_pct: -136.2286346218026
Average q (qsum/M+1):  54.298481602822584
Optimal xi:  [-34.958042]
Expected(across Rb) median(across samples) p_equity:  0.3027287165323893
obj fun:  tensor(-1615.1387, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.9022,  2.1997],
        [-1.9027,  2.2008],
        [-2.5359, 11.3012],
        [-1.8886,  2.1637],
        [-1.8893,  2.1661],
        [-1.9038,  2.2030],
        [ 9.4669, -1.8947],
        [-1.9020,  2.1992],
        [35.0777, 11.0529],
        [-1.8888,  2.1644]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-3.9099, -3.9084, 10.2351, -3.9580, -3.9548, -3.9054, -7.6292, -3.9106,
        11.8060, -3.9570], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [ 0.3042,  0.3058, -8.5943,  0.2453,  0.2499,  0.3090,  9.7746,  0.3035,
         -9.4847,  0.2468],
        [-0.0459, -0.0461,  4.1241, -0.0414, -0.0417, -0.0464, -4.8504, -0.0459,
          3.0107, -0.0415],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [-0.0452, -0.0448,  7.3092, -0.0621, -0.0604, -0.0441, -8.0983, -0.0454,
          5.0301, -0.0615],
        [ 0.1863,  0.1879,  8.1710,  0.1243,  0.1296,  0.1910, -8.9808,  0.1857,
          5.5552,  0.1261],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.8905, -1.8905, 12.2159, -7.6068, -1.8905, -1.8905, -7.5374, -8.3188,
        -1.8905, -1.8905], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.0837e-02, -2.0837e-02, -2.3517e+01,  3.7147e+00, -2.0837e-02,
         -2.0837e-02,  8.4533e+00,  1.1297e+01, -2.0837e-02, -2.0837e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 18.1543,   1.8292],
        [ -2.4963,   0.3935],
        [ -8.9401, -18.6446],
        [-17.8212,   2.2503],
        [-12.9877,  -4.5806],
        [-17.9568,  -0.0681],
        [  3.7617,  -6.9727],
        [-14.2883,  -9.6505],
        [-10.6075,   2.0204],
        [ 14.0266,  12.6445]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.7796,  -5.1752, -17.3002,   3.2370,  -3.7983,  14.2039,  -7.1168,
        -12.1026,   6.0532,  11.8976], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8406e+00, -5.0604e-05,  9.2377e+00, -1.2129e+01,  2.6529e-01,
          7.5686e+00, -1.3486e+00,  2.8181e-02,  5.0669e-01, -4.5431e+00],
        [-5.6833e-01, -3.1932e-01,  2.1166e+00, -2.7407e-01,  1.9631e+00,
         -6.3949e+00, -4.3871e-01,  4.6075e-01, -2.7837e+00, -9.8641e-01],
        [-3.0841e-01,  1.6513e-01, -8.2044e+00,  7.2278e+00, -5.8960e+00,
          6.5574e+00, -1.7555e+01, -2.4367e+00,  8.9103e+00, -6.3373e+00],
        [-3.3190e+00, -2.4528e-03, -3.5481e-01, -1.5983e-01, -3.2941e-02,
         -1.8793e+00, -1.1023e+00,  1.3698e-01, -5.6799e-01, -2.1772e+00],
        [-7.7940e-02, -1.9193e-01, -1.0964e+00,  9.2041e-02,  3.0211e+00,
         -3.6099e-01,  9.7791e-01,  4.3726e-01,  1.6731e-01,  6.6685e-03],
        [-1.0566e+01, -4.3523e-02,  7.8878e+00, -2.0272e+00,  1.1324e+01,
         -6.2711e-01, -4.3568e-01,  2.7229e+00,  4.6671e+00, -2.5642e+01],
        [-1.4986e+01, -9.2645e-03,  1.4251e+01, -5.3074e+00,  2.7459e+00,
          8.9358e+00,  1.5125e+00,  1.7465e+00,  5.1019e+00, -1.6309e+01],
        [-1.7331e+00, -5.9463e-04,  9.3456e-02,  7.4209e-02,  2.0586e-01,
         -2.6298e+00,  9.9453e-01, -2.5439e-01, -1.6643e-01, -1.3235e+00],
        [-2.7923e+00,  2.4169e-03, -2.6580e-01, -1.7699e-01, -1.0988e-01,
         -1.4099e+00, -9.9616e-01,  2.9782e-01, -5.3087e-01, -1.8743e+00],
        [-2.1056e+00,  2.7644e-01, -6.3773e+00,  4.0284e+00,  1.2414e+00,
          4.4436e+00, -1.4050e+01,  1.1258e+00,  1.5008e+00, -1.4302e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -7.8213,  -4.0747,  -9.6565,  -2.8734,  -2.6047,  -8.8432, -13.9188,
         -5.4919,  -3.6199,  -1.2448], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.6687,   1.1627,  -4.6536,   0.3585,   2.6883,  12.7566,  -5.4592,
           1.4591,   0.4631,   0.4319],
        [  0.4730,  -1.1627,   4.6537,  -0.3589,  -2.6882, -12.7178,   5.4710,
          -1.4592,  -0.4039,  -0.5650]], device='cuda:0'))])
loaded xi:  84.25147
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1572.667118667343
Current xi:  [83.712]
objective value function right now is: -1572.667118667343
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1574.7677974020917
Current xi:  [82.404274]
objective value function right now is: -1574.7677974020917
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [81.47352]
objective value function right now is: -1573.7375107417768
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [80.861534]
objective value function right now is: -1570.6288396561404
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [80.225975]
objective value function right now is: -1569.6026204020948
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [79.56657]
objective value function right now is: -1573.1413897217965
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [78.78405]
objective value function right now is: -1573.5040174414903
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [78.053665]
objective value function right now is: -1572.5276357163366
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [77.49287]
objective value function right now is: -1574.5739047346585
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [76.779076]
objective value function right now is: -1572.4358714759308
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [76.297295]
objective value function right now is: -1574.016888117833
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1575.1100262792838
Current xi:  [75.595314]
objective value function right now is: -1575.1100262792838
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [73.550964]
objective value function right now is: -1529.1994394520218
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [69.39384]
objective value function right now is: -1531.1592312744356
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [66.02174]
objective value function right now is: -1546.797743732506
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [63.18773]
objective value function right now is: -1548.7051734006795
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [60.758015]
objective value function right now is: -1549.5833067754193
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [58.33416]
objective value function right now is: -1548.4452021466502
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.97866]
objective value function right now is: -1549.2506920925389
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [54.09619]
objective value function right now is: -1548.5944198295906
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.290997]
objective value function right now is: -1550.2124912734841
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.75588]
objective value function right now is: -1550.6777025079361
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [49.196648]
objective value function right now is: -1549.6061618181272
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [47.56437]
objective value function right now is: -1548.251247965069
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [46.253452]
objective value function right now is: -1557.5411528793059
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.0638]
objective value function right now is: -1556.49078573968
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.34387]
objective value function right now is: -1568.2054371737029
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [46.032585]
objective value function right now is: -1571.2961116373244
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [46.808605]
objective value function right now is: -1569.1806731980587
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [47.467724]
objective value function right now is: -1570.6267331003785
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [48.21814]
objective value function right now is: -1571.596087888396
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [49.034077]
objective value function right now is: -1571.3740577750764
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [49.400963]
objective value function right now is: -1569.5066638921994
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.12591]
objective value function right now is: -1572.9856585102766
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.720695]
objective value function right now is: -1572.3306328796186
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [50.875652]
objective value function right now is: -1574.7354238379776
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.07191]
objective value function right now is: -1574.5650025881926
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.30644]
objective value function right now is: -1574.3937021727638
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.454132]
objective value function right now is: -1575.0784876453006
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.600327]
objective value function right now is: -1575.05145483913
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.78157]
objective value function right now is: -1574.7144102922512
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.993877]
objective value function right now is: -1574.9484249358425
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.16289]
objective value function right now is: -1574.8675226296048
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.313503]
objective value function right now is: -1574.8519293506215
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.47759]
objective value function right now is: -1574.945200068031
new min fval from sgd:  -1575.1139253559948
new min fval from sgd:  -1575.1150201428482
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.657803]
objective value function right now is: -1574.4767587178071
new min fval from sgd:  -1575.1339453341147
new min fval from sgd:  -1575.1669356497111
new min fval from sgd:  -1575.2318541103798
new min fval from sgd:  -1575.2524065259179
new min fval from sgd:  -1575.3106410802352
new min fval from sgd:  -1575.3190998176156
new min fval from sgd:  -1575.339921670564
new min fval from sgd:  -1575.3788574076113
new min fval from sgd:  -1575.3814630192658
new min fval from sgd:  -1575.3856069083517
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [52.798714]
objective value function right now is: -1574.7984878775997
new min fval from sgd:  -1575.400633882307
new min fval from sgd:  -1575.407058089351
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.059933]
objective value function right now is: -1574.0954782537328
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.18671]
objective value function right now is: -1569.4211933959402
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [53.181072]
objective value function right now is: -1571.399601827983
min fval:  -1575.407058089351
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.1782,  1.1679],
        [-1.1755,  1.1658],
        [-1.2865, 11.4851],
        [-1.1737,  1.1749],
        [-1.1740,  1.1747],
        [ 0.2333,  8.4279],
        [11.4307, -3.0828],
        [-1.1791,  1.1686],
        [51.1609, 11.0161],
        [-1.1738,  1.1748]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-3.0899, -3.1047, 10.1147, -3.0689, -3.0687, -3.9139, -9.1845, -3.0857,
        12.0260, -3.0688], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0229, -0.0229, -0.5237, -0.0229, -0.0229, -0.1009, -0.0677, -0.0229,
         -1.0387, -0.0229],
        [-0.0229, -0.0229, -0.5237, -0.0229, -0.0229, -0.1009, -0.0677, -0.0229,
         -1.0387, -0.0229],
        [ 0.0932,  0.0939, -9.9377,  0.0753,  0.0760,  2.7650,  9.6589,  0.0930,
         -9.3327,  0.0755],
        [-0.0186, -0.0186, -0.5255, -0.0188, -0.0188, -0.0971, -0.0746, -0.0187,
         -1.0359, -0.0188],
        [-0.0229, -0.0229, -0.5237, -0.0229, -0.0229, -0.1009, -0.0677, -0.0229,
         -1.0387, -0.0229],
        [-0.0229, -0.0229, -0.5237, -0.0229, -0.0229, -0.1009, -0.0677, -0.0229,
         -1.0387, -0.0229],
        [ 0.1245,  0.1267,  8.9527,  0.0918,  0.0923, -2.4231, -8.1496,  0.1237,
          5.0728,  0.0919],
        [ 0.1155,  0.1151,  9.8816,  0.0988,  0.0998, -2.6780, -9.2569,  0.1156,
          5.6117,  0.0991],
        [-0.0229, -0.0229, -0.5237, -0.0229, -0.0229, -0.1009, -0.0677, -0.0229,
         -1.0387, -0.0229],
        [-0.0229, -0.0229, -0.5237, -0.0229, -0.0229, -0.1009, -0.0677, -0.0229,
         -1.0387, -0.0229]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.1619, -1.1619, 12.3229, -1.2396, -1.1619, -1.1619, -7.2843, -8.0424,
        -1.1619, -1.1619], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.1346e-02, -1.1346e-02, -2.3106e+01, -1.7027e-02, -1.1346e-02,
         -1.1346e-02,  8.0197e+00,  1.1086e+01, -1.1346e-02, -1.1346e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.9843,   2.7583],
        [ -2.1407,   0.0797],
        [-15.6645, -17.8329],
        [-16.1201,  -1.6406],
        [-13.8635,  -4.9907],
        [-17.8848,   0.4473],
        [ -0.0202,  -1.7303],
        [-15.5357,  -7.9707],
        [ -2.0061,   9.1601],
        [ 16.7932,   9.9673]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -9.2361,  -4.8038, -17.3677,   7.9160,  -2.7580,  14.1947, -10.7229,
        -12.9636,   9.1621,  14.1272], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.4340e+00, -1.3297e-02,  7.4515e+00, -2.6086e+00,  1.9023e-01,
          4.6657e+00, -1.2640e+00,  5.8962e-01,  4.3945e-03, -6.3416e+00],
        [-1.7929e+00, -3.7322e-04,  5.5858e-01, -1.8950e-01, -1.3110e-01,
         -3.3744e+00, -4.6960e-01,  7.8005e-02, -2.1086e-02, -1.0088e+00],
        [ 3.0896e-01,  1.4555e-01,  1.3976e+00,  1.2994e+00,  5.4678e+00,
          6.3448e-01, -9.8751e-02,  8.4754e-01,  1.5397e+00, -3.6209e+00],
        [-1.6492e+00, -4.9271e-04, -7.8407e-01,  2.7532e-02, -1.0395e-01,
         -1.4850e+00, -6.6533e-03,  1.4894e-01, -1.2611e+00, -2.8873e+00],
        [ 2.5496e-01,  1.1423e-01, -2.4175e-01, -7.4675e+00, -4.7766e-01,
         -2.3030e-01,  3.4286e-01, -2.3477e+00, -8.4779e-01,  7.3511e-01],
        [-6.9450e+00, -2.6698e-06,  8.2689e+00, -2.3372e+00,  7.7382e+00,
         -1.3105e-01,  3.5761e-01,  2.1748e+00,  5.4522e-02, -2.3618e+01],
        [-8.2936e+00,  5.8653e-07,  1.2352e+01, -2.1159e+00,  3.4406e-03,
          5.2866e+00,  8.9230e-01,  2.2967e+00,  9.1632e-01, -1.8521e+01],
        [-3.1689e+00, -5.3437e-02,  4.4123e+00,  3.6999e+00,  9.4598e+00,
          1.8706e+00,  7.2303e-01,  2.0667e+00, -1.4688e+01, -3.3721e+00],
        [-3.7465e+00, -3.6721e-01,  6.0800e+00,  6.8949e+00,  2.4834e+01,
         -4.8139e+00, -8.7860e-01,  3.0984e+00, -1.8339e+01, -1.2148e+01],
        [-1.9033e+00,  1.5567e-01, -2.6517e+01, -1.1555e+00,  9.7076e+00,
          3.9650e+00, -3.2664e-01, -1.7466e-01, -3.0512e-01,  1.4243e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -8.1589,  -5.6256,  -4.7901,  -2.7704,  -2.5993,  -8.2419, -15.0443,
         -4.6613,  -9.0365,  -1.0102], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  0.5734,   0.3885,   1.6328,   0.6679,   2.1521,  10.7333,  -5.1302,
          -4.7430,   9.1697,   0.2517],
        [ -0.7620,  -0.3885,  -1.6328,  -0.6680,  -2.1520, -10.7059,   5.1404,
           4.7429,  -9.1597,  -0.3841]], device='cuda:0'))])
xi:  [53.01862]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 625.8841020034462
W_T_median: 277.20592170306566
W_T_pctile_5: 53.066175567459304
W_T_CVAR_5_pct: -43.08153617298363
Average q (qsum/M+1):  52.20951203377016
Optimal xi:  [53.01862]
Expected(across Rb) median(across samples) p_equity:  0.3630886644124985
obj fun:  tensor(-1575.4071, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.3135,  0.7434],
        [-1.3135,  0.7434],
        [-3.6036, 12.0503],
        [-1.3135,  0.7434],
        [-1.3135,  0.7434],
        [ 2.1665,  5.3625],
        [14.8901, -3.0004],
        [-1.3135,  0.7434],
        [41.0310, 11.6400],
        [-1.3135,  0.7434]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.1356,  -3.1357,   9.6844,  -3.1356,  -3.1356,  -7.8763, -10.4544,
         -3.1356,  11.9602,  -3.1356], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-1.1860e-02, -1.1826e-02, -1.2484e+01, -1.2008e-02, -1.2000e-02,
          5.8568e+00,  1.2112e+01, -1.1867e-02, -9.5974e+00, -1.2005e-02],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-1.5016e-01, -1.5015e-01,  1.1306e+01, -1.5020e-01, -1.5020e-01,
         -4.5917e+00, -1.0061e+01, -1.5016e-01,  4.8845e+00, -1.5020e-01],
        [-7.7669e-02, -7.7614e-02,  1.2175e+01, -7.7911e-02, -7.7897e-02,
         -5.6226e+00, -1.1451e+01, -7.7680e-02,  5.5440e+00, -7.7906e-02],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9735, -0.9735, 11.9834, -0.9735, -0.9735, -0.9735, -6.9804, -7.8026,
        -0.9735, -0.9735], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.4353e-03, -4.4353e-03, -2.2854e+01, -4.4353e-03, -4.4353e-03,
         -4.4353e-03,  7.5316e+00,  1.0955e+01, -4.4353e-03, -4.4353e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.8755,  -0.1131],
        [  2.2070,  -2.9190],
        [-12.7398, -19.1529],
        [-20.8026,   2.6402],
        [-14.8786,  -5.8655],
        [-20.8819,  -0.4727],
        [ -1.6507,  -4.2140],
        [ -6.4622,  -2.9494],
        [-16.0097,  -0.3170],
        [ 19.5946,  11.0485]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.4815,  -4.6727, -16.7637,   4.1533,  -2.0845,  15.4023, -10.0394,
         -8.8638,   7.3508,  10.3669], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.9834e-01,  5.4517e+00,  7.6134e+00, -7.4001e+00,  3.2587e+00,
          6.7567e+00, -8.7228e-01,  1.6534e-01,  4.2096e+00, -9.0514e+00],
        [ 2.5227e-02, -5.3127e+00,  1.6561e+00, -7.3609e-01, -1.7339e-01,
         -7.0294e+00, -3.8729e-01, -6.1912e-03, -2.4455e-01,  8.4616e-01],
        [ 2.9578e-04, -8.0853e-01, -9.2585e+00,  1.6414e+01, -1.0574e+01,
          6.6998e+00, -6.4838e-02, -2.8747e-02,  3.6350e+00, -5.0883e+00],
        [-6.5285e-03, -1.4741e+00, -7.4034e-01, -3.3373e-02, -6.6340e-02,
         -1.7186e+00, -2.0529e-02,  1.2917e-05, -1.4412e-01, -3.2719e+00],
        [-1.0602e-01, -1.0111e+00, -6.9635e+00,  8.3494e+00,  7.0579e+00,
         -1.2499e+00, -7.1254e-02,  1.3946e-01, -1.3285e+00, -2.7928e+00],
        [-4.8657e-02,  1.3083e-01,  4.2694e+00, -8.8046e+00,  7.0404e+00,
         -3.9656e-02, -5.1103e-01,  3.4600e+00,  1.6739e+01, -2.5370e+01],
        [-6.9055e-02,  7.4749e+00,  1.4589e+01, -7.7810e+00,  6.7638e+00,
          1.0506e+01, -9.9878e-01,  2.7763e-01,  4.9551e+00, -1.7466e+01],
        [ 4.0192e-02,  3.9246e+00,  5.9270e-01, -3.2575e-01, -4.0014e+00,
         -9.5712e+00, -1.5409e+00,  5.2173e-02, -5.6609e-01, -3.2432e+00],
        [ 2.1377e-02,  4.8526e+00,  7.1024e-01, -3.6780e-01, -5.5796e+00,
         -8.1754e+00, -2.9666e+00, -2.0890e-02,  4.4906e-01, -2.1805e+00],
        [ 1.8129e+00,  6.0915e-01, -9.5672e+00,  8.2498e+00,  2.8312e+00,
          5.4126e-02, -7.4598e+00, -1.0013e-01,  6.1623e-01,  4.9794e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -9.3533,  -4.7132,  -9.6718,  -4.4008,  -4.4709, -12.2256, -16.4986,
         -4.0382,  -4.4854,  -0.2981], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.6295,  -0.7010,  -3.9059,  -0.3621,   0.8007,  12.5427,  -5.9097,
           2.2696,   1.9996,   0.4451],
        [  0.4385,   0.7010,   3.9059,   0.3621,  -0.8007, -12.5331,   5.9200,
          -2.2696,  -1.9994,  -0.5734]], device='cuda:0'))])
loaded xi:  148.70326
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1559.8398131944982
Current xi:  [147.42673]
objective value function right now is: -1559.8398131944982
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.4780137350538
Current xi:  [145.85284]
objective value function right now is: -1562.4780137350538
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [144.21428]
objective value function right now is: -1557.965439179902
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [142.16302]
objective value function right now is: -1562.4006297693209
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [140.10727]
objective value function right now is: -1555.5234393763121
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [138.49681]
objective value function right now is: -1561.4158450216362
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [137.3904]
objective value function right now is: -1550.030880748985
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [136.2037]
objective value function right now is: -1561.1465701150698
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1562.514345351921
Current xi:  [135.26456]
objective value function right now is: -1562.514345351921
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [133.8822]
objective value function right now is: -1562.5057083653373
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [133.00569]
objective value function right now is: -1559.9783720958735
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [132.3823]
objective value function right now is: -1558.2887113834354
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [131.26794]
objective value function right now is: -1561.9632377338862
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [130.64685]
objective value function right now is: -1559.8907496445258
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [129.94147]
objective value function right now is: -1559.044953089322
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [129.0444]
objective value function right now is: -1549.8544836399612
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [128.84624]
objective value function right now is: -1562.2085057274169
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [128.13069]
objective value function right now is: -1557.6384459512292
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.3805531084424
Current xi:  [127.96577]
objective value function right now is: -1563.3805531084424
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [128.1336]
objective value function right now is: -1559.0422234323194
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [127.592476]
objective value function right now is: -1556.3874928950252
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1563.8083517402713
Current xi:  [126.48674]
objective value function right now is: -1563.8083517402713
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [126.02194]
objective value function right now is: -1560.4964575542342
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [125.52943]
objective value function right now is: -1562.2618547513919
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [125.16002]
objective value function right now is: -1562.7130009362484
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.869125]
objective value function right now is: -1559.0019108441063
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.94451]
objective value function right now is: -1549.014329882782
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [124.135124]
objective value function right now is: -1556.6008986708434
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [124.36965]
objective value function right now is: -1559.430243452483
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.510796]
objective value function right now is: -1560.6078316586177
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [125.003235]
objective value function right now is: -1561.567995293878
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.990364]
objective value function right now is: -1562.3696977708619
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.60601]
objective value function right now is: -1548.5927933987816
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.00588]
objective value function right now is: -1563.789953246691
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.26183]
objective value function right now is: -1560.5314803259137
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1564.4616610535575
Current xi:  [124.29219]
objective value function right now is: -1564.4616610535575
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1565.8283957427848
Current xi:  [124.34212]
objective value function right now is: -1565.8283957427848
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.123811603653
Current xi:  [124.28516]
objective value function right now is: -1566.123811603653
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1566.181129848269
Current xi:  [124.27403]
objective value function right now is: -1566.181129848269
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.39981]
objective value function right now is: -1565.3444822388883
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.384415]
objective value function right now is: -1565.708242810041
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.45175]
objective value function right now is: -1566.1301245605534
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.38418]
objective value function right now is: -1566.0411441563242
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.425415]
objective value function right now is: -1565.3127224057882
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.40141]
objective value function right now is: -1564.4603933162218
new min fval from sgd:  -1566.2270459141955
new min fval from sgd:  -1566.3095712618097
new min fval from sgd:  -1566.428228924982
new min fval from sgd:  -1566.4496363833414
new min fval from sgd:  -1566.470381625148
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.40811]
objective value function right now is: -1566.002343000136
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.532104]
objective value function right now is: -1565.5062914494765
new min fval from sgd:  -1566.484855698865
new min fval from sgd:  -1566.5391173995347
new min fval from sgd:  -1566.5977868961502
new min fval from sgd:  -1566.6303201096734
new min fval from sgd:  -1566.6388943146408
new min fval from sgd:  -1566.6489828183535
new min fval from sgd:  -1566.6899031191856
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.58124]
objective value function right now is: -1566.427417740237
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.56841]
objective value function right now is: -1566.6016583208368
new min fval from sgd:  -1566.6918805112025
new min fval from sgd:  -1566.6985432269414
new min fval from sgd:  -1566.7001464711927
new min fval from sgd:  -1566.7035498271669
new min fval from sgd:  -1566.7069827230105
new min fval from sgd:  -1566.7141143208692
new min fval from sgd:  -1566.7217228387055
new min fval from sgd:  -1566.7280302456377
new min fval from sgd:  -1566.7367106749552
new min fval from sgd:  -1566.7460898760346
new min fval from sgd:  -1566.7534598372467
new min fval from sgd:  -1566.7634778775018
new min fval from sgd:  -1566.7772021077521
new min fval from sgd:  -1566.7867130166485
new min fval from sgd:  -1566.7974987073783
new min fval from sgd:  -1566.797719756737
new min fval from sgd:  -1566.7993171883472
new min fval from sgd:  -1566.8050955147278
new min fval from sgd:  -1566.8097099306374
new min fval from sgd:  -1566.8104076629481
new min fval from sgd:  -1566.8109418695103
new min fval from sgd:  -1566.8192032133065
new min fval from sgd:  -1566.8273262339603
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [124.54598]
objective value function right now is: -1566.7539986772247
min fval:  -1566.8273262339603
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.1399,  0.8723],
        [-1.1399,  0.8723],
        [-3.6026, 12.1487],
        [-1.1399,  0.8723],
        [-1.1399,  0.8723],
        [ 3.3460,  6.2014],
        [15.6437, -3.5683],
        [-1.1399,  0.8723],
        [47.9822, 11.8561],
        [-1.1399,  0.8723]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.2796,  -3.2796,   9.8453,  -3.2796,  -3.2796,  -8.3126, -11.3723,
         -3.2796,  11.8934,  -3.2796], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.0916e-03, -6.0916e-03, -5.1690e-01, -6.0916e-03, -6.0916e-03,
         -8.0279e-03, -2.0244e-01, -6.0916e-03, -1.0234e+00, -6.0916e-03],
        [-6.0916e-03, -6.0916e-03, -5.1689e-01, -6.0916e-03, -6.0916e-03,
         -8.0279e-03, -2.0244e-01, -6.0916e-03, -1.0234e+00, -6.0916e-03],
        [ 1.6762e-02,  1.6764e-02, -1.2779e+01,  1.6751e-02,  1.6752e-02,
          7.1923e+00,  1.2833e+01,  1.6761e-02, -9.2551e+00,  1.6751e-02],
        [-6.0916e-03, -6.0916e-03, -5.1689e-01, -6.0916e-03, -6.0916e-03,
         -8.0280e-03, -2.0244e-01, -6.0916e-03, -1.0234e+00, -6.0916e-03],
        [-6.0916e-03, -6.0916e-03, -5.1690e-01, -6.0916e-03, -6.0916e-03,
         -8.0279e-03, -2.0244e-01, -6.0916e-03, -1.0234e+00, -6.0916e-03],
        [-6.0916e-03, -6.0916e-03, -5.1690e-01, -6.0916e-03, -6.0916e-03,
         -8.0281e-03, -2.0244e-01, -6.0916e-03, -1.0234e+00, -6.0916e-03],
        [-5.4630e-02, -5.4629e-02,  1.2186e+01, -5.4632e-02, -5.4632e-02,
         -4.8762e+00, -1.0529e+01, -5.4630e-02,  4.6673e+00, -5.4632e-02],
        [-9.5678e-03, -9.5602e-03,  1.3233e+01, -9.5998e-03, -9.5980e-03,
         -6.1310e+00, -1.1993e+01, -9.5689e-03,  5.2952e+00, -9.5988e-03],
        [-6.0916e-03, -6.0916e-03, -5.1689e-01, -6.0916e-03, -6.0916e-03,
         -8.0278e-03, -2.0244e-01, -6.0916e-03, -1.0234e+00, -6.0916e-03],
        [-6.0916e-03, -6.0916e-03, -5.1689e-01, -6.0916e-03, -6.0916e-03,
         -8.0278e-03, -2.0244e-01, -6.0916e-03, -1.0234e+00, -6.0916e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.1640, -1.1640, 12.3229, -1.1640, -1.1640, -1.1640, -6.7630, -7.6705,
        -1.1640, -1.1640], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-3.6893e-03, -3.6893e-03, -2.2266e+01, -3.6893e-03, -3.6893e-03,
         -3.6893e-03,  6.8036e+00,  1.0293e+01, -3.6894e-03, -3.6894e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.2012,   0.4266],
        [ -0.7041,  -0.7780],
        [-13.7936, -19.0139],
        [-17.3310,   1.7286],
        [-15.3308,  -7.4443],
        [-21.5930,  -1.1479],
        [ -2.6760,  -5.2016],
        [ -2.0180,  -0.5773],
        [-17.7645,  -0.9040],
        [ 21.2366,  10.2990]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.8730,  -6.0334, -16.7628,   2.9339,  -1.8374,  15.6330, -11.4213,
         -4.5605,   8.4187,   9.6790], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.8486e-01,  2.0443e+00,  9.7822e+00, -9.8677e+00,  9.3654e-01,
          7.9022e+00,  1.4386e+00, -4.2033e-01,  4.1065e+00, -8.4712e+00],
        [-9.2531e-02,  1.6236e-01,  1.5221e+00, -9.7978e-02, -1.2310e+00,
         -1.9251e+00, -1.7031e-01,  1.4235e-01, -1.3822e+00, -1.1344e+00],
        [-8.6037e-01,  2.3215e+00,  1.4304e+00, -3.2726e+00, -7.7769e+00,
         -2.3340e-01, -3.8530e-02,  5.3793e-01, -4.3411e+00, -6.9608e-01],
        [-1.3639e-01,  2.7548e-01,  6.5437e-01, -2.9902e-01,  2.4082e+00,
         -4.3218e+00,  3.9204e+00,  4.0630e-02, -2.0508e+00, -4.1049e+00],
        [ 5.8218e-03, -2.6429e-03, -1.7500e-01,  6.0333e-02,  8.3664e-02,
         -9.2539e-01, -6.4658e-03, -8.6155e-03, -5.6628e-01, -3.7024e+00],
        [-4.5176e-01,  3.7814e-01,  4.4022e+00, -1.1581e+01,  5.7975e+00,
          9.7266e-01, -6.6521e+00, -5.7848e-01,  2.0058e+01, -2.2087e+01],
        [-3.9567e-02,  2.6737e+00,  1.6009e+01, -9.4287e+00,  8.0142e+00,
          1.0704e+01, -1.6493e+00,  1.5128e-01,  6.2921e+00, -1.7088e+01],
        [-1.7007e-01,  9.5803e-01,  3.0339e+00,  2.4421e+00,  1.0594e+00,
         -5.3408e+00,  2.2894e-01,  5.3753e-01,  3.6421e-01, -3.0982e+00],
        [ 1.5258e-01,  2.5699e+00,  1.2024e+00,  6.3013e+00,  7.7398e-02,
         -2.0140e+00, -1.3744e+00,  3.6685e-01,  1.8898e+00, -2.4105e+00],
        [ 2.3356e-01,  7.0669e-01, -1.5407e+01,  1.2517e+01,  1.2832e+00,
          9.5200e-01,  2.1132e+00,  8.2236e-01,  4.5899e-01, -4.3998e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -8.8558,  -6.0637,  -2.7833,  -3.7948,  -3.9303, -13.0618, -17.1310,
         -3.6497,  -4.4077,  -0.8310], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.7368,   1.4676,   4.2729,   2.2238,   0.1898,  14.0518,  -6.5514,
           3.4168,   2.0024,   0.4521],
        [  0.5474,  -1.4676,  -4.2729,  -2.2238,  -0.1898, -14.0454,   6.5610,
          -3.4167,  -2.0023,  -0.5803]], device='cuda:0'))])
xi:  [124.54354]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 608.8104477754364
W_T_median: 341.7736247374377
W_T_pctile_5: 124.54331101807556
W_T_CVAR_5_pct: -3.190588314072418
Average q (qsum/M+1):  50.69718293220766
Optimal xi:  [124.54354]
Expected(across Rb) median(across samples) p_equity:  0.3189428046345711
obj fun:  tensor(-1566.8273, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.4421,  0.6998],
        [-1.4421,  0.6998],
        [-6.3492, 12.4015],
        [-1.4421,  0.6998],
        [-1.4421,  0.6998],
        [ 4.8942,  4.5255],
        [18.5544, -3.4131],
        [-1.4421,  0.6998],
        [27.0829, 12.6198],
        [-1.4421,  0.6998]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.5873,  -3.5873,   9.4148,  -3.5873,  -3.5873, -10.2517, -12.3317,
         -3.5873,  12.0448,  -3.5873], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [  0.0268,   0.0268, -14.7872,   0.0268,   0.0268,   7.6196,  15.0122,
           0.0268,  -9.6735,   0.0268],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.1968,  -0.1968,  13.3947,  -0.1968,  -0.1968,  -5.6681, -12.5020,
          -0.1968,   4.5406,  -0.1968],
        [ -0.0833,  -0.0833,  14.5832,  -0.0833,  -0.0833,  -7.0292, -14.4098,
          -0.0833,   5.3216,  -0.0833],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.4053, -1.4053, 11.8715, -1.4053, -1.4053, -1.4053, -6.2928, -7.3809,
        -1.4053, -1.4053], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.7480e-02, -1.7480e-02, -2.1852e+01, -1.7480e-02, -1.7480e-02,
         -1.7480e-02,  6.0917e+00,  9.9946e+00, -1.7480e-02, -1.7480e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.3457,   0.1683],
        [ -2.3556,   0.1658],
        [-13.6623, -18.1119],
        [-14.1734,   0.8251],
        [-15.6911,  -7.6266],
        [-23.3223,  -2.0399],
        [ -5.1833,  -3.0089],
        [ -2.3570,   0.1645],
        [-19.1378,  -2.7418],
        [ 22.9187,  10.0907]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.1298,  -5.1236, -15.6308,   2.0844,  -4.3454,  15.5897,  -9.9291,
         -5.1242,   5.7734,   9.9800], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.2296e-03, -1.2382e-02,  7.4049e+00, -7.7891e-01,  2.7601e+00,
          5.1472e+00,  9.9429e-02, -1.0842e-02, -1.7745e+00, -7.6753e+00],
        [-4.3285e-02, -2.3705e-02,  1.5169e+00,  1.5922e+00,  7.3540e+00,
         -7.8423e+00,  1.9123e+00, -3.7753e-02,  1.2147e+00, -1.3745e+00],
        [-6.1686e-02, -3.4079e-02,  3.3667e+00,  5.4968e+00, -6.8225e+00,
          7.4478e+00,  3.8429e-01, -4.6123e-02,  1.5284e+00, -3.6440e+00],
        [ 5.5519e-02, -2.7000e-02, -3.2171e-01,  3.8403e-02,  8.1400e-01,
         -3.0495e+00, -6.8656e-02,  1.2582e-02, -3.6820e+00, -2.7169e+00],
        [ 2.2316e-01,  1.9804e-01, -3.9594e+00,  8.6284e+00,  4.9462e+00,
         -1.7010e+00,  3.4354e-02,  2.1236e-01, -1.7169e+00, -3.8754e+00],
        [-1.1419e-01, -1.2562e-01,  7.4684e+00, -1.5122e+01,  8.4482e+00,
          2.8189e+00,  1.4867e+00, -1.2448e-01,  2.0376e+01, -2.6961e+01],
        [-2.8001e-01, -3.6862e-01,  1.3206e+01, -1.1624e+01,  6.8216e+00,
          1.6269e+01,  2.7123e-01, -3.3274e-01,  6.6447e+00, -1.5781e+01],
        [ 1.9747e-02,  1.9323e-02,  1.6043e-02,  1.7572e-02, -2.7489e-01,
         -1.5159e+00, -1.9185e-03,  1.9510e-02, -3.5549e-01, -4.1801e+00],
        [-1.0960e-01, -1.2240e-01,  1.3278e+00, -4.5093e+00, -2.5747e+01,
         -1.3892e+00, -2.8935e-01, -1.2066e-01, -1.0021e+01, -6.9452e-01],
        [ 1.4152e-01,  1.3157e-01, -3.0338e+01,  1.5325e+01,  2.7307e+00,
          7.9736e-01, -3.4945e-01,  1.3333e-01,  1.3387e+00,  2.4970e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -9.6930,  -5.0978,  -7.9940,  -2.7008,  -3.5311, -13.0242, -17.0067,
         -4.2502,  -2.9973,  -0.7233], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -1.8994,   4.0386,  -1.8969,   4.5199,   2.3008,  15.5971,  -6.4718,
           0.7752,   5.3116,   0.6160],
        [  1.7275,  -4.0386,   1.8970,  -4.5199,  -2.3008, -15.5912,   6.4825,
          -0.7752,  -5.3114,  -0.7441]], device='cuda:0'))])
loaded xi:  199.69646
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1577.4580474702964
Current xi:  [198.27856]
objective value function right now is: -1577.4580474702964
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1578.1819119430552
Current xi:  [195.56879]
objective value function right now is: -1578.1819119430552
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [193.05476]
objective value function right now is: -1574.09367678296
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [190.947]
objective value function right now is: -1577.8883317580633
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [188.96605]
objective value function right now is: -1568.546487321187
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [186.53662]
objective value function right now is: -1566.0166563452779
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [184.10991]
objective value function right now is: -1571.942314533054
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.97054]
objective value function right now is: -1567.489297807544
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.22311]
objective value function right now is: -1575.7593407214567
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1579.3931933266517
Current xi:  [180.87057]
objective value function right now is: -1579.3931933266517
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.00034]
objective value function right now is: -1578.0909213121436
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.44435]
objective value function right now is: -1578.6872165171706
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [177.8127]
objective value function right now is: -1508.50521460783
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1581.9148864941678
Current xi:  [177.14398]
objective value function right now is: -1581.9148864941678
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [176.61148]
objective value function right now is: -1567.565429006768
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [176.29585]
objective value function right now is: -1581.7936827622927
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [176.02711]
objective value function right now is: -1574.544908856509
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [175.91774]
objective value function right now is: -1570.1677818761025
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [176.37086]
objective value function right now is: -1578.3572455485723
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [175.60205]
objective value function right now is: -1578.6487573337122
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [174.4712]
objective value function right now is: -1573.3585037453036
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [173.60927]
objective value function right now is: -1578.1486253689043
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.85428]
objective value function right now is: -1568.8673433224822
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.6535]
objective value function right now is: -1573.6666193739557
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.32735]
objective value function right now is: -1578.816794055339
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.92264]
objective value function right now is: -1556.3498236261303
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.92015]
objective value function right now is: -1580.8296941869323
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [172.88403]
objective value function right now is: -1575.738715197367
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [173.35936]
objective value function right now is: -1579.940483726834
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.60468]
objective value function right now is: -1575.9899881326185
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.63441]
objective value function right now is: -1578.2023304796842
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [172.61403]
objective value function right now is: -1577.3013940249962
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.64319]
objective value function right now is: -1575.0866833392233
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.934]
objective value function right now is: -1569.587518602171
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [171.02995]
objective value function right now is: -1571.9327980712942
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.7453]
objective value function right now is: -1578.4499484511111
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.63672]
objective value function right now is: -1578.043177441504
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.40517]
objective value function right now is: -1578.2692628276968
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.37204]
objective value function right now is: -1574.5107929666283
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.37642]
objective value function right now is: -1580.0695520354482
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.37769]
objective value function right now is: -1576.0370853246773
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.35005]
objective value function right now is: -1579.90019220661
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.26442]
objective value function right now is: -1580.124137793717
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.23567]
objective value function right now is: -1577.9263702146902
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.225]
objective value function right now is: -1575.0304986366937
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.27261]
objective value function right now is: -1578.7890607917357
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.09912]
objective value function right now is: -1579.2997097585408
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.10545]
objective value function right now is: -1579.4145506105926
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.0247]
objective value function right now is: -1580.3230727398075
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.00682]
objective value function right now is: -1580.8520453521005
min fval:  -1579.3691607691544
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.4781,  0.9045],
        [-1.4781,  0.9045],
        [-5.7277, 12.4442],
        [-1.4781,  0.9045],
        [-1.4781,  0.9045],
        [ 4.9569,  5.3891],
        [18.8995, -3.9013],
        [-1.4781,  0.9045],
        [36.5000, 12.6707],
        [-1.4781,  0.9045]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -4.5244,  -4.5244,   9.4769,  -4.5244,  -4.5244, -11.0367, -12.6464,
         -4.5244,  12.0612,  -4.5244], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-9.8288e-03, -9.8288e-03, -6.0398e-01, -9.8288e-03, -9.8288e-03,
         -3.7219e-02, -9.2030e-02, -9.8288e-03, -1.7394e+00, -9.8288e-03],
        [-9.8288e-03, -9.8288e-03, -6.0398e-01, -9.8288e-03, -9.8288e-03,
         -3.7219e-02, -9.2030e-02, -9.8288e-03, -1.7394e+00, -9.8288e-03],
        [ 6.2160e-02,  6.2160e-02, -1.4574e+01,  6.2160e-02,  6.2160e-02,
          9.7237e+00,  1.4974e+01,  6.2160e-02, -9.3878e+00,  6.2160e-02],
        [-9.8288e-03, -9.8288e-03, -6.0398e-01, -9.8288e-03, -9.8288e-03,
         -3.7219e-02, -9.2030e-02, -9.8288e-03, -1.7394e+00, -9.8288e-03],
        [-9.8288e-03, -9.8288e-03, -6.0398e-01, -9.8288e-03, -9.8288e-03,
         -3.7219e-02, -9.2030e-02, -9.8288e-03, -1.7394e+00, -9.8288e-03],
        [-9.8288e-03, -9.8288e-03, -6.0398e-01, -9.8288e-03, -9.8288e-03,
         -3.7219e-02, -9.2030e-02, -9.8288e-03, -1.7394e+00, -9.8288e-03],
        [ 4.6713e-02,  4.6713e-02,  1.3172e+01,  4.6713e-02,  4.6713e-02,
         -7.1033e+00, -1.2602e+01,  4.6713e-02,  4.4035e+00,  4.6713e-02],
        [ 5.5124e-02,  5.5124e-02,  1.4453e+01,  5.5124e-02,  5.5124e-02,
         -8.9396e+00, -1.4463e+01,  5.5123e-02,  5.1450e+00,  5.5124e-02],
        [-9.8288e-03, -9.8288e-03, -6.0398e-01, -9.8288e-03, -9.8288e-03,
         -3.7219e-02, -9.2030e-02, -9.8288e-03, -1.7394e+00, -9.8288e-03],
        [-9.8288e-03, -9.8288e-03, -6.0398e-01, -9.8288e-03, -9.8288e-03,
         -3.7219e-02, -9.2030e-02, -9.8288e-03, -1.7394e+00, -9.8288e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.9680, -1.9680, 12.1724, -1.9680, -1.9680, -1.9680, -6.2669, -7.4407,
        -1.9680, -1.9680], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.5318e-02, -1.5318e-02, -2.1738e+01, -1.5317e-02, -1.5318e-02,
         -1.5318e-02,  5.4844e+00,  9.4223e+00, -1.5318e-02, -1.5318e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.8017,   0.0803],
        [ -2.9185,   0.1361],
        [-12.7456, -18.1367],
        [-16.4041,   1.2018],
        [-15.2883,  -7.7163],
        [-23.7040,  -2.1112],
        [ -2.8176,   0.1974],
        [ -3.3599,  -0.6465],
        [-20.0194,  -2.4826],
        [ 22.6361,  10.6662]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.3798,  -6.3600, -15.6504,   2.6281,  -4.5510,  15.1688,  -6.7062,
         -6.6060,   5.9686,   9.4749], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 3.1673e-03,  3.4975e-04,  7.1309e+00, -5.5172e+00,  1.8421e+00,
          4.5165e+00, -6.8769e-03,  8.1010e-03, -6.4485e+00, -8.2044e+00],
        [-4.3021e-03, -3.1962e-03, -6.7335e-01,  1.3334e+00,  5.3574e+00,
         -9.1419e+00, -2.4399e-03, -5.8652e-03,  6.1103e-01, -3.1719e+00],
        [ 1.6238e-01,  2.3130e-01,  4.6379e+00, -1.2773e+00, -2.9481e+00,
          7.2806e+00,  3.3159e-01,  3.0938e-01,  2.1278e+00, -3.7770e+00],
        [-8.2024e-05, -4.0436e-03, -8.9217e-01,  1.4930e-01, -9.6423e-01,
         -1.2099e+00, -1.8754e-02,  2.2550e-01, -5.7415e+00, -3.2574e+00],
        [ 2.3499e-02, -4.0834e-02, -7.1506e+00,  7.9306e+00,  3.6456e+00,
         -1.3843e+00, -3.8122e-02,  2.4039e-02, -1.4800e+00, -3.5150e+00],
        [-1.1569e-01, -2.5501e-01,  5.5108e+00, -1.4681e+01,  6.6283e+00,
         -1.0154e-01, -2.3001e-01, -2.2512e-01,  2.3206e+01, -2.5397e+01],
        [ 1.0596e-01, -4.0204e-02,  1.3775e+01, -1.3955e+01,  5.8916e+00,
          1.6570e+01, -1.7519e-01,  2.0372e-01,  6.4676e+00, -1.5407e+01],
        [ 7.2919e-03,  9.9413e-04, -7.1621e-01, -9.9030e-03, -1.7734e-01,
         -1.0814e+00, -6.8015e-03,  1.4928e-02, -2.6003e-01, -4.6947e+00],
        [-1.9384e-01, -1.8011e-02,  9.9724e-01, -3.1509e+00, -2.9493e+01,
         -1.2953e+00,  9.0556e-02,  5.9704e-01, -1.0517e+01, -5.4284e-01],
        [ 4.8204e-01,  1.3143e-01, -2.8888e+01,  1.5403e+01,  7.9165e-01,
          1.4235e+00, -1.2763e-01,  6.8820e-01,  1.8115e+00, -1.8043e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -9.9238,  -7.0244,  -8.2044,  -3.2817,  -3.6472, -15.2495, -16.7625,
         -4.8169,  -2.8455,  -0.9256], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -1.8966,   2.0683,  -1.5802,   4.1544,   1.9125,  13.8827,  -7.6585,
           0.3855,   5.7239,   0.5527],
        [  1.7291,  -2.0683,   1.5802,  -4.1544,  -1.9125, -13.8757,   7.6694,
          -0.3855,  -5.7237,  -0.6809]], device='cuda:0'))])
xi:  [170.225]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 656.8153843859649
W_T_median: 405.557674226176
W_T_pctile_5: 178.2604615970512
W_T_CVAR_5_pct: 18.848183581076757
Average q (qsum/M+1):  49.21163841985887
Optimal xi:  [170.225]
Expected(across Rb) median(across samples) p_equity:  0.2961614355444908
obj fun:  tensor(-1579.3692, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.9019,  0.4988],
        [-1.9019,  0.4988],
        [-8.1422, 12.4736],
        [-1.9019,  0.4988],
        [-1.9019,  0.4988],
        [ 5.6019,  5.0115],
        [19.6630, -3.6740],
        [-1.9019,  0.4988],
        [23.3244, 13.1194],
        [-1.9019,  0.4988]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -5.2356,  -5.2356,   9.2481,  -5.2356,  -5.2356, -11.5191, -12.8330,
         -5.2356,  12.0039,  -5.2356], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3313e-01,  1.3313e-01, -1.5371e+01,  1.3313e-01,  1.3313e-01,
          9.7604e+00,  1.5560e+01,  1.3313e-01, -9.6678e+00,  1.3313e-01],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 8.2326e-02,  8.2326e-02,  1.3864e+01,  8.2326e-02,  8.2326e-02,
         -6.7444e+00, -1.3487e+01,  8.2326e-02,  4.4834e+00,  8.2326e-02],
        [-4.3109e-02, -4.3109e-02,  1.5211e+01, -4.3109e-02, -4.3109e-02,
         -8.9699e+00, -1.5253e+01, -4.3109e-02,  5.2065e+00, -4.3109e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.3658, -2.3658, 11.9154, -2.3658, -2.3658, -2.3658, -5.9631, -7.2303,
        -2.3658, -2.3658], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.1782e-03, -1.1781e-03, -2.1808e+01, -1.1780e-03, -1.1780e-03,
         -1.1781e-03,  5.7429e+00,  1.0034e+01, -1.1781e-03, -1.1781e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.7719,  -0.5696],
        [ -2.8855,  -0.9812],
        [-12.2353, -18.2173],
        [-14.5578,   1.1447],
        [-14.0825,  -8.2473],
        [-24.7263,  -3.3739],
        [ -3.4310,   0.2902],
        [ -2.8191,  -0.9488],
        [-20.8330,  -3.1332],
        [ 22.4955,  10.5721]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.3110,  -5.8883, -15.8407,   2.0953,  -6.1630,  14.9391,  -6.2312,
         -5.9700,   5.3374,  10.8521], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.4941e-01, -1.1865e+00,  7.1373e+00, -4.1889e+00,  4.1603e+00,
          5.1771e+00, -1.3789e-02, -9.4881e-01, -4.8181e+00, -8.4303e+00],
        [ 3.9608e-01,  4.5232e-01,  6.1251e-01,  4.2718e+00,  4.2220e+00,
         -2.4626e+00,  3.3116e-01,  4.3654e-01,  2.8868e+00, -2.5238e+00],
        [ 5.3466e-01,  6.4230e-01,  4.2585e+00, -2.6127e+00, -1.9798e-01,
          7.1470e+00, -4.0727e-01,  6.0353e-01,  1.5804e+00, -3.6629e+00],
        [ 1.1310e-01, -1.6970e-01,  1.3849e+00, -1.5478e+00, -5.2431e-01,
         -8.5469e+00, -3.7096e-01, -1.8283e-01, -2.0932e+00, -3.1289e+00],
        [-1.1634e-02, -7.1284e-02, -7.8162e-01,  4.5978e+00,  3.9493e+00,
         -1.7412e+00,  3.3428e-03, -6.0435e-02, -1.1142e+00, -6.6281e+00],
        [-3.5520e+00, -5.1921e+00,  7.0374e+00, -1.6692e+01,  6.5776e+00,
          8.0984e-01, -1.2232e+00, -5.0146e+00,  2.7216e+01, -2.7095e+01],
        [-8.9406e-01, -6.6762e-01,  1.4005e+01, -8.5994e+00,  5.7129e+00,
          1.5630e+01, -3.9389e-01, -7.2259e-01,  7.9317e+00, -1.5731e+01],
        [ 2.0524e-03, -4.4305e-04, -6.1746e-01,  5.0932e-02, -2.6030e-01,
         -1.7705e+00,  8.6110e-03,  3.6191e-04, -4.6635e-01, -4.7315e+00],
        [ 4.4417e-01, -1.5968e-01,  4.3980e-01, -5.1847e+00, -2.1919e+01,
         -1.8255e+00, -7.5973e-01, -2.4026e-02, -2.1976e+01, -7.1638e-01],
        [-6.8731e-01, -1.5296e+00, -2.5830e+01,  1.3671e+01,  2.7949e+00,
          4.2890e-01,  8.4443e-01, -1.5023e+00,  1.0097e+00,  8.0914e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -9.5060,  -5.9330,  -8.3280,  -3.2013,  -5.4233, -14.7743, -17.0758,
         -4.8064,  -3.0177,  -0.6631], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -3.0095,   2.3753,  -1.8263,   5.0601,   1.0920,  16.1966,  -6.7467,
           0.5833,   5.9697,   0.6871],
        [  2.8518,  -2.3753,   1.8263,  -5.0601,  -1.0920, -16.1889,   6.7577,
          -0.5833,  -5.9696,  -0.8152]], device='cuda:0'))])
loaded xi:  -0.0055836756
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1339.2025353665024
Current xi:  [5.1669073]
objective value function right now is: -1339.2025353665024
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1372.7181837270193
Current xi:  [11.955963]
objective value function right now is: -1372.7181837270193
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1385.1061668979403
Current xi:  [19.106663]
objective value function right now is: -1385.1061668979403
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1409.9915991294883
Current xi:  [25.875212]
objective value function right now is: -1409.9915991294883
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1429.0061518513342
Current xi:  [32.83706]
objective value function right now is: -1429.0061518513342
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1437.9930872407483
Current xi:  [39.516228]
objective value function right now is: -1437.9930872407483
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1455.5206091813204
Current xi:  [45.905823]
objective value function right now is: -1455.5206091813204
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1462.3708356371362
Current xi:  [52.249134]
objective value function right now is: -1462.3708356371362
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1485.8862735455245
Current xi:  [58.3661]
objective value function right now is: -1485.8862735455245
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1489.7135292494581
Current xi:  [64.675385]
objective value function right now is: -1489.7135292494581
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1501.3892354032976
Current xi:  [70.837265]
objective value function right now is: -1501.3892354032976
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [77.09173]
objective value function right now is: -1478.878053657095
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1508.4881841614372
Current xi:  [82.764206]
objective value function right now is: -1508.4881841614372
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1537.5851214569175
Current xi:  [88.85173]
objective value function right now is: -1537.5851214569175
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1551.458524852628
Current xi:  [94.59457]
objective value function right now is: -1551.458524852628
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1552.9429669315666
Current xi:  [99.65865]
objective value function right now is: -1552.9429669315666
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1560.2832546742595
Current xi:  [104.969406]
objective value function right now is: -1560.2832546742595
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [110.10818]
objective value function right now is: -1410.088526700904
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [115.13328]
objective value function right now is: -1549.296722577084
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [120.945045]
objective value function right now is: -1556.0006209925118
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1577.713608788647
Current xi:  [125.72634]
objective value function right now is: -1577.713608788647
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1592.0738432692513
Current xi:  [130.55519]
objective value function right now is: -1592.0738432692513
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [134.89465]
objective value function right now is: -1588.915453597298
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [139.20233]
objective value function right now is: -1592.0388144955573
50.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.8591306597957
Current xi:  [142.79065]
objective value function right now is: -1600.8591306597957
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.1412197996713
Current xi:  [146.27986]
objective value function right now is: -1602.1412197996713
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [149.80168]
objective value function right now is: -1589.3842769787852
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [153.81584]
objective value function right now is: -1601.4735233727386
57.99999999999999% of gradient descent iterations done. Method = Adam
new min fval:  -1607.3490728965999
Current xi:  [158.0517]
objective value function right now is: -1607.3490728965999
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1612.2524642709527
Current xi:  [161.61497]
objective value function right now is: -1612.2524642709527
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [165.22658]
objective value function right now is: -1608.147607296636
64.0% of gradient descent iterations done. Method = Adam
new min fval:  -1616.9179586648224
Current xi:  [167.96565]
objective value function right now is: -1616.9179586648224
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1618.4701076004492
Current xi:  [170.05472]
objective value function right now is: -1618.4701076004492
68.0% of gradient descent iterations done. Method = Adam
new min fval:  -1619.49375669443
Current xi:  [172.10857]
objective value function right now is: -1619.49375669443
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [174.43878]
objective value function right now is: -1617.991384050024
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1626.6843744213068
Current xi:  [174.87515]
objective value function right now is: -1626.6843744213068
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1626.8433325137564
Current xi:  [175.41933]
objective value function right now is: -1626.8433325137564
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [175.55156]
objective value function right now is: -1624.8431407496741
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1627.6242407668765
Current xi:  [176.1178]
objective value function right now is: -1627.6242407668765
80.0% of gradient descent iterations done. Method = Adam
new min fval:  -1627.9394190062214
Current xi:  [176.38753]
objective value function right now is: -1627.9394190062214
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [176.70737]
objective value function right now is: -1627.4543325929071
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [177.16243]
objective value function right now is: -1624.5225126367152
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1628.2103978594816
Current xi:  [177.62334]
objective value function right now is: -1628.2103978594816
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [177.96236]
objective value function right now is: -1622.2851601816685
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [178.52216]
objective value function right now is: -1627.917712411915
new min fval from sgd:  -1628.3469816308868
new min fval from sgd:  -1628.5053677488154
new min fval from sgd:  -1628.571729623267
new min fval from sgd:  -1628.6458748755638
new min fval from sgd:  -1628.7343599914386
new min fval from sgd:  -1628.7730489142532
new min fval from sgd:  -1628.8073105244423
new min fval from sgd:  -1628.832614235696
new min fval from sgd:  -1628.8725295545535
new min fval from sgd:  -1628.931667870871
new min fval from sgd:  -1629.0289290614398
new min fval from sgd:  -1629.092784624872
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [178.94995]
objective value function right now is: -1626.7634548961576
new min fval from sgd:  -1629.2014697930772
new min fval from sgd:  -1629.3441273494832
new min fval from sgd:  -1629.4521013700275
new min fval from sgd:  -1629.5055029105035
new min fval from sgd:  -1629.5105242104755
new min fval from sgd:  -1629.5181724643937
new min fval from sgd:  -1629.524230535425
new min fval from sgd:  -1629.5435881633211
new min fval from sgd:  -1629.5790423161145
new min fval from sgd:  -1629.6048886618757
new min fval from sgd:  -1629.6160890840288
new min fval from sgd:  -1629.6488899230683
new min fval from sgd:  -1629.6834338282533
new min fval from sgd:  -1629.8365284640477
new min fval from sgd:  -1629.8586637780109
new min fval from sgd:  -1629.9486242190605
new min fval from sgd:  -1629.9853724747427
new min fval from sgd:  -1630.005687466594
new min fval from sgd:  -1630.0374625985623
new min fval from sgd:  -1630.1132041348128
new min fval from sgd:  -1630.1893520436577
new min fval from sgd:  -1630.2175683143378
new min fval from sgd:  -1630.2444419639396
new min fval from sgd:  -1630.2714297148862
new min fval from sgd:  -1630.2819861653763
new min fval from sgd:  -1630.2897761644592
new min fval from sgd:  -1630.3010527493436
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.44069]
objective value function right now is: -1628.524858639951
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [179.76253]
objective value function right now is: -1629.421958493399
new min fval from sgd:  -1630.3256549336782
new min fval from sgd:  -1630.3468990760466
new min fval from sgd:  -1630.3648217301022
new min fval from sgd:  -1630.3797117676438
new min fval from sgd:  -1630.392485307104
new min fval from sgd:  -1630.402292413632
new min fval from sgd:  -1630.4123279732587
new min fval from sgd:  -1630.4178889855384
new min fval from sgd:  -1630.4179439063066
new min fval from sgd:  -1630.4196949217599
new min fval from sgd:  -1630.4199415128944
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.05547]
objective value function right now is: -1630.123362073662
new min fval from sgd:  -1630.4349166976394
new min fval from sgd:  -1630.4639255331197
new min fval from sgd:  -1630.491592675725
new min fval from sgd:  -1630.5219013905648
new min fval from sgd:  -1630.5454976144526
new min fval from sgd:  -1630.563073455108
new min fval from sgd:  -1630.5751496479622
new min fval from sgd:  -1630.589209752111
new min fval from sgd:  -1630.6049622195465
new min fval from sgd:  -1630.6140329262382
new min fval from sgd:  -1630.6218271150008
new min fval from sgd:  -1630.631279991889
new min fval from sgd:  -1630.6391351997618
new min fval from sgd:  -1630.6490585483632
new min fval from sgd:  -1630.6591229800083
new min fval from sgd:  -1630.6628358566543
new min fval from sgd:  -1630.6674477587892
new min fval from sgd:  -1630.6691384840683
new min fval from sgd:  -1630.6751019081491
new min fval from sgd:  -1630.677467222844
new min fval from sgd:  -1630.6885931643285
new min fval from sgd:  -1630.709934558551
new min fval from sgd:  -1630.7343199827797
new min fval from sgd:  -1630.7551053722698
new min fval from sgd:  -1630.7709719978925
new min fval from sgd:  -1630.7822387875603
new min fval from sgd:  -1630.7930412400196
new min fval from sgd:  -1630.8040905447956
new min fval from sgd:  -1630.8179393556557
new min fval from sgd:  -1630.8298372806846
new min fval from sgd:  -1630.832497690117
new min fval from sgd:  -1630.8358954256648
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [180.16402]
objective value function right now is: -1630.810246548635
min fval:  -1630.8358954256648
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.4830e+00,  3.0718e-02],
        [-1.4830e+00,  3.0717e-02],
        [-6.3673e+00,  1.2632e+01],
        [-1.4830e+00,  3.0717e-02],
        [-1.4830e+00,  3.0717e-02],
        [ 6.8203e+00,  5.7258e+00],
        [ 2.0545e+01, -4.8124e+00],
        [-1.4830e+00,  3.0718e-02],
        [ 4.0414e+01,  1.3095e+01],
        [-1.4830e+00,  3.0717e-02]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.9128,  -3.9128,   9.5392,  -3.9128,  -3.9128, -11.5636, -13.0913,
         -3.9128,  12.3596,  -3.9128], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-1.0563e-02, -1.0563e-02, -8.2439e-01, -1.0563e-02, -1.0563e-02,
         -1.0488e-01, -4.2615e-01, -1.0563e-02, -1.4408e+00, -1.0563e-02],
        [-1.0563e-02, -1.0563e-02, -8.2439e-01, -1.0563e-02, -1.0563e-02,
         -1.0488e-01, -4.2615e-01, -1.0563e-02, -1.4408e+00, -1.0563e-02],
        [-9.5457e-02, -9.5457e-02, -1.5627e+01, -9.5457e-02, -9.5457e-02,
          1.1399e+01,  1.5160e+01, -9.5457e-02, -9.4408e+00, -9.5457e-02],
        [-1.0563e-02, -1.0563e-02, -8.2439e-01, -1.0563e-02, -1.0563e-02,
         -1.0488e-01, -4.2615e-01, -1.0563e-02, -1.4408e+00, -1.0563e-02],
        [-1.0563e-02, -1.0563e-02, -8.2439e-01, -1.0563e-02, -1.0563e-02,
         -1.0488e-01, -4.2615e-01, -1.0563e-02, -1.4408e+00, -1.0563e-02],
        [-1.0563e-02, -1.0563e-02, -8.2439e-01, -1.0563e-02, -1.0563e-02,
         -1.0488e-01, -4.2615e-01, -1.0563e-02, -1.4408e+00, -1.0563e-02],
        [-2.8364e-01, -2.8364e-01,  1.4495e+01, -2.8364e-01, -2.8364e-01,
         -3.8909e+00, -1.2539e+01, -2.8364e-01,  4.0974e+00, -2.8364e-01],
        [-3.3170e-01, -3.3170e-01,  1.6052e+01, -3.3171e-01, -3.3170e-01,
         -1.0792e+01, -1.4380e+01, -3.3170e-01,  4.6668e+00, -3.3170e-01],
        [-1.0563e-02, -1.0563e-02, -8.2439e-01, -1.0563e-02, -1.0563e-02,
         -1.0488e-01, -4.2615e-01, -1.0563e-02, -1.4408e+00, -1.0563e-02],
        [-1.0563e-02, -1.0563e-02, -8.2439e-01, -1.0563e-02, -1.0563e-02,
         -1.0488e-01, -4.2615e-01, -1.0563e-02, -1.4408e+00, -1.0563e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.5591, -1.5591, 12.2476, -1.5591, -1.5591, -1.5591, -5.7974, -7.3604,
        -1.5591, -1.5591], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0581,   0.0581, -21.6139,   0.0581,   0.0581,   0.0581,   5.0675,
           9.5836,   0.0581,   0.0581]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[-2.8252e+00, -7.7867e-04],
        [-2.6740e+00, -2.9813e-02],
        [-1.3410e+01, -1.8116e+01],
        [-1.6457e+01,  1.9596e+00],
        [-1.6639e+01, -5.6714e+00],
        [-2.5881e+01, -3.3602e+00],
        [-2.9084e+00, -4.7585e-03],
        [-2.6220e+00, -8.6022e-02],
        [-2.1337e+01, -3.2181e+00],
        [ 2.1584e+01,  1.1774e+01]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -4.7412,  -4.8833, -15.6636,   2.9378,  -9.1299,  14.1175,  -4.6720,
         -5.0307,   5.0993,  10.6526], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-7.4588e-04, -1.4778e-03,  4.7667e+00, -1.8108e+00,  3.8164e+00,
          4.2135e+00, -5.6866e-04, -1.7925e-03, -3.1324e+00, -8.9948e+00],
        [ 4.8932e-03,  5.8277e-03, -2.0477e-01, -3.4624e-02,  4.0407e-01,
         -1.5406e+00,  5.5450e-03,  7.9204e-03, -3.5944e-01, -3.2495e+00],
        [ 2.5174e-01,  2.6574e-01,  5.6501e+00, -1.9406e+01,  1.1352e+00,
          7.1777e+00,  3.3144e-01,  3.4326e-01,  1.4224e+00, -3.1270e+00],
        [ 4.2452e-01,  5.2788e-01,  1.6169e+00, -2.7109e+00,  9.9586e-01,
         -1.1995e+01,  4.4376e-01,  6.5145e-01, -7.7820e-01, -2.6677e+00],
        [ 3.2161e-01,  3.5678e-01,  1.2443e+00,  7.0572e+00,  2.3647e+00,
         -5.6697e+00,  3.3542e-01,  3.2294e-01,  4.9641e+00, -4.9839e+00],
        [-7.3225e-01, -6.1848e-01,  7.4295e+00, -1.3165e+01,  5.7183e+00,
         -1.9732e+00, -7.9000e-01, -5.4210e-01,  2.7446e+01, -2.6823e+01],
        [ 5.2490e-01,  4.9068e-01,  1.5618e+01, -9.3955e+00,  4.0380e+00,
          1.5561e+01,  5.3541e-01,  4.9896e-01,  8.6275e+00, -1.5693e+01],
        [ 8.9285e-03,  8.3526e-03,  1.9714e+00, -5.5550e-02,  1.0070e+00,
         -4.8502e+00,  1.1147e-02,  9.2999e-03, -1.8475e-01, -4.9102e+00],
        [ 3.0692e-01,  5.0179e-01, -4.9770e-01, -1.4226e+01, -1.3137e+01,
         -1.0436e+00,  2.2329e-01,  8.1363e-01, -1.8844e+01, -4.4678e-01],
        [ 1.4322e+00,  1.1763e+00, -2.7737e+01,  1.6145e+01,  2.0293e+00,
          1.4628e+00,  1.7951e+00,  1.2399e+00,  1.9787e+00, -3.3616e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -9.0338,  -6.0156,  -7.7327,  -2.7984,  -2.9694, -15.8852, -16.8099,
         -4.6450,  -2.8357,  -1.0713], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -3.9711,   0.7353,  -0.7936,   5.9505,   2.7723,  15.1545,  -7.7544,
           1.3151,   6.6677,   0.6086],
        [  3.8704,  -0.7352,   0.7934,  -5.9505,  -2.7721, -15.1471,   7.7628,
          -1.3151,  -6.6676,  -0.7366]], device='cuda:0'))])
xi:  [180.16188]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 640.5280498217464
W_T_median: 415.49973747902095
W_T_pctile_5: 180.44329375516756
W_T_CVAR_5_pct: 24.646533251508473
Average q (qsum/M+1):  48.633934759324596
Optimal xi:  [180.16188]
Expected(across Rb) median(across samples) p_equity:  0.27882125675678254
obj fun:  tensor(-1630.8359, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
pre-loaded NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.3717,   0.6695],
        [ -0.3716,   0.6698],
        [-13.0855,  12.5922],
        [ -0.3717,   0.6693],
        [ -0.3716,   0.6697],
        [  5.9153,   4.0793],
        [ 23.7400,  -3.6224],
        [ -0.3717,   0.6695],
        [ 10.1120,  15.2336],
        [ -0.3717,   0.6692]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -5.0505,  -5.0505,   9.7641,  -5.0505,  -5.0505, -15.2228, -13.3805,
         -5.0505,  12.2042,  -5.0505], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.8485e-03, -5.8551e-03, -7.6404e-01, -5.8444e-03, -5.8528e-03,
         -1.4220e-03, -1.1432e-01, -5.8490e-03, -1.5264e+00, -5.8435e-03],
        [-5.8485e-03, -5.8551e-03, -7.6404e-01, -5.8444e-03, -5.8528e-03,
         -1.4219e-03, -1.1432e-01, -5.8490e-03, -1.5264e+00, -5.8434e-03],
        [ 3.2859e-02,  3.2874e-02, -1.7032e+01,  3.2850e-02,  3.2868e-02,
          1.2754e+01,  1.8633e+01,  3.2859e-02, -9.4154e+00,  3.2848e-02],
        [-5.8486e-03, -5.8552e-03, -7.6404e-01, -5.8444e-03, -5.8529e-03,
         -1.4219e-03, -1.1432e-01, -5.8490e-03, -1.5264e+00, -5.8435e-03],
        [-5.8485e-03, -5.8551e-03, -7.6404e-01, -5.8443e-03, -5.8528e-03,
         -1.4220e-03, -1.1432e-01, -5.8489e-03, -1.5264e+00, -5.8434e-03],
        [-5.8485e-03, -5.8551e-03, -7.6404e-01, -5.8444e-03, -5.8528e-03,
         -1.4219e-03, -1.1432e-01, -5.8490e-03, -1.5264e+00, -5.8435e-03],
        [ 3.6141e-01,  3.6144e-01,  1.5413e+01,  3.6139e-01,  3.6143e-01,
         -9.7311e-01, -1.5143e+01,  3.6141e-01,  3.7923e+00,  3.6138e-01],
        [ 5.6909e-02,  5.6910e-02,  1.6920e+01,  5.6907e-02,  5.6911e-02,
         -1.2295e+01, -1.7740e+01,  5.6910e-02,  4.4796e+00,  5.6907e-02],
        [-5.8484e-03, -5.8551e-03, -7.6404e-01, -5.8443e-03, -5.8528e-03,
         -1.4219e-03, -1.1432e-01, -5.8489e-03, -1.5264e+00, -5.8433e-03],
        [-5.8484e-03, -5.8550e-03, -7.6404e-01, -5.8443e-03, -5.8527e-03,
         -1.4219e-03, -1.1432e-01, -5.8489e-03, -1.5264e+00, -5.8433e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.2693, -2.2693, 12.4049, -2.2693, -2.2693, -2.2693, -6.7425, -8.2173,
        -2.2693, -2.2693], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.1030,  -0.1030, -20.5494,  -0.1030,  -0.1030,  -0.1030,   2.5710,
           9.2263,  -0.1030,  -0.1030]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -4.3861,   5.4599],
        [ -4.2397,  -0.0905],
        [ -4.4164,   0.2492],
        [ -2.9491,   7.3178],
        [-13.3487,  -8.8484],
        [-25.0765,  -1.7542],
        [-17.2729,   1.0833],
        [-27.6241,   1.9478],
        [-20.7803,  -4.1706],
        [ 25.9706,  10.3564]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-8.0550, -5.5325, -5.9800,  6.6908, -5.8258, 18.0276, -1.6679,  2.1162,
         9.0046,  8.0369], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.3907e-01, -3.8374e-03, -6.7160e-03, -8.6798e-01, -2.9549e-01,
         -4.5666e-01, -8.2811e-05, -1.3701e-02, -1.9292e-01, -4.2104e+00],
        [ 1.0908e-01,  1.0019e+00,  8.6860e-01,  7.9105e-01,  3.0713e+00,
         -7.6095e-01,  2.2625e+00,  3.1546e+00,  2.0174e+00, -3.6993e+00],
        [ 1.3899e-01,  1.1567e+00,  8.7849e-01, -5.9272e+00,  4.7075e+00,
          5.2993e+00,  3.1115e-01, -4.7893e+00,  1.6887e+00, -2.9310e+00],
        [ 7.6994e-03,  6.6736e-01,  2.2744e-01, -1.7566e+01,  1.8967e+00,
         -1.1864e+01,  3.1853e-03, -5.2087e-04,  1.3813e+00, -2.2098e+00],
        [-2.7124e+00,  2.6005e+00,  3.1840e+00,  1.3198e+01, -1.8426e+00,
          2.2916e+00,  6.2049e+00,  7.3561e+00,  5.2305e+00, -1.0786e+01],
        [-9.9072e-03, -3.2262e+00, -2.0612e+00, -5.9365e+01,  7.4415e+00,
          1.9468e+00,  1.7966e+00, -3.3964e+00,  2.5606e+01, -2.8539e+01],
        [-3.9568e-03,  5.4525e-02,  1.9926e-01, -1.6575e+01,  9.2899e+00,
          1.6442e+01, -4.5718e+00, -7.6150e+00,  9.2888e+00, -1.4181e+01],
        [-9.6519e-01, -1.3308e-01, -1.2591e-01, -3.4696e+00,  1.5107e+00,
          2.9861e+00, -3.3982e+00, -6.5947e+00,  1.6287e+00, -3.8129e+00],
        [-5.1323e-03,  1.7279e-01, -2.3867e-01, -2.1756e+01,  4.6246e+00,
         -1.3993e+01,  5.5183e-03, -5.8642e-05, -1.5793e+01, -6.3953e-01],
        [ 1.4124e-02, -4.2348e-01,  5.5579e-01,  2.3239e+01, -2.0920e+01,
         -7.1915e-01,  7.2353e+00,  2.0006e+01, -8.4537e-01,  6.5294e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -6.3065,  -5.4810,  -8.4394,  -2.2189,  -2.9406, -16.0554, -15.9864,
         -3.8750,  -2.9321,  -0.1042], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.0503,   1.2914,  -4.0073,   6.8973,   0.2758,  19.5233,  -7.0728,
          -2.3013,   7.2728,   0.3457],
        [  0.0342,  -1.2913,   4.0074,  -6.8973,  -0.2753, -19.5148,   7.0846,
           2.3013,  -7.2726,  -0.4738]], device='cuda:0'))])
loaded xi:  247.51015
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1439.2724125859495
W_T_median: 1086.6311344566764
W_T_pctile_5: -114.4532091189134
W_T_CVAR_5_pct: -274.67719995288854
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2978.047645232179
Current xi:  [244.25638]
objective value function right now is: -2978.047645232179
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [235.48444]
objective value function right now is: 316.84124082794153
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [232.82812]
objective value function right now is: -2562.170925527725
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [231.54572]
objective value function right now is: -2783.185114696414
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -2982.2802248309144
Current xi:  [230.09024]
objective value function right now is: -2982.2802248309144
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -2984.8370786754904
Current xi:  [228.70583]
objective value function right now is: -2984.8370786754904
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [228.00508]
objective value function right now is: -2907.6360458621066
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -3033.307048165432
Current xi:  [227.4018]
objective value function right now is: -3033.307048165432
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [226.05058]
objective value function right now is: -3033.1861138619106
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [224.26971]
objective value function right now is: -3016.160716749615
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -3050.1940993672747
Current xi:  [224.1546]
objective value function right now is: -3050.1940993672747
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [223.28094]
objective value function right now is: -2722.5233670714338
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -3069.151228373111
Current xi:  [222.61996]
objective value function right now is: -3069.151228373111
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [221.77397]
objective value function right now is: -3038.1229166118605
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -3091.0354613491036
Current xi:  [220.95894]
objective value function right now is: -3091.0354613491036
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [221.0199]
objective value function right now is: -3019.1679955683817
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [221.31728]
objective value function right now is: -3029.0904217722377
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [221.02203]
objective value function right now is: -3059.5238552698597
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [218.04208]
objective value function right now is: 30265.762384084814
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.70622]
objective value function right now is: -2600.0804142182155
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.78445]
objective value function right now is: -2752.623242125729
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.5394]
objective value function right now is: -2878.73173152286
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.27293]
objective value function right now is: -2851.7886200668736
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.1338]
objective value function right now is: -2774.9331762793013
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.35919]
objective value function right now is: -2698.3552843727657
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.05075]
objective value function right now is: -2896.4192957204646
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.68492]
objective value function right now is: -2823.027489289158
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [212.57794]
objective value function right now is: -2981.3520586953828
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [212.91571]
objective value function right now is: -2900.150205472991
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.03682]
objective value function right now is: -2929.2695467101116
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.74571]
objective value function right now is: -2904.8251538169234
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.65703]
objective value function right now is: -3000.239505479266
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.36072]
objective value function right now is: -1993.0392495679273
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.1403]
objective value function right now is: -2834.453629443125
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.09778]
objective value function right now is: -2956.2339366733413
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.24954]
objective value function right now is: -3028.586384310254
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.34094]
objective value function right now is: -3031.921875741209
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.33423]
objective value function right now is: -3041.628842038468
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.34781]
objective value function right now is: -2897.2874431579903
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.61452]
objective value function right now is: -2876.381127846391
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.74205]
objective value function right now is: -3044.8242176701556
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.03412]
objective value function right now is: -3032.062673279935
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.2219]
objective value function right now is: -2975.2596345793636
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.29234]
objective value function right now is: -3051.850011506599
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.245]
objective value function right now is: -3035.453332349903
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.33049]
objective value function right now is: -2977.1113429534935
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.20114]
objective value function right now is: -3047.912037682729
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.09204]
objective value function right now is: -3046.0535450070715
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.06969]
objective value function right now is: -3054.9467447974644
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [214.13533]
objective value function right now is: -3050.889205041322
min fval:  -3088.059024503695
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -1.6642,   0.2200],
        [ -1.6640,   0.2202],
        [-14.2824,  12.5865],
        [ -1.6644,   0.2199],
        [ -1.6641,   0.2201],
        [  6.5632,   4.4345],
        [ 23.7188,  -3.1788],
        [ -1.6642,   0.2200],
        [ 13.2400,  15.9223],
        [ -1.6645,   0.2198]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -5.8620,  -5.8621,   9.7037,  -5.8620,  -5.8620, -15.1364, -14.0266,
         -5.8620,  11.8252,  -5.8620], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 2.4426e-03,  2.4527e-03, -1.4393e+00,  2.4371e-03,  2.4495e-03,
         -5.3434e-01, -5.7187e-01,  2.4434e-03, -2.2848e+00,  2.4358e-03],
        [ 2.4425e-03,  2.4525e-03, -1.4393e+00,  2.4369e-03,  2.4494e-03,
         -5.3434e-01, -5.7187e-01,  2.4433e-03, -2.2848e+00,  2.4357e-03],
        [-3.3870e-02, -3.4042e-02, -1.7249e+01, -3.3766e-02, -3.3989e-02,
          1.1535e+01,  1.7552e+01, -3.3880e-02, -1.0647e+01, -3.3739e-02],
        [ 2.4425e-03,  2.4526e-03, -1.4393e+00,  2.4370e-03,  2.4494e-03,
         -5.3434e-01, -5.7187e-01,  2.4433e-03, -2.2848e+00,  2.4357e-03],
        [ 2.4425e-03,  2.4525e-03, -1.4393e+00,  2.4369e-03,  2.4493e-03,
         -5.3434e-01, -5.7187e-01,  2.4433e-03, -2.2848e+00,  2.4357e-03],
        [ 2.4426e-03,  2.4527e-03, -1.4393e+00,  2.4371e-03,  2.4495e-03,
         -5.3434e-01, -5.7186e-01,  2.4434e-03, -2.2848e+00,  2.4359e-03],
        [ 1.8270e-01,  1.8276e-01,  1.5515e+01,  1.8266e-01,  1.8274e-01,
          2.4888e-01, -1.4176e+01,  1.8270e-01,  4.6476e+00,  1.8265e-01],
        [ 4.6293e-01,  4.6308e-01,  1.7589e+01,  4.6285e-01,  4.6304e-01,
         -1.1054e+01, -1.6825e+01,  4.6294e-01,  5.4075e+00,  4.6282e-01],
        [ 2.4425e-03,  2.4525e-03, -1.4393e+00,  2.4369e-03,  2.4494e-03,
         -5.3434e-01, -5.7186e-01,  2.4433e-03, -2.2848e+00,  2.4357e-03],
        [ 2.4425e-03,  2.4525e-03, -1.4393e+00,  2.4369e-03,  2.4493e-03,
         -5.3434e-01, -5.7187e-01,  2.4433e-03, -2.2848e+00,  2.4357e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-3.1840, -3.1840, 11.9265, -3.1840, -3.1840, -3.1840, -6.0131, -7.4370,
        -3.1840, -3.1840], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.2303,  -0.2303, -19.9441,  -0.2303,  -0.2303,  -0.2303,   2.2195,
           8.8020,  -0.2303,  -0.2303]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -0.0451,   6.1791],
        [  0.7593,  -4.5607],
        [ -5.0093,  -0.2366],
        [ -8.1709,   6.7186],
        [-14.2165,  -8.3646],
        [-25.2660,  -1.4830],
        [-25.0663,   1.0526],
        [-18.8863,   1.2476],
        [-21.0892,  -3.8272],
        [ 25.7293,  10.1660]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-3.7827, -4.0401, -3.2205,  5.6508, -5.9248, 18.0168, -0.9090,  2.0474,
         8.4924,  8.4892], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 1.5123e+00,  4.9789e+00, -3.2326e+00,  1.4836e-01,  6.4102e+00,
         -8.0065e+00, -2.9692e+00, -3.6398e+00, -8.2168e-01, -4.4785e+00],
        [-4.7494e-01,  3.3424e+00, -1.3109e-01,  5.5933e-02,  2.0409e+00,
         -2.4503e+00,  4.6293e-01,  7.1544e-01,  4.7596e-01, -5.2584e+00],
        [-4.0140e+00,  6.7634e+00,  2.9287e+00, -1.0128e+01,  3.6400e+00,
          4.7080e+00,  5.6766e+00, -6.7393e+00,  6.1886e-01, -3.7161e+00],
        [-1.9566e+01, -4.1399e+00,  1.8420e+00, -2.6940e+01,  2.1418e+00,
         -1.1284e+01,  1.8828e-04,  4.2566e+00,  2.0603e+00, -2.9578e+00],
        [-3.7683e+00,  5.7293e+00,  4.8106e+00,  1.2388e+01, -5.2083e+00,
          1.6045e+00,  8.0609e+00,  8.0180e+00,  4.2716e+00, -1.1570e+01],
        [-4.1224e-01,  9.9679e+00, -4.7977e+00, -6.4519e+01,  6.8170e+00,
          8.9301e-01, -1.2428e+01, -1.2640e+01,  2.4696e+01, -2.9009e+01],
        [-2.7142e+00,  6.7578e+00,  2.9910e+00, -1.4793e+01,  9.7536e+00,
          1.5342e+01, -3.8166e+00, -3.4509e+00,  8.0222e+00, -1.5439e+01],
        [ 5.5162e-01, -9.5321e+00, -6.1862e-01, -4.7041e+00,  1.6496e+00,
          2.0588e+00, -1.8171e+00, -7.0648e+00,  3.4485e-01, -4.6692e+00],
        [-7.0881e+00, -5.0167e+00,  2.2634e-02, -2.3215e+01,  4.2879e+00,
         -1.3750e+01,  3.4764e-03,  2.3174e-04, -1.5181e+01, -2.8039e+00],
        [-1.7911e+01, -1.4464e+00,  5.7957e+00,  1.7269e+01, -2.1223e+01,
          3.2125e-01, -3.1876e-02,  1.4820e+01,  1.4142e-02,  1.1715e+00]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -6.0739,  -7.0162,  -9.1784,  -2.9716,  -3.7971, -16.7715, -17.1763,
         -4.7021,  -5.0947,   0.3935], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[  2.8853,   0.1714,  -2.8316,   6.1902,   0.3509,  18.8505,  -8.0302,
          -1.5423,   4.8987,   0.5316],
        [ -2.9004,  -0.1713,   2.8318,  -6.1902,  -0.3503, -18.8414,   8.0424,
           1.5423,  -4.8985,  -0.6597]], device='cuda:0'))])
xi:  [214.245]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 628.5422235836606
W_T_median: 465.3843405447223
W_T_pctile_5: 218.47879405188988
W_T_CVAR_5_pct: 33.30430484014646
Average q (qsum/M+1):  46.05180900327621
Optimal xi:  [214.245]
Expected(across Rb) median(across samples) p_equity:  0.22331277653574944
obj fun:  tensor(-3088.0590, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1164.1098552935396
W_T_median: 917.4991733058827
W_T_pctile_5: 223.7662744337785
W_T_CVAR_5_pct: 35.462864123301635
Average q (qsum/M+1):  35.0
Optimal xi:  [220.98354]
Expected(across Rb) median(across samples) p_equity:  0.22134497140844664
obj fun:  tensor(-35.4246, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 999.0
-----------------------------------------------
