Starting at: 
09-03-23_17:08

 numpy seed:  3  


 pytorch seed:  3  

Key parameters-------
paths: 256000
iterations: 30000
batchsize: 1000
remove neg:  False
w constaint activation:  yy_fix_jan29


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_real_ret', 'VWD_real_ret']
############# End: defined asset  basket #################
               CPI  CPI_nom_ret  ...  VWD_real_ret  EWD_real_ret
date                             ...                            
192512  100.000000          NaN  ...           NaN           NaN
192601  100.000000     0.000000  ...      0.000561      0.023174
192602  100.000000     0.000000  ...     -0.033046     -0.053510
192603   99.441303    -0.005587  ...     -0.058743     -0.091750
192604  100.000000     0.005618  ...      0.031235      0.027204

[5 rows x 12 columns]
                CPI  CPI_nom_ret  ...  VWD_real_ret  EWD_real_ret
date                              ...                            
202008  1452.055956     0.003153  ...      0.065084      0.035618
202009  1454.078149     0.001393  ...     -0.036399     -0.028709
202010  1454.681696     0.000415  ...     -0.020584      0.000169
202011  1453.793232    -0.000611  ...      0.124393      0.175130
202012  1455.162018     0.000942  ...      0.044065      0.071843

[5 rows x 12 columns]


############# Defined asset basket #################
timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names'])
timeseries_basket['basket_type'] = asset
timeseries_basket['basket_id'] = B10_and_VWD
timeseries_basket['basket_desc'] = CRSP data: B10 and VWD
timeseries_basket['basket_columns'] = 
['B10_nom_ret', 'VWD_nom_ret']
############# End: defined asset  basket #################
        CPI_nom_ret  B10_nom_ret  VWD_nom_ret
date                                         
192601     0.000000     0.004350     0.000561
192602     0.000000     0.004338    -0.033046
192603    -0.005587     0.004327    -0.064002
192604     0.005618     0.004316     0.037029
192605    -0.005587     0.004304     0.012095
        CPI_nom_ret  B10_nom_ret  VWD_nom_ret
date                                         
202008     0.003153    -0.014709     0.068443
202009     0.001393     0.002560    -0.035057
202010     0.000415    -0.017311    -0.020178
202011    -0.000611     0.004016     0.123706
202012     0.000942    -0.004965     0.045048
Indices constructed with column names:
Index(['date_for_plt', 'CPI_nom_ret_ind', 'B10_nom_ret_ind',
       'VWD_nom_ret_ind'],
      dtype='object')
############# Updated: defined asset basket #################
timeseries_basket['data_df_mean'] = 
B10_real_ret    0.001986
VWD_real_ret    0.006974
dtype: float64


timeseries_basket['data_df_stdev'] = 
B10_real_ret    0.018939
VWD_real_ret    0.053569
dtype: float64


timeseries_basket['data_df_corr'] = 
              B10_real_ret  VWD_real_ret
B10_real_ret      1.000000      0.075353
VWD_real_ret      0.075353      1.000000


timeseries_basket.keys() = 
dict_keys(['basket_type', 'basket_id', 'basket_desc', 'basket_label', 'basket_columns', 'basket_timeseries_names', 'data_df', 'data_df_mean', 'data_df_stdev', 'data_df_corr'])
############# End: updated: defined asset basket #################
-----------------------------------------------
Dates to be REMOVED due to missing values (should be at *start* only due to trading signals):
Int64Index([], dtype='int64')
-----------------------------------------------
Dates from available data for bootstrapping:
Start: 192601
End: 202012
-----------------------------------------------
Bootstrap block size: 3
0.0% of bootstrap sample done.
5.0% of bootstrap sample done.
10.0% of bootstrap sample done.
15.0% of bootstrap sample done.
20.0% of bootstrap sample done.
25.0% of bootstrap sample done.
30.0% of bootstrap sample done.
35.0% of bootstrap sample done.
40.0% of bootstrap sample done.
45.0% of bootstrap sample done.
50.0% of bootstrap sample done.
55.00000000000001% of bootstrap sample done.
60.0% of bootstrap sample done.
65.0% of bootstrap sample done.
70.0% of bootstrap sample done.
75.0% of bootstrap sample done.
80.0% of bootstrap sample done.
85.0% of bootstrap sample done.
90.0% of bootstrap sample done.
95.0% of bootstrap sample done.
Withdrawal NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       1       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       1              none   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 1)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
Allocation NN:
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes activation x_l(weights)  \
0        obj.layers[0]        0   input_layer       2       None         None   
1        obj.layers[1]        1  hidden_layer    None       None         None   
2        obj.layers[2]        2  hidden_layer    None       None         None   
3        obj.layers[3]        3  output_layer       2       None         None   

  add_bias b_l(biases)  
0    False        None  
1    False        None  
2    False        None  
3    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
Neural_Network object 'obj' has instance attribute obj.layers.
obj.layers is a list of 'Neural_Network_Layer' objects with following attributes:

/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
/home/marcchen/Documents/testing_pyt_decum/researchcode/class_Neural_Network.py:344: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  layers_info_df = layers_info_df.append(layer_dict, ignore_index=True)
  obj.layers[layer_id] layer_id   description n_nodes        activation  \
0        obj.layers[0]        0   input_layer       2              None   
1        obj.layers[1]        1  hidden_layer      10  logistic_sigmoid   
2        obj.layers[2]        2  hidden_layer      10  logistic_sigmoid   
3        obj.layers[3]        3  output_layer       2           softmax   

  x_l(weights) add_bias b_l(biases)  
0         None    False        None  
1      (2, 10)     True          10  
2     (10, 10)     True          10  
3      (10, 2)    False        None  

 Run 'update_layer' method of Neural_Network object to change layer attributes.
--------------------------------------------------------------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
{'methods': ['Adam'], 'output_progress': False, 'tol': 1e-06, 'itbound_scipy_algorithms': 1000, 'check_exit_criteria': False, 'nit_running_min': 3000, 'itbound_SGD_algorithms': 30000, 'nit_IterateAveragingStart': 27000, 'batchsize': 1000, 'SGD_learningrate': 50.0, 'Adagrad_epsilon': 1e-08, 'Adagrad_eta': 1.0, 'Adadelta_ewma': 0.9, 'Adadelta_epsilon': 1e-08, 'RMSprop_ewma': 0.8, 'RMSprop_epsilon': 1e-08, 'RMSprop_eta': 0.1, 'Adam_ewma_1': 0.9, 'Adam_ewma_2': 0.998, 'Adam_eta': 0.05, 'Adam_epsilon': 1e-08, 'Adam_weight_decay': 0.0001, 'running_min_from_avg': False, 'running_min_from_sgd': True, 'lr_schedule': True}
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1506.8368317040647
W_T_median: 1144.7259758050877
W_T_pctile_5: -92.78690539124324
W_T_CVAR_5_pct: -256.52592661793926
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1695.2943244467708
Current xi:  [80.25336]
objective value function right now is: -1695.2943244467708
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1703.9425651577073
Current xi:  [58.758457]
objective value function right now is: -1703.9425651577073
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1709.8136668978605
Current xi:  [36.428585]
objective value function right now is: -1709.8136668978605
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1714.1650115053728
Current xi:  [13.646142]
objective value function right now is: -1714.1650115053728
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1717.6735853497908
Current xi:  [-7.1291366]
objective value function right now is: -1717.6735853497908
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1721.7905224823812
Current xi:  [-28.070366]
objective value function right now is: -1721.7905224823812
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1724.4295987030205
Current xi:  [-47.396812]
objective value function right now is: -1724.4295987030205
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1727.1665933774398
Current xi:  [-68.366455]
objective value function right now is: -1727.1665933774398
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1729.383582041154
Current xi:  [-87.633484]
objective value function right now is: -1729.383582041154
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1731.4158887345754
Current xi:  [-108.28088]
objective value function right now is: -1731.4158887345754
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1731.6418273549002
Current xi:  [-127.75331]
objective value function right now is: -1731.6418273549002
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1735.1640913398364
Current xi:  [-148.06133]
objective value function right now is: -1735.1640913398364
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1736.9246198213289
Current xi:  [-167.97203]
objective value function right now is: -1736.9246198213289
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1738.1373377938814
Current xi:  [-187.57848]
objective value function right now is: -1738.1373377938814
30.0% of gradient descent iterations done. Method = Adam
new min fval:  -1739.5070547349442
Current xi:  [-207.66594]
objective value function right now is: -1739.5070547349442
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1740.7068488970751
Current xi:  [-227.20732]
objective value function right now is: -1740.7068488970751
34.0% of gradient descent iterations done. Method = Adam
new min fval:  -1741.560373156123
Current xi:  [-246.69164]
objective value function right now is: -1741.560373156123
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1742.425833285905
Current xi:  [-265.60828]
objective value function right now is: -1742.425833285905
38.0% of gradient descent iterations done. Method = Adam
new min fval:  -1744.0635244886464
Current xi:  [-284.2943]
objective value function right now is: -1744.0635244886464
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-302.40063]
objective value function right now is: -1743.7074665292917
42.0% of gradient descent iterations done. Method = Adam
new min fval:  -1744.6868857373731
Current xi:  [-320.3494]
objective value function right now is: -1744.6868857373731
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1745.600982934236
Current xi:  [-337.2224]
objective value function right now is: -1745.600982934236
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-353.48257]
objective value function right now is: -1745.4361269573105
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1746.4641023620018
Current xi:  [-369.43066]
objective value function right now is: -1746.4641023620018
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-384.02774]
objective value function right now is: -1745.68567326604
52.0% of gradient descent iterations done. Method = Adam
new min fval:  -1746.6534671966956
Current xi:  [-397.0403]
objective value function right now is: -1746.6534671966956
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1746.8543741172657
Current xi:  [-409.9728]
objective value function right now is: -1746.8543741172657
56.00000000000001% of gradient descent iterations done. Method = Adam
new min fval:  -1746.9558149751429
Current xi:  [-421.4625]
objective value function right now is: -1746.9558149751429
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-430.71207]
objective value function right now is: -1746.3637882816352
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.3817562084591
Current xi:  [-437.42215]
objective value function right now is: -1747.3817562084591
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-444.5945]
objective value function right now is: -1747.33920058202
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-448.7357]
objective value function right now is: -1747.19020260417
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-453.7838]
objective value function right now is: -1747.0963521035328
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-455.7479]
objective value function right now is: -1747.3500724683768
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-457.31113]
objective value function right now is: -1747.2085237755575
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.5261923061844
Current xi:  [-457.12405]
objective value function right now is: -1747.5261923061844
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-457.16217]
objective value function right now is: -1747.4460107700886
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.5578307920314
Current xi:  [-457.3896]
objective value function right now is: -1747.5578307920314
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.6249100614284
Current xi:  [-457.45544]
objective value function right now is: -1747.6249100614284
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-457.88214]
objective value function right now is: -1747.5924117534666
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-457.98685]
objective value function right now is: -1747.5558944706722
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-458.09146]
objective value function right now is: -1747.6134146188092
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1747.9185092653447
Current xi:  [-458.06805]
objective value function right now is: -1747.9185092653447
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1748.0824050777821
Current xi:  [-458.49048]
objective value function right now is: -1748.0824050777821
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-458.35965]
objective value function right now is: -1747.9970519251704
new min fval from sgd:  -1748.1008903696347
new min fval from sgd:  -1748.1234313015793
new min fval from sgd:  -1748.1388995540199
new min fval from sgd:  -1748.1429804140494
new min fval from sgd:  -1748.1451495011809
new min fval from sgd:  -1748.1454698685955
new min fval from sgd:  -1748.1492958869776
new min fval from sgd:  -1748.1498767779121
new min fval from sgd:  -1748.1586050996486
new min fval from sgd:  -1748.1621593120876
new min fval from sgd:  -1748.1639912788696
new min fval from sgd:  -1748.1641085692304
new min fval from sgd:  -1748.1644147403488
new min fval from sgd:  -1748.1667352708603
new min fval from sgd:  -1748.1733521553115
new min fval from sgd:  -1748.1766829900318
new min fval from sgd:  -1748.1798168398836
new min fval from sgd:  -1748.183855060517
new min fval from sgd:  -1748.1865064831293
new min fval from sgd:  -1748.1905328970956
new min fval from sgd:  -1748.1917920190533
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-458.0429]
objective value function right now is: -1748.0397432121063
new min fval from sgd:  -1748.194362594635
new min fval from sgd:  -1748.2007770089654
new min fval from sgd:  -1748.203793994571
new min fval from sgd:  -1748.2077957671104
new min fval from sgd:  -1748.2103844946098
new min fval from sgd:  -1748.211272215225
new min fval from sgd:  -1748.212376364509
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-458.19504]
objective value function right now is: -1748.1502481536745
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-458.13696]
objective value function right now is: -1748.1493497029176
new min fval from sgd:  -1748.2134768994958
new min fval from sgd:  -1748.2137740511835
new min fval from sgd:  -1748.2146018079588
new min fval from sgd:  -1748.2159249128756
new min fval from sgd:  -1748.2175522187717
new min fval from sgd:  -1748.218935421905
new min fval from sgd:  -1748.2191936126376
new min fval from sgd:  -1748.220187855784
new min fval from sgd:  -1748.2224824735583
new min fval from sgd:  -1748.2237607050647
new min fval from sgd:  -1748.2248256664775
new min fval from sgd:  -1748.2248770767892
new min fval from sgd:  -1748.2254668535838
new min fval from sgd:  -1748.2260503518735
new min fval from sgd:  -1748.2260895992556
new min fval from sgd:  -1748.2267612371868
new min fval from sgd:  -1748.228705738605
new min fval from sgd:  -1748.229849903923
new min fval from sgd:  -1748.2307011957007
new min fval from sgd:  -1748.2310801925703
new min fval from sgd:  -1748.2328471651833
new min fval from sgd:  -1748.234748295933
new min fval from sgd:  -1748.2367166495715
new min fval from sgd:  -1748.2385160192493
new min fval from sgd:  -1748.2410755631563
new min fval from sgd:  -1748.242766205771
new min fval from sgd:  -1748.243772134321
new min fval from sgd:  -1748.2438241341085
new min fval from sgd:  -1748.2451115717915
new min fval from sgd:  -1748.2473822105833
new min fval from sgd:  -1748.2499902939337
new min fval from sgd:  -1748.251288564923
new min fval from sgd:  -1748.2528615762071
new min fval from sgd:  -1748.2539377977562
new min fval from sgd:  -1748.255042801144
new min fval from sgd:  -1748.2558305944935
new min fval from sgd:  -1748.2563081732296
new min fval from sgd:  -1748.2565343514334
new min fval from sgd:  -1748.256693586187
new min fval from sgd:  -1748.2571209943123
new min fval from sgd:  -1748.2574797588597
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-458.1922]
objective value function right now is: -1748.192155263838
new min fval from sgd:  -1748.2575347910729
new min fval from sgd:  -1748.2588332591208
new min fval from sgd:  -1748.2600953733315
new min fval from sgd:  -1748.2609376600535
new min fval from sgd:  -1748.2610000041109
new min fval from sgd:  -1748.2615229523644
new min fval from sgd:  -1748.2617037904351
new min fval from sgd:  -1748.2617292223408
new min fval from sgd:  -1748.2621373855868
new min fval from sgd:  -1748.2630532106757
new min fval from sgd:  -1748.2645864623623
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-458.21777]
objective value function right now is: -1748.2514251880175
min fval:  -1748.2645864623623
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.3890,  1.2705],
        [-0.3890,  1.2705],
        [-3.1769,  6.0819],
        [13.5058,  1.4187],
        [-0.3899,  1.2758],
        [-0.3890,  1.2704],
        [-7.1279,  3.6605],
        [-0.3890,  1.2705],
        [-2.7274,  6.0897],
        [-7.4749, -4.6178]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.7172, -0.7172, 10.7230, -8.1522, -0.7211, -0.7172,  8.5092, -0.7172,
        10.6407,  0.0170], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01],
        [-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01],
        [ 1.2597e-01,  1.2597e-01, -7.3658e+00, -9.7405e+00,  8.8391e-02,
          1.2638e-01, -4.9376e+00,  1.2597e-01, -6.9440e+00,  1.7873e+00],
        [ 8.3322e-02,  8.3323e-02,  2.1481e+00,  3.8260e+00,  7.0418e-02,
          8.3492e-02,  1.7870e+00,  8.3323e-02,  2.0981e+00, -7.7238e-02],
        [-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01],
        [-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01],
        [ 4.2223e-02,  4.2225e-02,  3.4672e+00,  5.0553e+00,  6.1523e-03,
          4.2634e-02,  2.6714e+00,  4.2224e-02,  3.3334e+00, -1.9996e-01],
        [ 2.2983e-02,  2.2984e-02,  3.9047e+00,  5.5445e+00, -1.6427e-02,
          2.3378e-02,  2.9206e+00,  2.2984e-02,  3.7580e+00, -3.6710e-01],
        [-5.6500e-03, -5.6500e-03, -3.3708e-02, -1.4784e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6500e-03, -4.0836e-02, -4.7452e-01],
        [-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6068e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5222, -0.5222,  4.8970, -3.0995, -0.5222, -0.5222, -3.3636, -3.4951,
        -0.5221, -0.5222], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.4894e-03,  1.4894e-03, -1.3247e+01,  3.5944e+00,  1.4894e-03,
          1.4894e-03,  5.2154e+00,  5.9972e+00,  1.4895e-03,  1.4894e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 12.8302,   3.0636],
        [ -4.1623,  -9.0103],
        [  2.6807,  -7.5523],
        [-16.5897,  -5.9841],
        [-15.5150,  -2.9124],
        [-10.2233,   0.7646],
        [ -2.3654,  -4.4019],
        [-10.1674,  -9.3383],
        [ -8.7847,   0.8757],
        [  7.9507,   8.6118]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -2.6631, -11.6716,  -7.2738,  -3.6322,   1.6722,  11.6587,  -1.8508,
         -7.5078,   5.2666,   7.2578], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5515e+00,  1.5561e+00,  2.6742e+00, -3.0320e+00, -4.6687e+00,
         -9.0387e-01,  2.2213e+00,  5.3288e-01,  1.4450e+00, -4.0020e+00],
        [-2.4323e-01, -8.3041e-01, -1.0577e+00, -6.7899e-01, -5.0301e-01,
         -9.9127e-01, -1.5997e+00, -9.8904e-01, -3.1749e+00, -3.1361e-01],
        [-1.0866e+00, -4.0812e-01, -5.3434e+00,  9.1229e-01,  5.7108e+00,
         -1.3832e+00, -7.6576e+00, -2.6274e+00,  5.6125e+00, -8.4968e-01],
        [-4.1727e+00,  1.6408e-01,  2.5270e-02,  6.8018e+00,  9.1168e-03,
         -5.8992e+00, -3.1741e-01,  4.0828e-01, -3.6580e+00,  2.4430e-02],
        [-1.0131e+00, -3.2826e-01, -3.7271e+00, -1.5399e+00,  3.6249e+00,
         -1.3771e+00, -4.3775e+00, -2.7616e+00,  4.7750e+00, -7.7867e-01],
        [-1.5592e+00,  8.8254e+00, -9.7969e-01,  3.2892e+00,  8.9069e+00,
          4.6782e-03, -4.5292e+00, -2.0707e+00,  1.4748e+00, -2.4894e+00],
        [-4.3279e+00, -1.2782e+01,  4.2846e+00, -2.9663e+00, -4.8306e+00,
          6.2756e+00,  1.9816e-01,  7.3166e+00, -3.8854e+00, -7.5519e+00],
        [-4.0843e-01, -7.9911e-01, -1.1385e+00, -6.5685e-01, -4.2991e-01,
         -1.1014e+00, -1.6045e+00, -9.1947e-01, -2.6564e+00, -4.4088e-01],
        [-2.9625e+00,  1.0796e+00, -2.9583e-01,  5.3380e+00,  1.4345e+00,
         -4.7627e+00, -8.6064e-01, -3.2371e-01, -3.5119e+00,  1.2628e+00],
        [-8.1147e-01,  9.6900e+00, -5.2043e+00,  7.9194e+00,  1.0944e+00,
          1.1123e+00, -3.7378e+00, -4.9453e+00,  7.7430e+00,  1.2268e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.1657, -1.2410, -1.9436, -0.8159, -1.8813, -6.2588, -1.2579, -1.3573,
        -0.8754, -1.5403], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-0.3370, -1.0365, -4.3117, -4.1120, -2.4344,  5.4253, -7.8169, -0.8955,
         -1.9441,  0.7044],
        [ 0.1068,  1.0365,  4.3118,  4.1068,  2.4344, -5.3462,  7.8905,  0.8955,
          2.2557, -0.8519]], device='cuda:0'))])
xi:  [-458.1955]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1049.2216778473355
W_T_median: 129.98417084447914
W_T_pctile_5: -459.1263625082792
W_T_CVAR_5_pct: -553.9003022666624
Average q (qsum/M+1):  57.255189957157256
Optimal xi:  [-458.1955]
Observed VAR:  129.98417084447914
Expected(across Rb) median(across samples) p_equity:  0.5671622738242149
obj fun:  tensor(-1748.2646, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.05
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.3890,  1.2705],
        [-0.3890,  1.2705],
        [-3.1769,  6.0819],
        [13.5058,  1.4187],
        [-0.3899,  1.2758],
        [-0.3890,  1.2704],
        [-7.1279,  3.6605],
        [-0.3890,  1.2705],
        [-2.7274,  6.0897],
        [-7.4749, -4.6178]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.7172, -0.7172, 10.7230, -8.1522, -0.7211, -0.7172,  8.5092, -0.7172,
        10.6407,  0.0170], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01],
        [-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01],
        [ 1.2597e-01,  1.2597e-01, -7.3658e+00, -9.7405e+00,  8.8391e-02,
          1.2638e-01, -4.9376e+00,  1.2597e-01, -6.9440e+00,  1.7873e+00],
        [ 8.3322e-02,  8.3323e-02,  2.1481e+00,  3.8260e+00,  7.0418e-02,
          8.3492e-02,  1.7870e+00,  8.3323e-02,  2.0981e+00, -7.7238e-02],
        [-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01],
        [-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01],
        [ 4.2223e-02,  4.2225e-02,  3.4672e+00,  5.0553e+00,  6.1523e-03,
          4.2634e-02,  2.6714e+00,  4.2224e-02,  3.3334e+00, -1.9996e-01],
        [ 2.2983e-02,  2.2984e-02,  3.9047e+00,  5.5445e+00, -1.6427e-02,
          2.3378e-02,  2.9206e+00,  2.2984e-02,  3.7580e+00, -3.6710e-01],
        [-5.6500e-03, -5.6500e-03, -3.3708e-02, -1.4784e-01, -5.6069e-03,
         -5.6504e-03, -1.8284e-02, -5.6500e-03, -4.0836e-02, -4.7452e-01],
        [-5.6499e-03, -5.6499e-03, -3.3709e-02, -1.4785e-01, -5.6068e-03,
         -5.6504e-03, -1.8284e-02, -5.6499e-03, -4.0836e-02, -4.7452e-01]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.5222, -0.5222,  4.8970, -3.0995, -0.5222, -0.5222, -3.3636, -3.4951,
        -0.5221, -0.5222], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ 1.4894e-03,  1.4894e-03, -1.3247e+01,  3.5944e+00,  1.4894e-03,
          1.4894e-03,  5.2154e+00,  5.9972e+00,  1.4895e-03,  1.4894e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 12.8302,   3.0636],
        [ -4.1623,  -9.0103],
        [  2.6807,  -7.5523],
        [-16.5897,  -5.9841],
        [-15.5150,  -2.9124],
        [-10.2233,   0.7646],
        [ -2.3654,  -4.4019],
        [-10.1674,  -9.3383],
        [ -8.7847,   0.8757],
        [  7.9507,   8.6118]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -2.6631, -11.6716,  -7.2738,  -3.6322,   1.6722,  11.6587,  -1.8508,
         -7.5078,   5.2666,   7.2578], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[ 2.5515e+00,  1.5561e+00,  2.6742e+00, -3.0320e+00, -4.6687e+00,
         -9.0387e-01,  2.2213e+00,  5.3288e-01,  1.4450e+00, -4.0020e+00],
        [-2.4323e-01, -8.3041e-01, -1.0577e+00, -6.7899e-01, -5.0301e-01,
         -9.9127e-01, -1.5997e+00, -9.8904e-01, -3.1749e+00, -3.1361e-01],
        [-1.0866e+00, -4.0812e-01, -5.3434e+00,  9.1229e-01,  5.7108e+00,
         -1.3832e+00, -7.6576e+00, -2.6274e+00,  5.6125e+00, -8.4968e-01],
        [-4.1727e+00,  1.6408e-01,  2.5270e-02,  6.8018e+00,  9.1168e-03,
         -5.8992e+00, -3.1741e-01,  4.0828e-01, -3.6580e+00,  2.4430e-02],
        [-1.0131e+00, -3.2826e-01, -3.7271e+00, -1.5399e+00,  3.6249e+00,
         -1.3771e+00, -4.3775e+00, -2.7616e+00,  4.7750e+00, -7.7867e-01],
        [-1.5592e+00,  8.8254e+00, -9.7969e-01,  3.2892e+00,  8.9069e+00,
          4.6782e-03, -4.5292e+00, -2.0707e+00,  1.4748e+00, -2.4894e+00],
        [-4.3279e+00, -1.2782e+01,  4.2846e+00, -2.9663e+00, -4.8306e+00,
          6.2756e+00,  1.9816e-01,  7.3166e+00, -3.8854e+00, -7.5519e+00],
        [-4.0843e-01, -7.9911e-01, -1.1385e+00, -6.5685e-01, -4.2991e-01,
         -1.1014e+00, -1.6045e+00, -9.1947e-01, -2.6564e+00, -4.4088e-01],
        [-2.9625e+00,  1.0796e+00, -2.9583e-01,  5.3380e+00,  1.4345e+00,
         -4.7627e+00, -8.6064e-01, -3.2371e-01, -3.5119e+00,  1.2628e+00],
        [-8.1147e-01,  9.6900e+00, -5.2043e+00,  7.9194e+00,  1.0944e+00,
          1.1123e+00, -3.7378e+00, -4.9453e+00,  7.7430e+00,  1.2268e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-3.1657, -1.2410, -1.9436, -0.8159, -1.8813, -6.2588, -1.2579, -1.3573,
        -0.8754, -1.5403], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-0.3370, -1.0365, -4.3117, -4.1120, -2.4344,  5.4253, -7.8169, -0.8955,
         -1.9441,  0.7044],
        [ 0.1068,  1.0365,  4.3118,  4.1068,  2.4344, -5.3462,  7.8905,  0.8955,
          2.2557, -0.8519]], device='cuda:0'))])
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1506.8368317040647
W_T_median: 1144.7259758050877
W_T_pctile_5: -92.78690539124324
W_T_CVAR_5_pct: -256.52592661793926
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1645.5888033919218
Current xi:  [81.80711]
objective value function right now is: -1645.5888033919218
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1649.911663664912
Current xi:  [61.122223]
objective value function right now is: -1649.911663664912
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1658.1529385621884
Current xi:  [41.098206]
objective value function right now is: -1658.1529385621884
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1662.982357692469
Current xi:  [22.592346]
objective value function right now is: -1662.982357692469
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1665.7433705681424
Current xi:  [4.262971]
objective value function right now is: -1665.7433705681424
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1668.157161355987
Current xi:  [-5.507751]
objective value function right now is: -1668.157161355987
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1671.4597520595364
Current xi:  [-19.642653]
objective value function right now is: -1671.4597520595364
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1673.9618927263307
Current xi:  [-37.23131]
objective value function right now is: -1673.9618927263307
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1675.5076514296504
Current xi:  [-49.583233]
objective value function right now is: -1675.5076514296504
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.3269956774852
Current xi:  [-67.911934]
objective value function right now is: -1678.3269956774852
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1678.6728129243827
Current xi:  [-80.58157]
objective value function right now is: -1678.6728129243827
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -1680.0328828451968
Current xi:  [-94.826645]
objective value function right now is: -1680.0328828451968
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1682.551718689437
Current xi:  [-113.15684]
objective value function right now is: -1682.551718689437
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1683.1653058370007
Current xi:  [-122.76857]
objective value function right now is: -1683.1653058370007
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-135.60138]
objective value function right now is: -1683.0690403131555
32.0% of gradient descent iterations done. Method = Adam
new min fval:  -1684.1467308753356
Current xi:  [-151.94685]
objective value function right now is: -1684.1467308753356
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [-162.31241]
objective value function right now is: -1684.0521743100687
36.0% of gradient descent iterations done. Method = Adam
new min fval:  -1685.130045891495
Current xi:  [-170.56288]
objective value function right now is: -1685.130045891495
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [-179.3903]
objective value function right now is: -1685.049516927755
40.0% of gradient descent iterations done. Method = Adam
new min fval:  -1685.6758750002332
Current xi:  [-192.07672]
objective value function right now is: -1685.6758750002332
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [-199.90361]
objective value function right now is: -1685.5613274812963
44.0% of gradient descent iterations done. Method = Adam
new min fval:  -1685.7614855604118
Current xi:  [-203.64218]
objective value function right now is: -1685.7614855604118
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.0088848261662
Current xi:  [-203.82788]
objective value function right now is: -1686.0088848261662
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.1964655274508
Current xi:  [-204.03073]
objective value function right now is: -1686.1964655274508
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.003]
objective value function right now is: -1684.5378082128861
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.2914]
objective value function right now is: -1685.6696882241463
54.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.3908993729028
Current xi:  [-204.48279]
objective value function right now is: -1686.3908993729028
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-204.1918]
objective value function right now is: -1685.5403325545517
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-203.5714]
objective value function right now is: -1686.363968200206
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.49904]
objective value function right now is: -1686.236680097986
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.77684]
objective value function right now is: -1686.214684571651
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.30986]
objective value function right now is: -1686.3017008080687
66.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.4434320908467
Current xi:  [-204.28485]
objective value function right now is: -1686.4434320908467
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.67206]
objective value function right now is: -1685.8772260429082
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-205.68173]
objective value function right now is: -1685.5925626732508
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.5264801159208
Current xi:  [-205.53029]
objective value function right now is: -1686.5264801159208
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.7405295020362
Current xi:  [-205.41673]
objective value function right now is: -1686.7405295020362
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-205.55025]
objective value function right now is: -1686.5594826257823
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1686.7754460161827
Current xi:  [-205.30219]
objective value function right now is: -1686.7754460161827
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-205.15894]
objective value function right now is: -1686.566182610792
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-205.12912]
objective value function right now is: -1686.7527085128997
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-205.052]
objective value function right now is: -1686.656235078452
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.83104]
objective value function right now is: -1686.6887939521741
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.93352]
objective value function right now is: -1686.7548101642967
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.89636]
objective value function right now is: -1686.7167823117832
new min fval from sgd:  -1686.7772781585559
new min fval from sgd:  -1686.7789838408078
new min fval from sgd:  -1686.7816808314267
new min fval from sgd:  -1686.7896724519774
new min fval from sgd:  -1686.792950077092
new min fval from sgd:  -1686.7959244812755
new min fval from sgd:  -1686.802178436284
new min fval from sgd:  -1686.8098330600703
new min fval from sgd:  -1686.8111767792118
new min fval from sgd:  -1686.8189604127422
new min fval from sgd:  -1686.8241487198522
new min fval from sgd:  -1686.827370322959
new min fval from sgd:  -1686.829253830963
new min fval from sgd:  -1686.8315252963416
new min fval from sgd:  -1686.8377693322188
new min fval from sgd:  -1686.840077782436
new min fval from sgd:  -1686.8508608300804
new min fval from sgd:  -1686.8524189477498
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.85645]
objective value function right now is: -1686.8145524590864
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.80368]
objective value function right now is: -1686.5713232233784
new min fval from sgd:  -1686.8547334469458
new min fval from sgd:  -1686.8625607149727
new min fval from sgd:  -1686.862833042295
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.7314]
objective value function right now is: -1686.6809878075985
new min fval from sgd:  -1686.8637813140278
new min fval from sgd:  -1686.86402633639
new min fval from sgd:  -1686.867663203233
new min fval from sgd:  -1686.8724117905122
new min fval from sgd:  -1686.8757094609434
new min fval from sgd:  -1686.8791848342098
new min fval from sgd:  -1686.8821389217794
new min fval from sgd:  -1686.8842856553058
new min fval from sgd:  -1686.8854726013365
new min fval from sgd:  -1686.8862861925666
new min fval from sgd:  -1686.8866271885113
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.89134]
objective value function right now is: -1686.8594629587383
new min fval from sgd:  -1686.887104776595
new min fval from sgd:  -1686.8873173384584
new min fval from sgd:  -1686.887421596211
new min fval from sgd:  -1686.887848124447
new min fval from sgd:  -1686.8885316228088
new min fval from sgd:  -1686.8887925285721
new min fval from sgd:  -1686.8896797890943
new min fval from sgd:  -1686.8907812174018
new min fval from sgd:  -1686.8921186301263
new min fval from sgd:  -1686.8924544617148
new min fval from sgd:  -1686.8927060138337
new min fval from sgd:  -1686.8928450494138
new min fval from sgd:  -1686.8932293549822
new min fval from sgd:  -1686.8935938973807
new min fval from sgd:  -1686.8939597746323
new min fval from sgd:  -1686.8942904233506
new min fval from sgd:  -1686.8944629962484
new min fval from sgd:  -1686.8949813698407
new min fval from sgd:  -1686.895447235904
new min fval from sgd:  -1686.8960541182576
new min fval from sgd:  -1686.8963845467392
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-204.87914]
objective value function right now is: -1686.863011650382
min fval:  -1686.8963845467392
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.8181,  1.2655],
        [-0.8181,  1.2655],
        [-6.7043,  8.6399],
        [-0.8181,  1.2655],
        [-0.8181,  1.2655],
        [-0.8181,  1.2655],
        [-0.8319,  1.2686],
        [-0.8181,  1.2655],
        [ 4.0510,  9.2020],
        [-0.8181,  1.2655]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.9350, -0.9350, 12.2100, -0.9350, -0.9350, -0.9350, -0.9740, -0.9350,
         9.9181, -0.9350], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-9.6010e-03, -9.6010e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6010e-03, -5.0271e-01, -9.6008e-03],
        [-9.6010e-03, -9.6011e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6011e-03, -5.0271e-01, -9.6008e-03],
        [ 5.9245e-01,  5.9245e-01, -8.2989e+00,  5.9239e-01,  5.9240e-01,
          5.9246e-01,  4.7607e-01,  5.9245e-01, -9.4766e+00,  5.9239e-01],
        [-1.6111e-01, -1.6111e-01,  4.4591e+00, -1.6113e-01, -1.6113e-01,
         -1.6111e-01, -2.3334e-01, -1.6111e-01,  4.1745e+00, -1.6113e-01],
        [-9.6010e-03, -9.6010e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6010e-03, -5.0271e-01, -9.6008e-03],
        [-9.6011e-03, -9.6011e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6011e-03, -5.0271e-01, -9.6008e-03],
        [-1.7597e-01, -1.7597e-01,  5.2062e+00, -1.7602e-01, -1.7601e-01,
         -1.7597e-01, -2.4063e-01, -1.7597e-01,  4.7205e+00, -1.7602e-01],
        [-2.0765e-01, -2.0765e-01,  5.9716e+00, -2.0773e-01, -2.0771e-01,
         -2.0764e-01, -2.7815e-01, -2.0765e-01,  5.3062e+00, -2.0773e-01],
        [-9.6010e-03, -9.6011e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6011e-03, -5.0271e-01, -9.6008e-03],
        [-9.6010e-03, -9.6010e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6011e-03, -5.0271e-01, -9.6008e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7303, -0.7303,  8.0176, -4.1127, -0.7303, -0.7303, -4.6390, -5.1923,
        -0.7303, -0.7303], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0248,  -0.0248, -14.4336,   4.4326,  -0.0248,  -0.0248,   5.3747,
           6.5926,  -0.0248,  -0.0248]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 14.9886,   2.4124],
        [ -3.9607,  -6.8647],
        [  2.8421, -15.0439],
        [-15.0104,  -1.3554],
        [-17.8803,  -2.1979],
        [-13.6356,   0.1386],
        [  1.5865,  -2.9758],
        [ -9.1459,  -9.2862],
        [-12.0836,   1.7614],
        [ 12.6914,  10.9896]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.8115, -13.3717, -15.6698,   0.1876,   3.0066,  12.2546,  -2.6500,
         -8.0082,   6.1204,  11.2100], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-9.9379e-01, -4.8549e+00,  6.0099e+00, -1.0764e+01,  1.1539e+00,
          1.3655e+00, -1.6340e+00, -1.3220e+00,  3.1557e+00, -2.2392e+00],
        [ 3.0425e-01, -2.2314e+00, -1.1031e+00, -4.3235e-01, -1.0041e+00,
          4.4433e-01,  1.8019e+00, -1.6744e+00, -6.2733e+00, -1.5991e+00],
        [-1.5442e+00, -9.9958e-03, -6.8393e+00,  4.0706e+00,  9.5080e-01,
          4.8177e+00, -9.3012e+00, -5.4022e+00,  3.5393e+00, -2.0912e+00],
        [-3.6220e+00,  2.5934e+00,  1.8413e-03,  5.1178e+00,  3.6488e+00,
         -4.4076e+00, -1.4818e+00, -1.1371e+00, -3.7062e+00, -2.1213e+00],
        [-1.1637e+00, -3.8776e-01, -7.0991e-01, -3.7050e-01, -3.8767e-01,
         -8.3575e-01, -1.0543e+00, -4.7849e-01, -2.9402e-02, -1.2324e+00],
        [-8.0890e+00,  4.4759e-01,  5.8985e+00,  8.0189e-01,  9.5517e+00,
          2.0382e+00, -3.0454e+00,  1.2842e+00,  1.8420e+00, -2.0685e+01],
        [-9.6121e+00,  4.2774e-01,  8.8690e+00, -1.0471e+01,  5.8744e+00,
          8.2530e+00, -4.9189e+00,  2.4137e+00,  8.7083e+00, -1.4855e+01],
        [-1.1633e+00, -3.8767e-01, -7.0976e-01, -3.7032e-01, -3.8750e-01,
         -8.3573e-01, -1.0541e+00, -4.7844e-01, -2.9403e-02, -1.2321e+00],
        [-3.3022e+00,  2.4695e+00,  7.7086e-02,  4.9005e+00,  3.4609e+00,
         -4.0536e+00, -1.4189e+00, -1.1454e+00, -3.4640e+00, -2.0260e+00],
        [-2.0113e-01,  1.0013e+01, -4.8859e+00,  6.8310e+00,  1.8534e+00,
          3.4384e+00, -3.1067e+00, -4.9847e+00,  1.8256e+00, -5.1503e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.5917, -3.1860, -3.7571, -2.1880, -1.7545, -6.7424, -7.9975, -1.7556,
        -2.1591, -1.7526], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-2.2944e+00, -2.0514e+00, -4.9324e+00, -5.4339e+00, -4.6270e-03,
          7.9311e+00, -3.4188e+00, -4.6358e-03, -3.2780e+00,  8.6686e-01],
        [ 2.0845e+00,  2.0514e+00,  4.9324e+00,  5.4288e+00,  4.6146e-03,
         -7.8544e+00,  3.4369e+00,  4.6236e-03,  3.5806e+00, -1.0103e+00]],
       device='cuda:0'))])
xi:  [-204.86673]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1043.3230512156322
W_T_median: 110.61832899893491
W_T_pctile_5: -204.91945115648855
W_T_CVAR_5_pct: -294.26071709411934
Average q (qsum/M+1):  56.280816847278224
Optimal xi:  [-204.86673]
Observed VAR:  110.61832899893491
Expected(across Rb) median(across samples) p_equity:  0.5937154655655225
obj fun:  tensor(-1686.8964, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.2
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-0.8181,  1.2655],
        [-0.8181,  1.2655],
        [-6.7043,  8.6399],
        [-0.8181,  1.2655],
        [-0.8181,  1.2655],
        [-0.8181,  1.2655],
        [-0.8319,  1.2686],
        [-0.8181,  1.2655],
        [ 4.0510,  9.2020],
        [-0.8181,  1.2655]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-0.9350, -0.9350, 12.2100, -0.9350, -0.9350, -0.9350, -0.9740, -0.9350,
         9.9181, -0.9350], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-9.6010e-03, -9.6010e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6010e-03, -5.0271e-01, -9.6008e-03],
        [-9.6010e-03, -9.6011e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6011e-03, -5.0271e-01, -9.6008e-03],
        [ 5.9245e-01,  5.9245e-01, -8.2989e+00,  5.9239e-01,  5.9240e-01,
          5.9246e-01,  4.7607e-01,  5.9245e-01, -9.4766e+00,  5.9239e-01],
        [-1.6111e-01, -1.6111e-01,  4.4591e+00, -1.6113e-01, -1.6113e-01,
         -1.6111e-01, -2.3334e-01, -1.6111e-01,  4.1745e+00, -1.6113e-01],
        [-9.6010e-03, -9.6010e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6010e-03, -5.0271e-01, -9.6008e-03],
        [-9.6011e-03, -9.6011e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6011e-03, -5.0271e-01, -9.6008e-03],
        [-1.7597e-01, -1.7597e-01,  5.2062e+00, -1.7602e-01, -1.7601e-01,
         -1.7597e-01, -2.4063e-01, -1.7597e-01,  4.7205e+00, -1.7602e-01],
        [-2.0765e-01, -2.0765e-01,  5.9716e+00, -2.0773e-01, -2.0771e-01,
         -2.0764e-01, -2.7815e-01, -2.0765e-01,  5.3062e+00, -2.0773e-01],
        [-9.6010e-03, -9.6011e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6011e-03, -5.0271e-01, -9.6008e-03],
        [-9.6010e-03, -9.6010e-03, -1.3388e-01, -9.6008e-03, -9.6008e-03,
         -9.6011e-03, -9.0134e-03, -9.6011e-03, -5.0271e-01, -9.6008e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.7303, -0.7303,  8.0176, -4.1127, -0.7303, -0.7303, -4.6390, -5.1923,
        -0.7303, -0.7303], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.0248,  -0.0248, -14.4336,   4.4326,  -0.0248,  -0.0248,   5.3747,
           6.5926,  -0.0248,  -0.0248]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 14.9886,   2.4124],
        [ -3.9607,  -6.8647],
        [  2.8421, -15.0439],
        [-15.0104,  -1.3554],
        [-17.8803,  -2.1979],
        [-13.6356,   0.1386],
        [  1.5865,  -2.9758],
        [ -9.1459,  -9.2862],
        [-12.0836,   1.7614],
        [ 12.6914,  10.9896]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.8115, -13.3717, -15.6698,   0.1876,   3.0066,  12.2546,  -2.6500,
         -8.0082,   6.1204,  11.2100], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-9.9379e-01, -4.8549e+00,  6.0099e+00, -1.0764e+01,  1.1539e+00,
          1.3655e+00, -1.6340e+00, -1.3220e+00,  3.1557e+00, -2.2392e+00],
        [ 3.0425e-01, -2.2314e+00, -1.1031e+00, -4.3235e-01, -1.0041e+00,
          4.4433e-01,  1.8019e+00, -1.6744e+00, -6.2733e+00, -1.5991e+00],
        [-1.5442e+00, -9.9958e-03, -6.8393e+00,  4.0706e+00,  9.5080e-01,
          4.8177e+00, -9.3012e+00, -5.4022e+00,  3.5393e+00, -2.0912e+00],
        [-3.6220e+00,  2.5934e+00,  1.8413e-03,  5.1178e+00,  3.6488e+00,
         -4.4076e+00, -1.4818e+00, -1.1371e+00, -3.7062e+00, -2.1213e+00],
        [-1.1637e+00, -3.8776e-01, -7.0991e-01, -3.7050e-01, -3.8767e-01,
         -8.3575e-01, -1.0543e+00, -4.7849e-01, -2.9402e-02, -1.2324e+00],
        [-8.0890e+00,  4.4759e-01,  5.8985e+00,  8.0189e-01,  9.5517e+00,
          2.0382e+00, -3.0454e+00,  1.2842e+00,  1.8420e+00, -2.0685e+01],
        [-9.6121e+00,  4.2774e-01,  8.8690e+00, -1.0471e+01,  5.8744e+00,
          8.2530e+00, -4.9189e+00,  2.4137e+00,  8.7083e+00, -1.4855e+01],
        [-1.1633e+00, -3.8767e-01, -7.0976e-01, -3.7032e-01, -3.8750e-01,
         -8.3573e-01, -1.0541e+00, -4.7844e-01, -2.9403e-02, -1.2321e+00],
        [-3.3022e+00,  2.4695e+00,  7.7086e-02,  4.9005e+00,  3.4609e+00,
         -4.0536e+00, -1.4189e+00, -1.1454e+00, -3.4640e+00, -2.0260e+00],
        [-2.0113e-01,  1.0013e+01, -4.8859e+00,  6.8310e+00,  1.8534e+00,
          3.4384e+00, -3.1067e+00, -4.9847e+00,  1.8256e+00, -5.1503e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([-4.5917, -3.1860, -3.7571, -2.1880, -1.7545, -6.7424, -7.9975, -1.7556,
        -2.1591, -1.7526], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-2.2944e+00, -2.0514e+00, -4.9324e+00, -5.4339e+00, -4.6270e-03,
          7.9311e+00, -3.4188e+00, -4.6358e-03, -3.2780e+00,  8.6686e-01],
        [ 2.0845e+00,  2.0514e+00,  4.9324e+00,  5.4288e+00,  4.6146e-03,
         -7.8544e+00,  3.4369e+00,  4.6236e-03,  3.5806e+00, -1.0103e+00]],
       device='cuda:0'))])
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1506.8368317040647
W_T_median: 1144.7259758050877
W_T_pctile_5: -92.78690539124324
W_T_CVAR_5_pct: -256.52592661793926
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.1035663986195
Current xi:  [88.273605]
objective value function right now is: -1602.1035663986195
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1609.5382805232407
Current xi:  [72.783226]
objective value function right now is: -1609.5382805232407
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1610.5830718999607
Current xi:  [57.79829]
objective value function right now is: -1610.5830718999607
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1615.5098395397754
Current xi:  [43.168262]
objective value function right now is: -1615.5098395397754
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1617.8139889373292
Current xi:  [29.134302]
objective value function right now is: -1617.8139889373292
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1620.82248828891
Current xi:  [15.89414]
objective value function right now is: -1620.82248828891
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1625.6589597458867
Current xi:  [-0.42203096]
objective value function right now is: -1625.6589597458867
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1625.8931750517347
Current xi:  [-0.13241926]
objective value function right now is: -1625.8931750517347
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1626.1868050590433
Current xi:  [-0.03509893]
objective value function right now is: -1626.1868050590433
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06435367]
objective value function right now is: -1624.9088957761023
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01056619]
objective value function right now is: -1623.859802076154
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.18615761]
objective value function right now is: -1622.5565515122978
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00637713]
objective value function right now is: -1626.1843579773524
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [-0.06383147]
objective value function right now is: -1625.804458212775
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.11074454]
objective value function right now is: -1625.942782853467
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.03370957]
objective value function right now is: -1625.865015800011
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00469634]
objective value function right now is: -1626.1141744281238
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.07845007]
objective value function right now is: -1625.819775262321
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.0224984]
objective value function right now is: -1625.3667612347508
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.04250728]
objective value function right now is: -1624.727602835007
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.00449296]
objective value function right now is: -1623.3027736435793
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.01157538]
objective value function right now is: -1623.9731066295542
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.09902149]
objective value function right now is: -1625.6060948681247
48.0% of gradient descent iterations done. Method = Adam
new min fval:  -1626.349074273523
Current xi:  [-0.04610465]
objective value function right now is: -1626.349074273523
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.348845]
objective value function right now is: -1617.3870109605928
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.804627]
objective value function right now is: -1625.001564299491
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [-19.710972]
objective value function right now is: -1624.632795068775
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [-27.338259]
objective value function right now is: -1626.1447542136843
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [-34.101315]
objective value function right now is: -1625.8476658068785
60.0% of gradient descent iterations done. Method = Adam
new min fval:  -1627.6873855644103
Current xi:  [-34.52996]
objective value function right now is: -1627.6873855644103
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [-35.009224]
objective value function right now is: -1625.9161759955111
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.89442]
objective value function right now is: -1627.0207097731668
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.874176]
objective value function right now is: -1626.582046654177
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.89565]
objective value function right now is: -1627.3047898643654
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.926903]
objective value function right now is: -1626.3437656618705
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1628.6474670613338
Current xi:  [-34.91463]
objective value function right now is: -1628.6474670613338
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.945107]
objective value function right now is: -1628.209470757398
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.949673]
objective value function right now is: -1628.644305159065
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1628.6975139709978
Current xi:  [-34.954426]
objective value function right now is: -1628.6975139709978
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.95664]
objective value function right now is: -1628.6170479907478
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.955765]
objective value function right now is: -1628.5062125289314
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.97396]
objective value function right now is: -1628.4046587565742
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.929996]
objective value function right now is: -1628.6476414898877
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1628.7345023214439
Current xi:  [-34.9603]
objective value function right now is: -1628.7345023214439
new min fval from sgd:  -1628.9353465857678
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.948166]
objective value function right now is: -1628.9353465857678
new min fval from sgd:  -1628.9457213508906
new min fval from sgd:  -1628.9734162328114
new min fval from sgd:  -1629.0010093910732
new min fval from sgd:  -1629.003493240249
new min fval from sgd:  -1629.0102860822315
new min fval from sgd:  -1629.0343764206425
new min fval from sgd:  -1629.0385442031525
new min fval from sgd:  -1629.04483622051
new min fval from sgd:  -1629.0474210101077
new min fval from sgd:  -1629.053400975409
new min fval from sgd:  -1629.0677822826003
new min fval from sgd:  -1629.0776287001834
new min fval from sgd:  -1629.0805032682833
new min fval from sgd:  -1629.0971976976
new min fval from sgd:  -1629.099366316079
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.930218]
objective value function right now is: -1623.9040819427642
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.968742]
objective value function right now is: -1628.3978718402711
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.969837]
objective value function right now is: -1628.801348604087
new min fval from sgd:  -1629.102661643248
new min fval from sgd:  -1629.1071537265832
new min fval from sgd:  -1629.1099724240112
new min fval from sgd:  -1629.1140742839655
new min fval from sgd:  -1629.1171310536574
new min fval from sgd:  -1629.1192397268544
new min fval from sgd:  -1629.1197725745296
new min fval from sgd:  -1629.1203346689967
new min fval from sgd:  -1629.1205112862951
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.981857]
objective value function right now is: -1629.116256460523
new min fval from sgd:  -1629.1213028451457
new min fval from sgd:  -1629.1220240778266
new min fval from sgd:  -1629.1222086387907
new min fval from sgd:  -1629.1245585118977
new min fval from sgd:  -1629.126100365431
new min fval from sgd:  -1629.1292125076511
new min fval from sgd:  -1629.129580179019
new min fval from sgd:  -1629.1300783271697
new min fval from sgd:  -1629.1309218844337
new min fval from sgd:  -1629.132553261622
new min fval from sgd:  -1629.1349290955443
new min fval from sgd:  -1629.136472399037
new min fval from sgd:  -1629.1380244567931
new min fval from sgd:  -1629.1385019336951
new min fval from sgd:  -1629.139575246564
new min fval from sgd:  -1629.1413212641867
new min fval from sgd:  -1629.1420261335982
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [-34.975872]
objective value function right now is: -1629.122648593153
min fval:  -1629.1420261335982
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-2.2247,  4.3209],
        [-2.2240,  4.3268],
        [-6.1086,  9.7295],
        [-2.2554,  4.0741],
        [-2.2525,  4.0967],
        [-2.2227,  4.3381],
        [ 6.7954,  2.8002],
        [-2.2250,  4.3185],
        [ 5.3083, 11.2098],
        [-2.2544,  4.0817]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 0.3963,  0.3968, 10.7253,  0.3753,  0.3772,  0.3978, -5.0732,  0.3960,
        11.5013,  0.3759], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [ -0.5656,  -0.5647,  -8.7616,  -0.6079,  -0.6036,  -0.5630,   7.2620,
          -0.5660, -10.7544,  -0.6064],
        [  0.7350,   0.7343,   5.9283,   0.7708,   0.7672,   0.7328,  -7.4856,
           0.7354,   4.0658,   0.7696],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [  2.2900,   2.2910,   6.5259,   2.2522,   2.2556,   2.2928,  -8.2870,
           2.2896,   5.1739,   2.2533],
        [  3.2348,   3.2364,   6.9482,   3.1717,   3.1774,   3.2393,  -8.8705,
           3.2342,   5.9754,   3.1736],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.4110, -1.4110, 10.7223, -6.0868, -1.4110, -1.4110, -6.8565, -7.6155,
        -1.4110, -1.4110], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0706,   0.0706, -20.7368,   6.3760,   0.0706,   0.0706,   8.9201,
          11.4583,   0.0706,   0.0706]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.5218,   1.7324],
        [ -2.1461,  -2.0257],
        [ -6.8512, -17.2638],
        [-14.5313,   1.7647],
        [-13.4452,  -3.1005],
        [-15.7318,   0.2365],
        [  3.6337,  -5.3431],
        [-11.6388, -11.4033],
        [-10.0472,   1.7281],
        [ 14.6455,  11.8789]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.0086,  -6.5859, -17.8332,   2.9281,  -0.8009,  14.0234,  -7.3603,
        -10.6435,   6.5159,  11.3695], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.0960e+00, -2.9719e+00,  9.1244e+00, -5.7432e+00, -3.2062e+00,
          6.8884e+00, -2.3659e+00,  1.3669e+00, -1.6958e-01, -4.1299e+00],
        [-2.9595e-01, -1.6511e+00,  3.7282e-02, -1.3465e-02, -1.7968e+00,
         -2.0560e+00, -4.6125e-01,  2.2892e-01, -5.2864e-01, -1.5267e+00],
        [ 4.0496e-02, -6.9984e-02,  5.6310e-02,  6.3262e+00, -5.6710e+00,
          4.8562e+00, -8.0886e+00, -3.3964e+00,  7.7433e+00, -5.1245e+00],
        [-4.1321e+00,  7.9852e+00, -8.7048e+00, -4.2872e+00,  5.4975e+00,
         -2.1416e+00, -7.9463e-03,  3.2576e+00,  9.6131e-02,  2.8003e-01],
        [-8.4917e-01, -6.3890e+00, -1.7492e-01, -9.8749e-01,  2.9528e+00,
         -4.2911e-01,  2.6289e+00, -1.3506e+00,  9.9220e-01, -8.4796e-01],
        [-6.9732e+00,  3.8222e-01,  8.0288e+00,  5.5360e-01,  1.0899e+01,
         -7.9086e-03, -1.3856e+00,  5.3801e-01,  1.4334e+00, -2.1538e+01],
        [-1.3112e+01, -8.6230e+00,  1.4464e+01, -7.7538e+00, -9.8681e-01,
          8.7489e+00, -1.5331e-01,  1.6045e+00,  6.5969e+00, -1.6779e+01],
        [-1.8426e-01, -6.0389e+00,  6.9451e-01, -2.1815e+00,  2.7693e+00,
         -2.0308e+00,  1.2657e+00,  4.2083e-01,  3.4479e+00, -1.4204e+00],
        [-3.7185e+00,  5.8697e+00, -1.1276e+00, -1.6690e-01,  4.5050e+00,
         -3.7879e+00, -7.8347e-01,  2.7785e-01, -2.4582e+00, -2.2583e+00],
        [-1.5464e+00,  2.1880e-01, -1.9021e+01,  8.7624e+00,  7.3152e+00,
          2.0446e+00, -9.2952e+00, -1.1962e+01,  1.4433e+00, -4.6725e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -7.2575,  -2.8650,  -7.9039,  -0.6463,  -3.3122,  -8.3409, -12.6841,
         -4.2731,  -1.7224,  -1.6095], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-0.6682,  0.8723, -5.8822, -5.2736,  3.3221,  8.6715, -4.6380,  2.5104,
         -3.1895,  1.8983],
        [ 0.4710, -0.8723,  5.8822,  5.2685, -3.3221, -8.5960,  4.6501, -2.5104,
          3.4879, -2.0341]], device='cuda:0'))])
xi:  [-34.98182]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 968.1833139303542
W_T_median: 148.47075596120982
W_T_pctile_5: -34.975711412995814
W_T_CVAR_5_pct: -124.56760756331725
Average q (qsum/M+1):  54.530899540070564
Optimal xi:  [-34.98182]
Observed VAR:  148.47075596120982
Expected(across Rb) median(across samples) p_equity:  0.48351966540018715
obj fun:  tensor(-1629.1420, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 0.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-2.2247,  4.3209],
        [-2.2240,  4.3268],
        [-6.1086,  9.7295],
        [-2.2554,  4.0741],
        [-2.2525,  4.0967],
        [-2.2227,  4.3381],
        [ 6.7954,  2.8002],
        [-2.2250,  4.3185],
        [ 5.3083, 11.2098],
        [-2.2544,  4.0817]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ 0.3963,  0.3968, 10.7253,  0.3753,  0.3772,  0.3978, -5.0732,  0.3960,
        11.5013,  0.3759], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [ -0.5656,  -0.5647,  -8.7616,  -0.6079,  -0.6036,  -0.5630,   7.2620,
          -0.5660, -10.7544,  -0.6064],
        [  0.7350,   0.7343,   5.9283,   0.7708,   0.7672,   0.7328,  -7.4856,
           0.7354,   4.0658,   0.7696],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [  2.2900,   2.2910,   6.5259,   2.2522,   2.2556,   2.2928,  -8.2870,
           2.2896,   5.1739,   2.2533],
        [  3.2348,   3.2364,   6.9482,   3.1717,   3.1774,   3.2393,  -8.8705,
           3.2342,   5.9754,   3.1736],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558],
        [ -0.1684,  -0.1687,  -0.9404,  -0.1554,  -0.1565,  -0.1693,  -0.8081,
          -0.1682,  -1.4002,  -0.1558]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.4110, -1.4110, 10.7223, -6.0868, -1.4110, -1.4110, -6.8565, -7.6155,
        -1.4110, -1.4110], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[  0.0706,   0.0706, -20.7368,   6.3760,   0.0706,   0.0706,   8.9201,
          11.4583,   0.0706,   0.0706]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 17.5218,   1.7324],
        [ -2.1461,  -2.0257],
        [ -6.8512, -17.2638],
        [-14.5313,   1.7647],
        [-13.4452,  -3.1005],
        [-15.7318,   0.2365],
        [  3.6337,  -5.3431],
        [-11.6388, -11.4033],
        [-10.0472,   1.7281],
        [ 14.6455,  11.8789]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.0086,  -6.5859, -17.8332,   2.9281,  -0.8009,  14.0234,  -7.3603,
        -10.6435,   6.5159,  11.3695], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-4.0960e+00, -2.9719e+00,  9.1244e+00, -5.7432e+00, -3.2062e+00,
          6.8884e+00, -2.3659e+00,  1.3669e+00, -1.6958e-01, -4.1299e+00],
        [-2.9595e-01, -1.6511e+00,  3.7282e-02, -1.3465e-02, -1.7968e+00,
         -2.0560e+00, -4.6125e-01,  2.2892e-01, -5.2864e-01, -1.5267e+00],
        [ 4.0496e-02, -6.9984e-02,  5.6310e-02,  6.3262e+00, -5.6710e+00,
          4.8562e+00, -8.0886e+00, -3.3964e+00,  7.7433e+00, -5.1245e+00],
        [-4.1321e+00,  7.9852e+00, -8.7048e+00, -4.2872e+00,  5.4975e+00,
         -2.1416e+00, -7.9463e-03,  3.2576e+00,  9.6131e-02,  2.8003e-01],
        [-8.4917e-01, -6.3890e+00, -1.7492e-01, -9.8749e-01,  2.9528e+00,
         -4.2911e-01,  2.6289e+00, -1.3506e+00,  9.9220e-01, -8.4796e-01],
        [-6.9732e+00,  3.8222e-01,  8.0288e+00,  5.5360e-01,  1.0899e+01,
         -7.9086e-03, -1.3856e+00,  5.3801e-01,  1.4334e+00, -2.1538e+01],
        [-1.3112e+01, -8.6230e+00,  1.4464e+01, -7.7538e+00, -9.8681e-01,
          8.7489e+00, -1.5331e-01,  1.6045e+00,  6.5969e+00, -1.6779e+01],
        [-1.8426e-01, -6.0389e+00,  6.9451e-01, -2.1815e+00,  2.7693e+00,
         -2.0308e+00,  1.2657e+00,  4.2083e-01,  3.4479e+00, -1.4204e+00],
        [-3.7185e+00,  5.8697e+00, -1.1276e+00, -1.6690e-01,  4.5050e+00,
         -3.7879e+00, -7.8347e-01,  2.7785e-01, -2.4582e+00, -2.2583e+00],
        [-1.5464e+00,  2.1880e-01, -1.9021e+01,  8.7624e+00,  7.3152e+00,
          2.0446e+00, -9.2952e+00, -1.1962e+01,  1.4433e+00, -4.6725e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -7.2575,  -2.8650,  -7.9039,  -0.6463,  -3.3122,  -8.3409, -12.6841,
         -4.2731,  -1.7224,  -1.6095], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[-0.6682,  0.8723, -5.8822, -5.2736,  3.3221,  8.6715, -4.6380,  2.5104,
         -3.1895,  1.8983],
        [ 0.4710, -0.8723,  5.8822,  5.2685, -3.3221, -8.5960,  4.6501, -2.5104,
          3.4879, -2.0341]], device='cuda:0'))])
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1506.8368317040647
W_T_median: 1144.7259758050877
W_T_pctile_5: -92.78690539124324
W_T_CVAR_5_pct: -256.52592661793926
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1594.17441309948
Current xi:  [99.11477]
objective value function right now is: -1594.17441309948
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [98.184265]
objective value function right now is: -1593.7606896499265
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1597.577475806904
Current xi:  [97.515076]
objective value function right now is: -1597.577475806904
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1598.8957256420917
Current xi:  [97.13051]
objective value function right now is: -1598.8957256420917
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [96.41828]
objective value function right now is: -1597.3022732344446
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [96.36307]
objective value function right now is: -1596.6311107300032
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [96.07653]
objective value function right now is: -1597.9404688373677
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [95.10211]
objective value function right now is: -1596.7655397806611
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [94.338264]
objective value function right now is: -1595.2860686531142
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1599.3927211087225
Current xi:  [94.413925]
objective value function right now is: -1599.3927211087225
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [94.64288]
objective value function right now is: -1598.3195454851011
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [94.55007]
objective value function right now is: -1598.9606658708701
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [94.76769]
objective value function right now is: -1598.624018347524
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1599.7534119375587
Current xi:  [94.29672]
objective value function right now is: -1599.7534119375587
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [91.78404]
objective value function right now is: -1289.2494175815164
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [47.31474]
objective value function right now is: -1338.9902144955327
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [44.24855]
objective value function right now is: -1566.267120180239
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [45.406418]
objective value function right now is: -1567.577286307966
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [47.58869]
objective value function right now is: -1581.715440176659
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [51.029415]
objective value function right now is: -1580.712207576849
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.139614]
objective value function right now is: -1582.1590187204756
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [59.015213]
objective value function right now is: -1581.6678841436762
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [62.44468]
objective value function right now is: -1581.1497911605336
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [65.261696]
objective value function right now is: -1583.7846951547458
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [68.08331]
objective value function right now is: -1584.551792301694
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [70.42447]
objective value function right now is: -1583.1247055580284
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [72.60364]
objective value function right now is: -1585.5247491308066
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [74.85486]
objective value function right now is: -1584.0604484358541
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [76.65297]
objective value function right now is: -1584.5946219247683
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [78.07257]
objective value function right now is: -1583.1999228034836
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [79.20441]
objective value function right now is: -1581.8216407903935
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [80.04847]
objective value function right now is: -1582.6118227308993
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [80.80765]
objective value function right now is: -1584.1986324018012
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [81.34058]
objective value function right now is: -1585.3926347131353
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [81.92939]
objective value function right now is: -1583.3522544963648
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [82.08882]
objective value function right now is: -1587.3276094182602
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [82.42458]
objective value function right now is: -1586.9791126119342
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [82.645195]
objective value function right now is: -1587.0149524479336
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [82.919754]
objective value function right now is: -1587.520568365726
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [83.17619]
objective value function right now is: -1587.5873826291217
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [83.375854]
objective value function right now is: -1587.4630115387895
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [83.57337]
objective value function right now is: -1587.148229983577
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [83.84574]
objective value function right now is: -1585.8828595092905
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [83.98956]
objective value function right now is: -1587.3333321849027
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [84.25147]
objective value function right now is: -1587.756872678423
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [84.46613]
objective value function right now is: -1587.9901820356563
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [84.73536]
objective value function right now is: -1588.185714684449
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [84.8985]
objective value function right now is: -1587.5962736453544
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [85.07781]
objective value function right now is: -1588.2848367734523
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [85.13688]
objective value function right now is: -1588.330247482208
min fval:  -1596.9257931460004
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.9022,  2.1997],
        [-1.9027,  2.2008],
        [-2.5359, 11.3012],
        [-1.8886,  2.1637],
        [-1.8893,  2.1661],
        [-1.9038,  2.2030],
        [ 9.4669, -1.8947],
        [-1.9020,  2.1992],
        [35.0777, 11.0529],
        [-1.8888,  2.1644]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-3.9099, -3.9084, 10.2351, -3.9580, -3.9548, -3.9054, -7.6292, -3.9106,
        11.8060, -3.9570], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [ 0.3042,  0.3058, -8.5943,  0.2453,  0.2499,  0.3090,  9.7746,  0.3035,
         -9.4847,  0.2468],
        [-0.0459, -0.0461,  4.1241, -0.0414, -0.0417, -0.0464, -4.8504, -0.0459,
          3.0107, -0.0415],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [-0.0452, -0.0448,  7.3092, -0.0621, -0.0604, -0.0441, -8.0983, -0.0454,
          5.0301, -0.0615],
        [ 0.1863,  0.1879,  8.1710,  0.1243,  0.1296,  0.1910, -8.9808,  0.1857,
          5.5552,  0.1261],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.8905, -1.8905, 12.2159, -7.6068, -1.8905, -1.8905, -7.5374, -8.3188,
        -1.8905, -1.8905], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.0837e-02, -2.0837e-02, -2.3517e+01,  3.7147e+00, -2.0837e-02,
         -2.0837e-02,  8.4533e+00,  1.1297e+01, -2.0837e-02, -2.0837e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 18.1543,   1.8292],
        [ -2.4963,   0.3935],
        [ -8.9401, -18.6446],
        [-17.8212,   2.2503],
        [-12.9877,  -4.5806],
        [-17.9568,  -0.0681],
        [  3.7617,  -6.9727],
        [-14.2883,  -9.6505],
        [-10.6075,   2.0204],
        [ 14.0266,  12.6445]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.7796,  -5.1752, -17.3002,   3.2370,  -3.7983,  14.2039,  -7.1168,
        -12.1026,   6.0532,  11.8976], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8406e+00, -5.0604e-05,  9.2377e+00, -1.2129e+01,  2.6529e-01,
          7.5686e+00, -1.3486e+00,  2.8181e-02,  5.0669e-01, -4.5431e+00],
        [-5.6833e-01, -3.1932e-01,  2.1166e+00, -2.7407e-01,  1.9631e+00,
         -6.3949e+00, -4.3871e-01,  4.6075e-01, -2.7837e+00, -9.8641e-01],
        [-3.0841e-01,  1.6513e-01, -8.2044e+00,  7.2278e+00, -5.8960e+00,
          6.5574e+00, -1.7555e+01, -2.4367e+00,  8.9103e+00, -6.3373e+00],
        [-3.3190e+00, -2.4528e-03, -3.5481e-01, -1.5983e-01, -3.2941e-02,
         -1.8793e+00, -1.1023e+00,  1.3698e-01, -5.6799e-01, -2.1772e+00],
        [-7.7940e-02, -1.9193e-01, -1.0964e+00,  9.2041e-02,  3.0211e+00,
         -3.6099e-01,  9.7791e-01,  4.3726e-01,  1.6731e-01,  6.6685e-03],
        [-1.0566e+01, -4.3523e-02,  7.8878e+00, -2.0272e+00,  1.1324e+01,
         -6.2711e-01, -4.3568e-01,  2.7229e+00,  4.6671e+00, -2.5642e+01],
        [-1.4986e+01, -9.2645e-03,  1.4251e+01, -5.3074e+00,  2.7459e+00,
          8.9358e+00,  1.5125e+00,  1.7465e+00,  5.1019e+00, -1.6309e+01],
        [-1.7331e+00, -5.9463e-04,  9.3456e-02,  7.4209e-02,  2.0586e-01,
         -2.6298e+00,  9.9453e-01, -2.5439e-01, -1.6643e-01, -1.3235e+00],
        [-2.7923e+00,  2.4169e-03, -2.6580e-01, -1.7699e-01, -1.0988e-01,
         -1.4099e+00, -9.9616e-01,  2.9782e-01, -5.3087e-01, -1.8743e+00],
        [-2.1056e+00,  2.7644e-01, -6.3773e+00,  4.0284e+00,  1.2414e+00,
          4.4436e+00, -1.4050e+01,  1.1258e+00,  1.5008e+00, -1.4302e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -7.8213,  -4.0747,  -9.6565,  -2.8734,  -2.6047,  -8.8432, -13.9188,
         -5.4919,  -3.6199,  -1.2448], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.6687,   1.1627,  -4.6536,   0.3585,   2.6883,  12.7566,  -5.4592,
           1.4591,   0.4631,   0.4319],
        [  0.4730,  -1.1627,   4.6537,  -0.3589,  -2.6882, -12.7178,   5.4710,
          -1.4592,  -0.4039,  -0.5650]], device='cuda:0'))])
xi:  [84.25147]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1068.3507507982845
W_T_median: 276.3148883447391
W_T_pctile_5: 96.38588847949855
W_T_CVAR_5_pct: -8.714532735544056
Average q (qsum/M+1):  51.861757339969756
Optimal xi:  [84.25147]
Observed VAR:  276.3148883447391
Expected(across Rb) median(across samples) p_equity:  0.3790254493554433
obj fun:  tensor(-1596.9258, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.9022,  2.1997],
        [-1.9027,  2.2008],
        [-2.5359, 11.3012],
        [-1.8886,  2.1637],
        [-1.8893,  2.1661],
        [-1.9038,  2.2030],
        [ 9.4669, -1.8947],
        [-1.9020,  2.1992],
        [35.0777, 11.0529],
        [-1.8888,  2.1644]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([-3.9099, -3.9084, 10.2351, -3.9580, -3.9548, -3.9054, -7.6292, -3.9106,
        11.8060, -3.9570], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [ 0.3042,  0.3058, -8.5943,  0.2453,  0.2499,  0.3090,  9.7746,  0.3035,
         -9.4847,  0.2468],
        [-0.0459, -0.0461,  4.1241, -0.0414, -0.0417, -0.0464, -4.8504, -0.0459,
          3.0107, -0.0415],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [-0.0452, -0.0448,  7.3092, -0.0621, -0.0604, -0.0441, -8.0983, -0.0454,
          5.0301, -0.0615],
        [ 0.1863,  0.1879,  8.1710,  0.1243,  0.1296,  0.1910, -8.9808,  0.1857,
          5.5552,  0.1261],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317],
        [-0.0323, -0.0323, -0.8341, -0.0317, -0.0318, -0.0324,  0.1713, -0.0323,
         -1.6498, -0.0317]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.8905, -1.8905, 12.2159, -7.6068, -1.8905, -1.8905, -7.5374, -8.3188,
        -1.8905, -1.8905], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-2.0837e-02, -2.0837e-02, -2.3517e+01,  3.7147e+00, -2.0837e-02,
         -2.0837e-02,  8.4533e+00,  1.1297e+01, -2.0837e-02, -2.0837e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ 18.1543,   1.8292],
        [ -2.4963,   0.3935],
        [ -8.9401, -18.6446],
        [-17.8212,   2.2503],
        [-12.9877,  -4.5806],
        [-17.9568,  -0.0681],
        [  3.7617,  -6.9727],
        [-14.2883,  -9.6505],
        [-10.6075,   2.0204],
        [ 14.0266,  12.6445]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -8.7796,  -5.1752, -17.3002,   3.2370,  -3.7983,  14.2039,  -7.1168,
        -12.1026,   6.0532,  11.8976], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-3.8406e+00, -5.0604e-05,  9.2377e+00, -1.2129e+01,  2.6529e-01,
          7.5686e+00, -1.3486e+00,  2.8181e-02,  5.0669e-01, -4.5431e+00],
        [-5.6833e-01, -3.1932e-01,  2.1166e+00, -2.7407e-01,  1.9631e+00,
         -6.3949e+00, -4.3871e-01,  4.6075e-01, -2.7837e+00, -9.8641e-01],
        [-3.0841e-01,  1.6513e-01, -8.2044e+00,  7.2278e+00, -5.8960e+00,
          6.5574e+00, -1.7555e+01, -2.4367e+00,  8.9103e+00, -6.3373e+00],
        [-3.3190e+00, -2.4528e-03, -3.5481e-01, -1.5983e-01, -3.2941e-02,
         -1.8793e+00, -1.1023e+00,  1.3698e-01, -5.6799e-01, -2.1772e+00],
        [-7.7940e-02, -1.9193e-01, -1.0964e+00,  9.2041e-02,  3.0211e+00,
         -3.6099e-01,  9.7791e-01,  4.3726e-01,  1.6731e-01,  6.6685e-03],
        [-1.0566e+01, -4.3523e-02,  7.8878e+00, -2.0272e+00,  1.1324e+01,
         -6.2711e-01, -4.3568e-01,  2.7229e+00,  4.6671e+00, -2.5642e+01],
        [-1.4986e+01, -9.2645e-03,  1.4251e+01, -5.3074e+00,  2.7459e+00,
          8.9358e+00,  1.5125e+00,  1.7465e+00,  5.1019e+00, -1.6309e+01],
        [-1.7331e+00, -5.9463e-04,  9.3456e-02,  7.4209e-02,  2.0586e-01,
         -2.6298e+00,  9.9453e-01, -2.5439e-01, -1.6643e-01, -1.3235e+00],
        [-2.7923e+00,  2.4169e-03, -2.6580e-01, -1.7699e-01, -1.0988e-01,
         -1.4099e+00, -9.9616e-01,  2.9782e-01, -5.3087e-01, -1.8743e+00],
        [-2.1056e+00,  2.7644e-01, -6.3773e+00,  4.0284e+00,  1.2414e+00,
          4.4436e+00, -1.4050e+01,  1.1258e+00,  1.5008e+00, -1.4302e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -7.8213,  -4.0747,  -9.6565,  -2.8734,  -2.6047,  -8.8432, -13.9188,
         -5.4919,  -3.6199,  -1.2448], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.6687,   1.1627,  -4.6536,   0.3585,   2.6883,  12.7566,  -5.4592,
           1.4591,   0.4631,   0.4319],
        [  0.4730,  -1.1627,   4.6537,  -0.3589,  -2.6882, -12.7178,   5.4710,
          -1.4592,  -0.4039,  -0.5650]], device='cuda:0'))])
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1506.8368317040647
W_T_median: 1144.7259758050877
W_T_pctile_5: -92.78690539124324
W_T_CVAR_5_pct: -256.52592661793926
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1596.3070696694479
Current xi:  [107.69355]
objective value function right now is: -1596.3070696694479
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1597.9590101841409
Current xi:  [114.82391]
objective value function right now is: -1597.9590101841409
6.0% of gradient descent iterations done. Method = Adam
Current xi:  [121.013214]
objective value function right now is: -1597.401040143656
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.3517323713652
Current xi:  [126.37829]
objective value function right now is: -1600.3517323713652
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [130.5425]
objective value function right now is: -1597.9664277310064
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [134.17238]
objective value function right now is: -1599.8664506109571
14.000000000000002% of gradient descent iterations done. Method = Adam
Current xi:  [136.93028]
objective value function right now is: -1599.2132400084222
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.6338650911746
Current xi:  [139.2535]
objective value function right now is: -1600.6338650911746
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1600.7595100929284
Current xi:  [141.18791]
objective value function right now is: -1600.7595100929284
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [143.13303]
objective value function right now is: -1598.127004297851
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -1601.812174697921
Current xi:  [144.52948]
objective value function right now is: -1601.812174697921
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [145.1525]
objective value function right now is: -1595.8930995072699
26.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.6288962665908
Current xi:  [146.15662]
objective value function right now is: -1602.6288962665908
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [147.00912]
objective value function right now is: -1602.074242759815
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.21135]
objective value function right now is: -1598.83271612633
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.58534]
objective value function right now is: -1596.965224374684
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.18445]
objective value function right now is: -1602.0870019568758
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [149.6935]
objective value function right now is: -1602.1985800999544
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.96117]
objective value function right now is: -1588.7061355608369
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.60907]
objective value function right now is: -1591.9141493061218
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.89122]
objective value function right now is: -1599.482000199482
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.51619]
objective value function right now is: -1595.6782120431103
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.28915]
objective value function right now is: -1598.5514314791315
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.04662]
objective value function right now is: -1599.114819884448
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.4916]
objective value function right now is: -1601.6642508178832
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.51392]
objective value function right now is: -1599.2212001934117
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.58133]
objective value function right now is: -1600.402508339142
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [147.65611]
objective value function right now is: -1598.4779662531955
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [147.30368]
objective value function right now is: -1600.039888769155
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [146.97197]
objective value function right now is: -1577.8316389484946
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [145.49165]
objective value function right now is: -1593.8959682207123
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [138.33153]
objective value function right now is: -1581.7630343524813
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [139.76137]
objective value function right now is: -1593.337222097606
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [141.7473]
objective value function right now is: -1596.0715943499304
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [143.10716]
objective value function right now is: -1592.8715056507185
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [143.48344]
objective value function right now is: -1600.9506580359302
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [143.81403]
objective value function right now is: -1601.6705479500556
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [144.10464]
objective value function right now is: -1602.290133752227
78.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.8262184958526
Current xi:  [144.42903]
objective value function right now is: -1602.8262184958526
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [144.81229]
objective value function right now is: -1602.7702588152442
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [145.33899]
objective value function right now is: -1602.2676894662222
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [146.34636]
objective value function right now is: -1602.6352444533277
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.03162]
objective value function right now is: -1602.6494618106622
88.0% of gradient descent iterations done. Method = Adam
new min fval:  -1602.9470722299284
Current xi:  [147.5131]
objective value function right now is: -1602.9470722299284
new min fval from sgd:  -1602.996658486973
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [147.87135]
objective value function right now is: -1602.996658486973
new min fval from sgd:  -1603.0180633420362
new min fval from sgd:  -1603.028236560097
new min fval from sgd:  -1603.054611887252
new min fval from sgd:  -1603.121801560266
new min fval from sgd:  -1603.1615004953715
new min fval from sgd:  -1603.1735452433193
new min fval from sgd:  -1603.2188178727238
new min fval from sgd:  -1603.2339239143184
new min fval from sgd:  -1603.2677613899448
new min fval from sgd:  -1603.3573232933852
new min fval from sgd:  -1603.3875550242947
new min fval from sgd:  -1603.4240376470468
new min fval from sgd:  -1603.4477653347162
new min fval from sgd:  -1603.475942706552
new min fval from sgd:  -1603.5056345103494
new min fval from sgd:  -1603.5409165578708
new min fval from sgd:  -1603.5487782756989
new min fval from sgd:  -1603.5503242439706
new min fval from sgd:  -1603.6087414564047
new min fval from sgd:  -1603.6129716070634
new min fval from sgd:  -1603.6329327777364
new min fval from sgd:  -1603.669546401524
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.1483]
objective value function right now is: -1603.270836992107
new min fval from sgd:  -1603.673641346872
new min fval from sgd:  -1603.7250145177338
new min fval from sgd:  -1603.7294549027106
new min fval from sgd:  -1603.7374795008138
new min fval from sgd:  -1603.7524751296564
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.51791]
objective value function right now is: -1602.4848274974152
new min fval from sgd:  -1603.759841119049
new min fval from sgd:  -1603.7749390821002
new min fval from sgd:  -1603.798574205453
new min fval from sgd:  -1603.8001208020896
new min fval from sgd:  -1603.8218416548889
new min fval from sgd:  -1603.857564004442
new min fval from sgd:  -1603.8850483327992
new min fval from sgd:  -1603.9121968105133
new min fval from sgd:  -1603.92015927675
new min fval from sgd:  -1603.9358402085768
new min fval from sgd:  -1603.974945866617
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.55748]
objective value function right now is: -1603.4729522379855
new min fval from sgd:  -1603.9816423475415
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.59146]
objective value function right now is: -1603.7133321798315
new min fval from sgd:  -1603.9900705633224
new min fval from sgd:  -1603.9975611126836
new min fval from sgd:  -1604.0034107521294
new min fval from sgd:  -1604.0111312327936
new min fval from sgd:  -1604.0149379545114
new min fval from sgd:  -1604.0225991344503
new min fval from sgd:  -1604.0243955903973
new min fval from sgd:  -1604.0255981179835
new min fval from sgd:  -1604.0264993251158
new min fval from sgd:  -1604.026733940889
new min fval from sgd:  -1604.0307667903678
new min fval from sgd:  -1604.031922941492
new min fval from sgd:  -1604.0348069088454
new min fval from sgd:  -1604.040222532017
new min fval from sgd:  -1604.0412779686067
new min fval from sgd:  -1604.0442775989968
new min fval from sgd:  -1604.0548075349177
new min fval from sgd:  -1604.0570116808321
new min fval from sgd:  -1604.0634705180694
new min fval from sgd:  -1604.0702150556788
new min fval from sgd:  -1604.074583032643
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [148.69667]
objective value function right now is: -1603.8976920325304
min fval:  -1604.074583032643
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.3135,  0.7434],
        [-1.3135,  0.7434],
        [-3.6036, 12.0503],
        [-1.3135,  0.7434],
        [-1.3135,  0.7434],
        [ 2.1665,  5.3625],
        [14.8901, -3.0004],
        [-1.3135,  0.7434],
        [41.0310, 11.6400],
        [-1.3135,  0.7434]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.1356,  -3.1357,   9.6844,  -3.1356,  -3.1356,  -7.8763, -10.4544,
         -3.1356,  11.9602,  -3.1356], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-1.1860e-02, -1.1826e-02, -1.2484e+01, -1.2008e-02, -1.2000e-02,
          5.8568e+00,  1.2112e+01, -1.1867e-02, -9.5974e+00, -1.2005e-02],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-1.5016e-01, -1.5015e-01,  1.1306e+01, -1.5020e-01, -1.5020e-01,
         -4.5917e+00, -1.0061e+01, -1.5016e-01,  4.8845e+00, -1.5020e-01],
        [-7.7669e-02, -7.7614e-02,  1.2175e+01, -7.7911e-02, -7.7897e-02,
         -5.6226e+00, -1.1451e+01, -7.7680e-02,  5.5440e+00, -7.7906e-02],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9735, -0.9735, 11.9834, -0.9735, -0.9735, -0.9735, -6.9804, -7.8026,
        -0.9735, -0.9735], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.4353e-03, -4.4353e-03, -2.2854e+01, -4.4353e-03, -4.4353e-03,
         -4.4353e-03,  7.5316e+00,  1.0955e+01, -4.4353e-03, -4.4353e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.8755,  -0.1131],
        [  2.2070,  -2.9190],
        [-12.7398, -19.1529],
        [-20.8026,   2.6402],
        [-14.8786,  -5.8655],
        [-20.8819,  -0.4727],
        [ -1.6507,  -4.2140],
        [ -6.4622,  -2.9494],
        [-16.0097,  -0.3170],
        [ 19.5946,  11.0485]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.4815,  -4.6727, -16.7637,   4.1533,  -2.0845,  15.4023, -10.0394,
         -8.8638,   7.3508,  10.3669], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.9834e-01,  5.4517e+00,  7.6134e+00, -7.4001e+00,  3.2587e+00,
          6.7567e+00, -8.7228e-01,  1.6534e-01,  4.2096e+00, -9.0514e+00],
        [ 2.5227e-02, -5.3127e+00,  1.6561e+00, -7.3609e-01, -1.7339e-01,
         -7.0294e+00, -3.8729e-01, -6.1912e-03, -2.4455e-01,  8.4616e-01],
        [ 2.9578e-04, -8.0853e-01, -9.2585e+00,  1.6414e+01, -1.0574e+01,
          6.6998e+00, -6.4838e-02, -2.8747e-02,  3.6350e+00, -5.0883e+00],
        [-6.5285e-03, -1.4741e+00, -7.4034e-01, -3.3373e-02, -6.6340e-02,
         -1.7186e+00, -2.0529e-02,  1.2917e-05, -1.4412e-01, -3.2719e+00],
        [-1.0602e-01, -1.0111e+00, -6.9635e+00,  8.3494e+00,  7.0579e+00,
         -1.2499e+00, -7.1254e-02,  1.3946e-01, -1.3285e+00, -2.7928e+00],
        [-4.8657e-02,  1.3083e-01,  4.2694e+00, -8.8046e+00,  7.0404e+00,
         -3.9656e-02, -5.1103e-01,  3.4600e+00,  1.6739e+01, -2.5370e+01],
        [-6.9055e-02,  7.4749e+00,  1.4589e+01, -7.7810e+00,  6.7638e+00,
          1.0506e+01, -9.9878e-01,  2.7763e-01,  4.9551e+00, -1.7466e+01],
        [ 4.0192e-02,  3.9246e+00,  5.9270e-01, -3.2575e-01, -4.0014e+00,
         -9.5712e+00, -1.5409e+00,  5.2173e-02, -5.6609e-01, -3.2432e+00],
        [ 2.1377e-02,  4.8526e+00,  7.1024e-01, -3.6780e-01, -5.5796e+00,
         -8.1754e+00, -2.9666e+00, -2.0890e-02,  4.4906e-01, -2.1805e+00],
        [ 1.8129e+00,  6.0915e-01, -9.5672e+00,  8.2498e+00,  2.8312e+00,
          5.4126e-02, -7.4598e+00, -1.0013e-01,  6.1623e-01,  4.9794e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -9.3533,  -4.7132,  -9.6718,  -4.4008,  -4.4709, -12.2256, -16.4986,
         -4.0382,  -4.4854,  -0.2981], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.6295,  -0.7010,  -3.9059,  -0.3621,   0.8007,  12.5427,  -5.9097,
           2.2696,   1.9996,   0.4451],
        [  0.4385,   0.7010,   3.9059,   0.3621,  -0.8007, -12.5331,   5.9200,
          -2.2696,  -1.9994,  -0.5734]], device='cuda:0'))])
xi:  [148.70326]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 1036.797872271722
W_T_median: 360.3442239415555
W_T_pctile_5: 148.6786823679864
W_T_CVAR_5_pct: 20.839799088106492
Average q (qsum/M+1):  50.70255402595766
Optimal xi:  [148.70326]
Observed VAR:  360.3442239415555
Expected(across Rb) median(across samples) p_equity:  0.3640940676132838
obj fun:  tensor(-1604.0746, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 1.5
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.3135,  0.7434],
        [-1.3135,  0.7434],
        [-3.6036, 12.0503],
        [-1.3135,  0.7434],
        [-1.3135,  0.7434],
        [ 2.1665,  5.3625],
        [14.8901, -3.0004],
        [-1.3135,  0.7434],
        [41.0310, 11.6400],
        [-1.3135,  0.7434]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.1356,  -3.1357,   9.6844,  -3.1356,  -3.1356,  -7.8763, -10.4544,
         -3.1356,  11.9602,  -3.1356], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-1.1860e-02, -1.1826e-02, -1.2484e+01, -1.2008e-02, -1.2000e-02,
          5.8568e+00,  1.2112e+01, -1.1867e-02, -9.5974e+00, -1.2005e-02],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-1.5016e-01, -1.5015e-01,  1.1306e+01, -1.5020e-01, -1.5020e-01,
         -4.5917e+00, -1.0061e+01, -1.5016e-01,  4.8845e+00, -1.5020e-01],
        [-7.7669e-02, -7.7614e-02,  1.2175e+01, -7.7911e-02, -7.7897e-02,
         -5.6226e+00, -1.1451e+01, -7.7680e-02,  5.5440e+00, -7.7906e-02],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03],
        [-6.2538e-03, -6.2537e-03, -3.7068e-01, -6.2543e-03, -6.2542e-03,
          2.8021e-03, -1.4149e-01, -6.2538e-03, -8.7595e-01, -6.2543e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-0.9735, -0.9735, 11.9834, -0.9735, -0.9735, -0.9735, -6.9804, -7.8026,
        -0.9735, -0.9735], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-4.4353e-03, -4.4353e-03, -2.2854e+01, -4.4353e-03, -4.4353e-03,
         -4.4353e-03,  7.5316e+00,  1.0955e+01, -4.4353e-03, -4.4353e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[  2.8755,  -0.1131],
        [  2.2070,  -2.9190],
        [-12.7398, -19.1529],
        [-20.8026,   2.6402],
        [-14.8786,  -5.8655],
        [-20.8819,  -0.4727],
        [ -1.6507,  -4.2140],
        [ -6.4622,  -2.9494],
        [-16.0097,  -0.3170],
        [ 19.5946,  11.0485]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-10.4815,  -4.6727, -16.7637,   4.1533,  -2.0845,  15.4023, -10.0394,
         -8.8638,   7.3508,  10.3669], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.9834e-01,  5.4517e+00,  7.6134e+00, -7.4001e+00,  3.2587e+00,
          6.7567e+00, -8.7228e-01,  1.6534e-01,  4.2096e+00, -9.0514e+00],
        [ 2.5227e-02, -5.3127e+00,  1.6561e+00, -7.3609e-01, -1.7339e-01,
         -7.0294e+00, -3.8729e-01, -6.1912e-03, -2.4455e-01,  8.4616e-01],
        [ 2.9578e-04, -8.0853e-01, -9.2585e+00,  1.6414e+01, -1.0574e+01,
          6.6998e+00, -6.4838e-02, -2.8747e-02,  3.6350e+00, -5.0883e+00],
        [-6.5285e-03, -1.4741e+00, -7.4034e-01, -3.3373e-02, -6.6340e-02,
         -1.7186e+00, -2.0529e-02,  1.2917e-05, -1.4412e-01, -3.2719e+00],
        [-1.0602e-01, -1.0111e+00, -6.9635e+00,  8.3494e+00,  7.0579e+00,
         -1.2499e+00, -7.1254e-02,  1.3946e-01, -1.3285e+00, -2.7928e+00],
        [-4.8657e-02,  1.3083e-01,  4.2694e+00, -8.8046e+00,  7.0404e+00,
         -3.9656e-02, -5.1103e-01,  3.4600e+00,  1.6739e+01, -2.5370e+01],
        [-6.9055e-02,  7.4749e+00,  1.4589e+01, -7.7810e+00,  6.7638e+00,
          1.0506e+01, -9.9878e-01,  2.7763e-01,  4.9551e+00, -1.7466e+01],
        [ 4.0192e-02,  3.9246e+00,  5.9270e-01, -3.2575e-01, -4.0014e+00,
         -9.5712e+00, -1.5409e+00,  5.2173e-02, -5.6609e-01, -3.2432e+00],
        [ 2.1377e-02,  4.8526e+00,  7.1024e-01, -3.6780e-01, -5.5796e+00,
         -8.1754e+00, -2.9666e+00, -2.0890e-02,  4.4906e-01, -2.1805e+00],
        [ 1.8129e+00,  6.0915e-01, -9.5672e+00,  8.2498e+00,  2.8312e+00,
          5.4126e-02, -7.4598e+00, -1.0013e-01,  6.1623e-01,  4.9794e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -9.3533,  -4.7132,  -9.6718,  -4.4008,  -4.4709, -12.2256, -16.4986,
         -4.0382,  -4.4854,  -0.2981], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.6295,  -0.7010,  -3.9059,  -0.3621,   0.8007,  12.5427,  -5.9097,
           2.2696,   1.9996,   0.4451],
        [  0.4385,   0.7010,   3.9059,   0.3621,  -0.8007, -12.5331,   5.9200,
          -2.2696,  -1.9994,  -0.5734]], device='cuda:0'))])
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1506.8368317040647
W_T_median: 1144.7259758050877
W_T_pctile_5: -92.78690539124324
W_T_CVAR_5_pct: -256.52592661793926
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1614.6918093100098
Current xi:  [116.36884]
objective value function right now is: -1614.6918093100098
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [131.16339]
objective value function right now is: -1613.7510160189127
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1631.2104212046281
Current xi:  [146.07661]
objective value function right now is: -1631.2104212046281
8.0% of gradient descent iterations done. Method = Adam
new min fval:  -1635.771330341372
Current xi:  [158.8688]
objective value function right now is: -1635.771330341372
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1635.9844373354633
Current xi:  [169.00327]
objective value function right now is: -1635.9844373354633
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -1649.962306197527
Current xi:  [177.92531]
objective value function right now is: -1649.962306197527
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1650.6503378533182
Current xi:  [184.7281]
objective value function right now is: -1650.6503378533182
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1653.288069766461
Current xi:  [190.21529]
objective value function right now is: -1653.288069766461
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -1655.241747437871
Current xi:  [193.09898]
objective value function right now is: -1655.241747437871
20.0% of gradient descent iterations done. Method = Adam
new min fval:  -1655.8609335437864
Current xi:  [196.23521]
objective value function right now is: -1655.8609335437864
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.12148]
objective value function right now is: -1647.0341602959452
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.41238]
objective value function right now is: -1654.598124780376
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [192.66777]
objective value function right now is: -1645.059860738658
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [193.98639]
objective value function right now is: -1651.8617489789292
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.20486]
objective value function right now is: -1651.2944975006512
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.91891]
objective value function right now is: -1655.7633174774571
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.06223]
objective value function right now is: -1655.2957754474241
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.61789]
objective value function right now is: -1650.612852673688
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.68567]
objective value function right now is: -1646.6399851769918
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [194.28365]
objective value function right now is: -1605.4144925980006
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [192.74408]
objective value function right now is: -1641.6418184007207
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [192.95837]
objective value function right now is: -1643.5527186112868
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.01833]
objective value function right now is: -1645.0262838862484
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.3549]
objective value function right now is: -1647.0120031470428
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.49617]
objective value function right now is: -1651.552101811486
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.45447]
objective value function right now is: -1647.2682817345608
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.21352]
objective value function right now is: -1653.950875061201
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [194.89514]
objective value function right now is: -1653.1209778935684
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [197.35635]
objective value function right now is: -1652.1363568045977
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [197.80658]
objective value function right now is: -1645.332673153365
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.64847]
objective value function right now is: -1651.5961983931543
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.97823]
objective value function right now is: -1644.6592371609686
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.29643]
objective value function right now is: -1646.7718086472307
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.93484]
objective value function right now is: -1643.9286533925078
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.69997]
objective value function right now is: -1647.9437018887784
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -1657.2020487203183
Current xi:  [197.1835]
objective value function right now is: -1657.2020487203183
74.0% of gradient descent iterations done. Method = Adam
new min fval:  -1659.138624607153
Current xi:  [197.81006]
objective value function right now is: -1659.138624607153
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.25113]
objective value function right now is: -1659.0100935833402
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.11055]
objective value function right now is: -1657.6138956706864
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.15004]
objective value function right now is: -1659.069427723364
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.41109]
objective value function right now is: -1659.0417761498466
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.45728]
objective value function right now is: -1658.7388295728858
86.0% of gradient descent iterations done. Method = Adam
new min fval:  -1659.4878719356907
Current xi:  [198.8648]
objective value function right now is: -1659.4878719356907
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.29494]
objective value function right now is: -1659.303624152829
new min fval from sgd:  -1659.7774156584264
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.63411]
objective value function right now is: -1659.7774156584264
new min fval from sgd:  -1659.7940593107605
new min fval from sgd:  -1659.814110030812
new min fval from sgd:  -1659.8563553993492
new min fval from sgd:  -1660.0455933704052
new min fval from sgd:  -1660.1541391743845
new min fval from sgd:  -1660.2044866542456
new min fval from sgd:  -1660.2343542025587
new min fval from sgd:  -1660.2459569214802
new min fval from sgd:  -1660.2850610006851
new min fval from sgd:  -1660.3254535119524
new min fval from sgd:  -1660.3392134648552
new min fval from sgd:  -1660.4762108331197
new min fval from sgd:  -1660.5577203672533
new min fval from sgd:  -1660.5752557171872
new min fval from sgd:  -1660.6980787238454
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.4081]
objective value function right now is: -1660.457906775891
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.77097]
objective value function right now is: -1658.885591100448
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.80475]
objective value function right now is: -1660.2967469416028
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.86108]
objective value function right now is: -1659.919372380736
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.84248]
objective value function right now is: -1660.6029628759618
min fval:  -1660.6980787238454
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.4421,  0.6998],
        [-1.4421,  0.6998],
        [-6.3492, 12.4015],
        [-1.4421,  0.6998],
        [-1.4421,  0.6998],
        [ 4.8942,  4.5255],
        [18.5544, -3.4131],
        [-1.4421,  0.6998],
        [27.0829, 12.6198],
        [-1.4421,  0.6998]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.5873,  -3.5873,   9.4148,  -3.5873,  -3.5873, -10.2517, -12.3317,
         -3.5873,  12.0448,  -3.5873], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [  0.0268,   0.0268, -14.7872,   0.0268,   0.0268,   7.6196,  15.0122,
           0.0268,  -9.6735,   0.0268],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.1968,  -0.1968,  13.3947,  -0.1968,  -0.1968,  -5.6681, -12.5020,
          -0.1968,   4.5406,  -0.1968],
        [ -0.0833,  -0.0833,  14.5832,  -0.0833,  -0.0833,  -7.0292, -14.4098,
          -0.0833,   5.3216,  -0.0833],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.4053, -1.4053, 11.8715, -1.4053, -1.4053, -1.4053, -6.2928, -7.3809,
        -1.4053, -1.4053], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.7480e-02, -1.7480e-02, -2.1852e+01, -1.7480e-02, -1.7480e-02,
         -1.7480e-02,  6.0917e+00,  9.9946e+00, -1.7480e-02, -1.7480e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.3457,   0.1683],
        [ -2.3556,   0.1658],
        [-13.6623, -18.1119],
        [-14.1734,   0.8251],
        [-15.6911,  -7.6266],
        [-23.3223,  -2.0399],
        [ -5.1833,  -3.0089],
        [ -2.3570,   0.1645],
        [-19.1378,  -2.7418],
        [ 22.9187,  10.0907]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.1298,  -5.1236, -15.6308,   2.0844,  -4.3454,  15.5897,  -9.9291,
         -5.1242,   5.7734,   9.9800], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.2296e-03, -1.2382e-02,  7.4049e+00, -7.7891e-01,  2.7601e+00,
          5.1472e+00,  9.9429e-02, -1.0842e-02, -1.7745e+00, -7.6753e+00],
        [-4.3285e-02, -2.3705e-02,  1.5169e+00,  1.5922e+00,  7.3540e+00,
         -7.8423e+00,  1.9123e+00, -3.7753e-02,  1.2147e+00, -1.3745e+00],
        [-6.1686e-02, -3.4079e-02,  3.3667e+00,  5.4968e+00, -6.8225e+00,
          7.4478e+00,  3.8429e-01, -4.6123e-02,  1.5284e+00, -3.6440e+00],
        [ 5.5519e-02, -2.7000e-02, -3.2171e-01,  3.8403e-02,  8.1400e-01,
         -3.0495e+00, -6.8656e-02,  1.2582e-02, -3.6820e+00, -2.7169e+00],
        [ 2.2316e-01,  1.9804e-01, -3.9594e+00,  8.6284e+00,  4.9462e+00,
         -1.7010e+00,  3.4354e-02,  2.1236e-01, -1.7169e+00, -3.8754e+00],
        [-1.1419e-01, -1.2562e-01,  7.4684e+00, -1.5122e+01,  8.4482e+00,
          2.8189e+00,  1.4867e+00, -1.2448e-01,  2.0376e+01, -2.6961e+01],
        [-2.8001e-01, -3.6862e-01,  1.3206e+01, -1.1624e+01,  6.8216e+00,
          1.6269e+01,  2.7123e-01, -3.3274e-01,  6.6447e+00, -1.5781e+01],
        [ 1.9747e-02,  1.9323e-02,  1.6043e-02,  1.7572e-02, -2.7489e-01,
         -1.5159e+00, -1.9185e-03,  1.9510e-02, -3.5549e-01, -4.1801e+00],
        [-1.0960e-01, -1.2240e-01,  1.3278e+00, -4.5093e+00, -2.5747e+01,
         -1.3892e+00, -2.8935e-01, -1.2066e-01, -1.0021e+01, -6.9452e-01],
        [ 1.4152e-01,  1.3157e-01, -3.0338e+01,  1.5325e+01,  2.7307e+00,
          7.9736e-01, -3.4945e-01,  1.3333e-01,  1.3387e+00,  2.4970e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -9.6930,  -5.0978,  -7.9940,  -2.7008,  -3.5311, -13.0242, -17.0067,
         -4.2502,  -2.9973,  -0.7233], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -1.8994,   4.0386,  -1.8969,   4.5199,   2.3008,  15.5971,  -6.4718,
           0.7752,   5.3116,   0.6160],
        [  1.7275,  -4.0386,   1.8970,  -4.5199,  -2.3008, -15.5912,   6.4825,
          -0.7752,  -5.3114,  -0.7441]], device='cuda:0'))])
xi:  [199.69646]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 797.0668332160355
W_T_median: 446.2452822299522
W_T_pctile_5: 200.2082279075583
W_T_CVAR_5_pct: 45.69367886627242
Average q (qsum/M+1):  49.12348790322581
Optimal xi:  [199.69646]
Observed VAR:  446.2452822299522
Expected(across Rb) median(across samples) p_equity:  0.3089634006222089
obj fun:  tensor(-1660.6981, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 3.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.4421,  0.6998],
        [-1.4421,  0.6998],
        [-6.3492, 12.4015],
        [-1.4421,  0.6998],
        [-1.4421,  0.6998],
        [ 4.8942,  4.5255],
        [18.5544, -3.4131],
        [-1.4421,  0.6998],
        [27.0829, 12.6198],
        [-1.4421,  0.6998]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -3.5873,  -3.5873,   9.4148,  -3.5873,  -3.5873, -10.2517, -12.3317,
         -3.5873,  12.0448,  -3.5873], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [  0.0268,   0.0268, -14.7872,   0.0268,   0.0268,   7.6196,  15.0122,
           0.0268,  -9.6735,   0.0268],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.1968,  -0.1968,  13.3947,  -0.1968,  -0.1968,  -5.6681, -12.5020,
          -0.1968,   4.5406,  -0.1968],
        [ -0.0833,  -0.0833,  14.5832,  -0.0833,  -0.0833,  -7.0292, -14.4098,
          -0.0833,   5.3216,  -0.0833],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164],
        [ -0.0164,  -0.0164,  -0.5062,  -0.0164,  -0.0164,  -0.0723,  -0.1750,
          -0.0164,  -1.2667,  -0.0164]], device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-1.4053, -1.4053, 11.8715, -1.4053, -1.4053, -1.4053, -6.2928, -7.3809,
        -1.4053, -1.4053], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.7480e-02, -1.7480e-02, -2.1852e+01, -1.7480e-02, -1.7480e-02,
         -1.7480e-02,  6.0917e+00,  9.9946e+00, -1.7480e-02, -1.7480e-02]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.3457,   0.1683],
        [ -2.3556,   0.1658],
        [-13.6623, -18.1119],
        [-14.1734,   0.8251],
        [-15.6911,  -7.6266],
        [-23.3223,  -2.0399],
        [ -5.1833,  -3.0089],
        [ -2.3570,   0.1645],
        [-19.1378,  -2.7418],
        [ 22.9187,  10.0907]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -5.1298,  -5.1236, -15.6308,   2.0844,  -4.3454,  15.5897,  -9.9291,
         -5.1242,   5.7734,   9.9800], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-8.2296e-03, -1.2382e-02,  7.4049e+00, -7.7891e-01,  2.7601e+00,
          5.1472e+00,  9.9429e-02, -1.0842e-02, -1.7745e+00, -7.6753e+00],
        [-4.3285e-02, -2.3705e-02,  1.5169e+00,  1.5922e+00,  7.3540e+00,
         -7.8423e+00,  1.9123e+00, -3.7753e-02,  1.2147e+00, -1.3745e+00],
        [-6.1686e-02, -3.4079e-02,  3.3667e+00,  5.4968e+00, -6.8225e+00,
          7.4478e+00,  3.8429e-01, -4.6123e-02,  1.5284e+00, -3.6440e+00],
        [ 5.5519e-02, -2.7000e-02, -3.2171e-01,  3.8403e-02,  8.1400e-01,
         -3.0495e+00, -6.8656e-02,  1.2582e-02, -3.6820e+00, -2.7169e+00],
        [ 2.2316e-01,  1.9804e-01, -3.9594e+00,  8.6284e+00,  4.9462e+00,
         -1.7010e+00,  3.4354e-02,  2.1236e-01, -1.7169e+00, -3.8754e+00],
        [-1.1419e-01, -1.2562e-01,  7.4684e+00, -1.5122e+01,  8.4482e+00,
          2.8189e+00,  1.4867e+00, -1.2448e-01,  2.0376e+01, -2.6961e+01],
        [-2.8001e-01, -3.6862e-01,  1.3206e+01, -1.1624e+01,  6.8216e+00,
          1.6269e+01,  2.7123e-01, -3.3274e-01,  6.6447e+00, -1.5781e+01],
        [ 1.9747e-02,  1.9323e-02,  1.6043e-02,  1.7572e-02, -2.7489e-01,
         -1.5159e+00, -1.9185e-03,  1.9510e-02, -3.5549e-01, -4.1801e+00],
        [-1.0960e-01, -1.2240e-01,  1.3278e+00, -4.5093e+00, -2.5747e+01,
         -1.3892e+00, -2.8935e-01, -1.2066e-01, -1.0021e+01, -6.9452e-01],
        [ 1.4152e-01,  1.3157e-01, -3.0338e+01,  1.5325e+01,  2.7307e+00,
          7.9736e-01, -3.4945e-01,  1.3333e-01,  1.3387e+00,  2.4970e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -9.6930,  -5.0978,  -7.9940,  -2.7008,  -3.5311, -13.0242, -17.0067,
         -4.2502,  -2.9973,  -0.7233], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -1.8994,   4.0386,  -1.8969,   4.5199,   2.3008,  15.5971,  -6.4718,
           0.7752,   5.3116,   0.6160],
        [  1.7275,  -4.0386,   1.8970,  -4.5199,  -2.3008, -15.5912,   6.4825,
          -0.7752,  -5.3114,  -0.7441]], device='cuda:0'))])
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1506.8368317040647
W_T_median: 1144.7259758050877
W_T_pctile_5: -92.78690539124324
W_T_CVAR_5_pct: -256.52592661793926
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -1663.7804563794975
Current xi:  [120.30755]
objective value function right now is: -1663.7804563794975
4.0% of gradient descent iterations done. Method = Adam
new min fval:  -1684.3092003213455
Current xi:  [139.3439]
objective value function right now is: -1684.3092003213455
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -1719.3260004816318
Current xi:  [155.62416]
objective value function right now is: -1719.3260004816318
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [170.59253]
objective value function right now is: -1711.234247394386
10.0% of gradient descent iterations done. Method = Adam
new min fval:  -1737.8765221932454
Current xi:  [183.27867]
objective value function right now is: -1737.8765221932454
12.0% of gradient descent iterations done. Method = Adam
Current xi:  [192.96437]
objective value function right now is: -1731.7850721477835
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -1746.8679845762565
Current xi:  [201.06758]
objective value function right now is: -1746.8679845762565
16.0% of gradient descent iterations done. Method = Adam
new min fval:  -1748.3224245756273
Current xi:  [206.90619]
objective value function right now is: -1748.3224245756273
18.0% of gradient descent iterations done. Method = Adam
Current xi:  [211.29404]
objective value function right now is: -1659.2917447261689
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [212.11964]
objective value function right now is: -1681.733308683564
22.0% of gradient descent iterations done. Method = Adam
Current xi:  [213.95515]
objective value function right now is: -1740.265547233192
24.0% of gradient descent iterations done. Method = Adam
Current xi:  [216.4156]
objective value function right now is: -1745.3144269636675
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [220.16785]
objective value function right now is: -1712.0225288199795
28.000000000000004% of gradient descent iterations done. Method = Adam
new min fval:  -1751.1071366161095
Current xi:  [219.10527]
objective value function right now is: -1751.1071366161095
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [217.45894]
objective value function right now is: -1730.6164159577397
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.42996]
objective value function right now is: -1425.364474980885
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.12056]
objective value function right now is: -1574.2754624427032
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.32166]
objective value function right now is: -1650.7450377971152
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [193.85466]
objective value function right now is: -1266.9518105270233
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [195.8068]
objective value function right now is: -1682.376251811805
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [198.91144]
objective value function right now is: -1434.3813051001493
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [200.35437]
objective value function right now is: -1693.438414500114
46.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.88638]
objective value function right now is: -1712.0098336337344
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [201.78479]
objective value function right now is: -1415.601473956062
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [199.87157]
objective value function right now is: -1483.8031662667888
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [196.82268]
objective value function right now is: -754.5501187385321
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [156.01447]
objective value function right now is: 1244.955139991675
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [126.696144]
objective value function right now is: 924.5777547216012
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [96.80414]
objective value function right now is: 1062.8796544880731
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [77.98369]
objective value function right now is: 112.5288784399767
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [55.584282]
objective value function right now is: 1322.7441417605612
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [19.839235]
objective value function right now is: 812.1398406649712
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.79956865]
objective value function right now is: -331.64042011303326
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [-9.58204]
objective value function right now is: -514.371324445284
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.992358]
objective value function right now is: -787.4013113899584
72.0% of gradient descent iterations done. Method = Adam
Current xi:  [-14.241014]
objective value function right now is: -822.2362254681367
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [-13.952054]
objective value function right now is: -879.4586877879286
76.0% of gradient descent iterations done. Method = Adam
Current xi:  [-12.959952]
objective value function right now is: -939.8915901134611
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [-10.398825]
objective value function right now is: -993.0765074358756
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.0650578]
objective value function right now is: -939.7171479153601
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [-7.2590766]
objective value function right now is: -988.0925286875333
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [-5.686851]
objective value function right now is: -1042.8102676877882
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [-1.8258575]
objective value function right now is: -1124.5502626969646
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00081654]
objective value function right now is: -1186.109179042038
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00558368]
objective value function right now is: -1174.272125760843
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00268235]
objective value function right now is: -1218.3814282854917
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.01091405]
objective value function right now is: -1143.8300318188349
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [-0.00157617]
objective value function right now is: -1147.1375004623824
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [0.05122178]
objective value function right now is: -1049.5439955274426
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [7.326668e-05]
objective value function right now is: -1048.831127620502
min fval:  -1210.1490744675389
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.9019,  0.4988],
        [-1.9019,  0.4988],
        [-8.1422, 12.4736],
        [-1.9019,  0.4988],
        [-1.9019,  0.4988],
        [ 5.6019,  5.0115],
        [19.6630, -3.6740],
        [-1.9019,  0.4988],
        [23.3244, 13.1194],
        [-1.9019,  0.4988]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -5.2356,  -5.2356,   9.2481,  -5.2356,  -5.2356, -11.5191, -12.8330,
         -5.2356,  12.0039,  -5.2356], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3313e-01,  1.3313e-01, -1.5371e+01,  1.3313e-01,  1.3313e-01,
          9.7604e+00,  1.5560e+01,  1.3313e-01, -9.6678e+00,  1.3313e-01],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 8.2326e-02,  8.2326e-02,  1.3864e+01,  8.2326e-02,  8.2326e-02,
         -6.7444e+00, -1.3487e+01,  8.2326e-02,  4.4834e+00,  8.2326e-02],
        [-4.3109e-02, -4.3109e-02,  1.5211e+01, -4.3109e-02, -4.3109e-02,
         -8.9699e+00, -1.5253e+01, -4.3109e-02,  5.2065e+00, -4.3109e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.3658, -2.3658, 11.9154, -2.3658, -2.3658, -2.3658, -5.9631, -7.2303,
        -2.3658, -2.3658], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.1782e-03, -1.1781e-03, -2.1808e+01, -1.1780e-03, -1.1780e-03,
         -1.1781e-03,  5.7429e+00,  1.0034e+01, -1.1781e-03, -1.1781e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.7719,  -0.5696],
        [ -2.8855,  -0.9812],
        [-12.2353, -18.2173],
        [-14.5578,   1.1447],
        [-14.0825,  -8.2473],
        [-24.7263,  -3.3739],
        [ -3.4310,   0.2902],
        [ -2.8191,  -0.9488],
        [-20.8330,  -3.1332],
        [ 22.4955,  10.5721]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.3110,  -5.8883, -15.8407,   2.0953,  -6.1630,  14.9391,  -6.2312,
         -5.9700,   5.3374,  10.8521], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.4941e-01, -1.1865e+00,  7.1373e+00, -4.1889e+00,  4.1603e+00,
          5.1771e+00, -1.3789e-02, -9.4881e-01, -4.8181e+00, -8.4303e+00],
        [ 3.9608e-01,  4.5232e-01,  6.1251e-01,  4.2718e+00,  4.2220e+00,
         -2.4626e+00,  3.3116e-01,  4.3654e-01,  2.8868e+00, -2.5238e+00],
        [ 5.3466e-01,  6.4230e-01,  4.2585e+00, -2.6127e+00, -1.9798e-01,
          7.1470e+00, -4.0727e-01,  6.0353e-01,  1.5804e+00, -3.6629e+00],
        [ 1.1310e-01, -1.6970e-01,  1.3849e+00, -1.5478e+00, -5.2431e-01,
         -8.5469e+00, -3.7096e-01, -1.8283e-01, -2.0932e+00, -3.1289e+00],
        [-1.1634e-02, -7.1284e-02, -7.8162e-01,  4.5978e+00,  3.9493e+00,
         -1.7412e+00,  3.3428e-03, -6.0435e-02, -1.1142e+00, -6.6281e+00],
        [-3.5520e+00, -5.1921e+00,  7.0374e+00, -1.6692e+01,  6.5776e+00,
          8.0984e-01, -1.2232e+00, -5.0146e+00,  2.7216e+01, -2.7095e+01],
        [-8.9406e-01, -6.6762e-01,  1.4005e+01, -8.5994e+00,  5.7129e+00,
          1.5630e+01, -3.9389e-01, -7.2259e-01,  7.9317e+00, -1.5731e+01],
        [ 2.0524e-03, -4.4305e-04, -6.1746e-01,  5.0932e-02, -2.6030e-01,
         -1.7705e+00,  8.6110e-03,  3.6191e-04, -4.6635e-01, -4.7315e+00],
        [ 4.4417e-01, -1.5968e-01,  4.3980e-01, -5.1847e+00, -2.1919e+01,
         -1.8255e+00, -7.5973e-01, -2.4026e-02, -2.1976e+01, -7.1638e-01],
        [-6.8731e-01, -1.5296e+00, -2.5830e+01,  1.3671e+01,  2.7949e+00,
          4.2890e-01,  8.4443e-01, -1.5023e+00,  1.0097e+00,  8.0914e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -9.5060,  -5.9330,  -8.3280,  -3.2013,  -5.4233, -14.7743, -17.0758,
         -4.8064,  -3.0177,  -0.6631], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -3.0095,   2.3753,  -1.8263,   5.0601,   1.0920,  16.1966,  -6.7467,
           0.5833,   5.9697,   0.6871],
        [  2.8518,  -2.3753,   1.8263,  -5.0601,  -1.0920, -16.1889,   6.7577,
          -0.5833,  -5.9696,  -0.8152]], device='cuda:0'))])
xi:  [-0.00558368]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 698.3988211015035
W_T_median: 478.6774210489033
W_T_pctile_5: 224.17784827662487
W_T_CVAR_5_pct: 50.36164878908348
Average q (qsum/M+1):  48.368963426159276
Optimal xi:  [-0.00558368]
Observed VAR:  478.6774210489033
Expected(across Rb) median(across samples) p_equity:  0.27797215680281323
obj fun:  tensor(-1210.1491, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 5.0
-----------------------------------------------
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        1              none   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 1)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
  obj.layers[layer_id]  layer_id   description  n_nodes        activation  \
0        obj.layers[0]         0   input_layer        2              None   
0        obj.layers[1]         1  hidden_layer       10  logistic_sigmoid   
0        obj.layers[2]         2  hidden_layer       10  logistic_sigmoid   
0        obj.layers[3]         3  output_layer        2           softmax   

  x_l(weights)  add_bias b_l(biases)  
0         None     False        None  
0      (2, 10)      True          10  
0     (10, 10)      True          10  
0      (10, 2)     False        None  
Pytorch NN pbject created from original NN class. Change            original NN object to change structure.
loaded continuation NN:  OrderedDict([('0.model.hidden_layer_1.weight', tensor([[-1.9019,  0.4988],
        [-1.9019,  0.4988],
        [-8.1422, 12.4736],
        [-1.9019,  0.4988],
        [-1.9019,  0.4988],
        [ 5.6019,  5.0115],
        [19.6630, -3.6740],
        [-1.9019,  0.4988],
        [23.3244, 13.1194],
        [-1.9019,  0.4988]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -5.2356,  -5.2356,   9.2481,  -5.2356,  -5.2356, -11.5191, -12.8330,
         -5.2356,  12.0039,  -5.2356], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3313e-01,  1.3313e-01, -1.5371e+01,  1.3313e-01,  1.3313e-01,
          9.7604e+00,  1.5560e+01,  1.3313e-01, -9.6678e+00,  1.3313e-01],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 8.2326e-02,  8.2326e-02,  1.3864e+01,  8.2326e-02,  8.2326e-02,
         -6.7444e+00, -1.3487e+01,  8.2326e-02,  4.4834e+00,  8.2326e-02],
        [-4.3109e-02, -4.3109e-02,  1.5211e+01, -4.3109e-02, -4.3109e-02,
         -8.9699e+00, -1.5253e+01, -4.3109e-02,  5.2065e+00, -4.3109e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02],
        [ 1.3275e-02,  1.3275e-02, -5.1937e-01,  1.3275e-02,  1.3275e-02,
          3.6032e-02,  5.0111e-02,  1.3275e-02, -2.0882e+00,  1.3275e-02]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.3658, -2.3658, 11.9154, -2.3658, -2.3658, -2.3658, -5.9631, -7.2303,
        -2.3658, -2.3658], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[-1.1782e-03, -1.1781e-03, -2.1808e+01, -1.1780e-03, -1.1780e-03,
         -1.1781e-03,  5.7429e+00,  1.0034e+01, -1.1781e-03, -1.1781e-03]],
       device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -2.7719,  -0.5696],
        [ -2.8855,  -0.9812],
        [-12.2353, -18.2173],
        [-14.5578,   1.1447],
        [-14.0825,  -8.2473],
        [-24.7263,  -3.3739],
        [ -3.4310,   0.2902],
        [ -2.8191,  -0.9488],
        [-20.8330,  -3.1332],
        [ 22.4955,  10.5721]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([ -6.3110,  -5.8883, -15.8407,   2.0953,  -6.1630,  14.9391,  -6.2312,
         -5.9700,   5.3374,  10.8521], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-1.4941e-01, -1.1865e+00,  7.1373e+00, -4.1889e+00,  4.1603e+00,
          5.1771e+00, -1.3789e-02, -9.4881e-01, -4.8181e+00, -8.4303e+00],
        [ 3.9608e-01,  4.5232e-01,  6.1251e-01,  4.2718e+00,  4.2220e+00,
         -2.4626e+00,  3.3116e-01,  4.3654e-01,  2.8868e+00, -2.5238e+00],
        [ 5.3466e-01,  6.4230e-01,  4.2585e+00, -2.6127e+00, -1.9798e-01,
          7.1470e+00, -4.0727e-01,  6.0353e-01,  1.5804e+00, -3.6629e+00],
        [ 1.1310e-01, -1.6970e-01,  1.3849e+00, -1.5478e+00, -5.2431e-01,
         -8.5469e+00, -3.7096e-01, -1.8283e-01, -2.0932e+00, -3.1289e+00],
        [-1.1634e-02, -7.1284e-02, -7.8162e-01,  4.5978e+00,  3.9493e+00,
         -1.7412e+00,  3.3428e-03, -6.0435e-02, -1.1142e+00, -6.6281e+00],
        [-3.5520e+00, -5.1921e+00,  7.0374e+00, -1.6692e+01,  6.5776e+00,
          8.0984e-01, -1.2232e+00, -5.0146e+00,  2.7216e+01, -2.7095e+01],
        [-8.9406e-01, -6.6762e-01,  1.4005e+01, -8.5994e+00,  5.7129e+00,
          1.5630e+01, -3.9389e-01, -7.2259e-01,  7.9317e+00, -1.5731e+01],
        [ 2.0524e-03, -4.4305e-04, -6.1746e-01,  5.0932e-02, -2.6030e-01,
         -1.7705e+00,  8.6110e-03,  3.6191e-04, -4.6635e-01, -4.7315e+00],
        [ 4.4417e-01, -1.5968e-01,  4.3980e-01, -5.1847e+00, -2.1919e+01,
         -1.8255e+00, -7.5973e-01, -2.4026e-02, -2.1976e+01, -7.1638e-01],
        [-6.8731e-01, -1.5296e+00, -2.5830e+01,  1.3671e+01,  2.7949e+00,
          4.2890e-01,  8.4443e-01, -1.5023e+00,  1.0097e+00,  8.0914e-02]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -9.5060,  -5.9330,  -8.3280,  -3.2013,  -5.4233, -14.7743, -17.0758,
         -4.8064,  -3.0177,  -0.6631], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -3.0095,   2.3753,  -1.8263,   5.0601,   1.0920,  16.1966,  -6.7467,
           0.5833,   5.9697,   0.6871],
        [  2.8518,  -2.3753,   1.8263,  -5.0601,  -1.0920, -16.1889,   6.7577,
          -0.5833,  -5.9696,  -0.8152]], device='cuda:0'))])
-----------------------------------------------
Selected results: ConstProp_strategy on TRAINING dataset
constant withdrawal:  40.0
constant allocation:  [0.6 0.4]
W_T_mean: 1506.8368317040647
W_T_median: 1144.7259758050877
W_T_pctile_5: -92.78690539124324
W_T_CVAR_5_pct: -256.52592661793926
-----------------------------------------------
2.0% of gradient descent iterations done. Method = Adam
new min fval:  -2622.1337568782856
Current xi:  [120.36945]
objective value function right now is: -2622.1337568782856
4.0% of gradient descent iterations done. Method = Adam
Current xi:  [142.0698]
objective value function right now is: -1915.9248345221588
6.0% of gradient descent iterations done. Method = Adam
new min fval:  -3438.2407207736915
Current xi:  [161.27953]
objective value function right now is: -3438.2407207736915
8.0% of gradient descent iterations done. Method = Adam
Current xi:  [177.46252]
objective value function right now is: -3286.3449035105828
10.0% of gradient descent iterations done. Method = Adam
Current xi:  [182.99403]
objective value function right now is: 2259.8431502341805
12.0% of gradient descent iterations done. Method = Adam
new min fval:  -3847.7876774575934
Current xi:  [192.05214]
objective value function right now is: -3847.7876774575934
14.000000000000002% of gradient descent iterations done. Method = Adam
new min fval:  -4083.914652334436
Current xi:  [206.12236]
objective value function right now is: -4083.914652334436
16.0% of gradient descent iterations done. Method = Adam
Current xi:  [218.66296]
objective value function right now is: -4036.222869098645
18.0% of gradient descent iterations done. Method = Adam
new min fval:  -4108.348035435416
Current xi:  [226.41951]
objective value function right now is: -4108.348035435416
20.0% of gradient descent iterations done. Method = Adam
Current xi:  [232.46997]
objective value function right now is: -3969.823796213069
22.0% of gradient descent iterations done. Method = Adam
new min fval:  -4292.550728846826
Current xi:  [236.55827]
objective value function right now is: -4292.550728846826
24.0% of gradient descent iterations done. Method = Adam
new min fval:  -4298.860823307511
Current xi:  [239.6397]
objective value function right now is: -4298.860823307511
26.0% of gradient descent iterations done. Method = Adam
Current xi:  [242.74109]
objective value function right now is: -4232.691413843022
28.000000000000004% of gradient descent iterations done. Method = Adam
Current xi:  [241.89227]
objective value function right now is: -4269.089660559498
30.0% of gradient descent iterations done. Method = Adam
Current xi:  [243.76593]
objective value function right now is: -4246.725640473696
32.0% of gradient descent iterations done. Method = Adam
Current xi:  [240.91685]
objective value function right now is: -4169.195671100913
34.0% of gradient descent iterations done. Method = Adam
Current xi:  [242.495]
objective value function right now is: -4204.575887294468
36.0% of gradient descent iterations done. Method = Adam
Current xi:  [243.71767]
objective value function right now is: -4268.684071014477
38.0% of gradient descent iterations done. Method = Adam
Current xi:  [244.89474]
objective value function right now is: -4270.415829508244
40.0% of gradient descent iterations done. Method = Adam
Current xi:  [246.47397]
objective value function right now is: -3016.248778365907
42.0% of gradient descent iterations done. Method = Adam
Current xi:  [244.99536]
objective value function right now is: -4202.606183460504
44.0% of gradient descent iterations done. Method = Adam
Current xi:  [245.28229]
objective value function right now is: -4275.307350928369
46.0% of gradient descent iterations done. Method = Adam
new min fval:  -4305.918777943764
Current xi:  [245.41191]
objective value function right now is: -4305.918777943764
48.0% of gradient descent iterations done. Method = Adam
Current xi:  [245.30145]
objective value function right now is: -4138.231643894758
50.0% of gradient descent iterations done. Method = Adam
Current xi:  [245.98167]
objective value function right now is: -4087.98024108155
52.0% of gradient descent iterations done. Method = Adam
Current xi:  [245.72691]
objective value function right now is: -4259.234700158933
54.0% of gradient descent iterations done. Method = Adam
Current xi:  [246.33936]
objective value function right now is: -4204.948207833266
56.00000000000001% of gradient descent iterations done. Method = Adam
Current xi:  [246.62907]
objective value function right now is: -4162.952874220534
57.99999999999999% of gradient descent iterations done. Method = Adam
Current xi:  [246.56496]
objective value function right now is: -4264.40794433274
60.0% of gradient descent iterations done. Method = Adam
Current xi:  [244.97324]
objective value function right now is: -4000.0759253607243
62.0% of gradient descent iterations done. Method = Adam
Current xi:  [242.97398]
objective value function right now is: -3556.9330899600714
64.0% of gradient descent iterations done. Method = Adam
Current xi:  [244.34708]
objective value function right now is: -4118.605734477831
66.0% of gradient descent iterations done. Method = Adam
Current xi:  [241.99876]
objective value function right now is: -4274.712682023651
68.0% of gradient descent iterations done. Method = Adam
Current xi:  [242.38173]
objective value function right now is: -4264.403081754172
70.0% of gradient descent iterations done. Method = Adam
Current xi:  [243.45073]
objective value function right now is: -4237.275914713528
72.0% of gradient descent iterations done. Method = Adam
new min fval:  -4363.096464417802
Current xi:  [244.37187]
objective value function right now is: -4363.096464417802
74.0% of gradient descent iterations done. Method = Adam
Current xi:  [245.11534]
objective value function right now is: -4358.879869302551
76.0% of gradient descent iterations done. Method = Adam
new min fval:  -4385.169321231736
Current xi:  [245.64417]
objective value function right now is: -4385.169321231736
78.0% of gradient descent iterations done. Method = Adam
Current xi:  [246.19203]
objective value function right now is: -4380.6266347129385
80.0% of gradient descent iterations done. Method = Adam
Current xi:  [246.448]
objective value function right now is: -4369.413581122479
82.0% of gradient descent iterations done. Method = Adam
Current xi:  [246.98239]
objective value function right now is: -4337.319494832404
84.0% of gradient descent iterations done. Method = Adam
Current xi:  [247.35986]
objective value function right now is: -4375.080161691386
86.0% of gradient descent iterations done. Method = Adam
Current xi:  [247.06438]
objective value function right now is: -4338.898297737463
88.0% of gradient descent iterations done. Method = Adam
Current xi:  [247.2193]
objective value function right now is: -4373.794730144564
new min fval from sgd:  -4423.446124737338
90.0% of gradient descent iterations done. Method = Adam
Current xi:  [247.50409]
objective value function right now is: -4423.446124737338
new min fval from sgd:  -4425.943940570471
new min fval from sgd:  -4427.6342242744195
92.0% of gradient descent iterations done. Method = Adam
Current xi:  [247.86346]
objective value function right now is: -4403.011299272759
94.0% of gradient descent iterations done. Method = Adam
Current xi:  [247.73705]
objective value function right now is: -4413.418305688686
96.0% of gradient descent iterations done. Method = Adam
Current xi:  [247.81937]
objective value function right now is: -4373.53154064162
98.0% of gradient descent iterations done. Method = Adam
Current xi:  [248.26363]
objective value function right now is: -4406.648241827024
100.0% of gradient descent iterations done. Method = Adam
Current xi:  [248.3695]
objective value function right now is: -4415.049645119397
min fval:  -4427.6342242744195
saved model: 
OrderedDict([('0.model.hidden_layer_1.weight', tensor([[ -0.3717,   0.6695],
        [ -0.3716,   0.6698],
        [-13.0855,  12.5922],
        [ -0.3717,   0.6693],
        [ -0.3716,   0.6697],
        [  5.9153,   4.0793],
        [ 23.7400,  -3.6224],
        [ -0.3717,   0.6695],
        [ 10.1120,  15.2336],
        [ -0.3717,   0.6692]], device='cuda:0')), ('0.model.hidden_layer_1.bias', tensor([ -5.0505,  -5.0505,   9.7641,  -5.0505,  -5.0505, -15.2228, -13.3805,
         -5.0505,  12.2042,  -5.0505], device='cuda:0')), ('0.model.hidden_layer_2.weight', tensor([[-5.8485e-03, -5.8551e-03, -7.6404e-01, -5.8444e-03, -5.8528e-03,
         -1.4220e-03, -1.1432e-01, -5.8490e-03, -1.5264e+00, -5.8435e-03],
        [-5.8485e-03, -5.8551e-03, -7.6404e-01, -5.8444e-03, -5.8528e-03,
         -1.4219e-03, -1.1432e-01, -5.8490e-03, -1.5264e+00, -5.8434e-03],
        [ 3.2859e-02,  3.2874e-02, -1.7032e+01,  3.2850e-02,  3.2868e-02,
          1.2754e+01,  1.8633e+01,  3.2859e-02, -9.4154e+00,  3.2848e-02],
        [-5.8486e-03, -5.8552e-03, -7.6404e-01, -5.8444e-03, -5.8529e-03,
         -1.4219e-03, -1.1432e-01, -5.8490e-03, -1.5264e+00, -5.8435e-03],
        [-5.8485e-03, -5.8551e-03, -7.6404e-01, -5.8443e-03, -5.8528e-03,
         -1.4220e-03, -1.1432e-01, -5.8489e-03, -1.5264e+00, -5.8434e-03],
        [-5.8485e-03, -5.8551e-03, -7.6404e-01, -5.8444e-03, -5.8528e-03,
         -1.4219e-03, -1.1432e-01, -5.8490e-03, -1.5264e+00, -5.8435e-03],
        [ 3.6141e-01,  3.6144e-01,  1.5413e+01,  3.6139e-01,  3.6143e-01,
         -9.7311e-01, -1.5143e+01,  3.6141e-01,  3.7923e+00,  3.6138e-01],
        [ 5.6909e-02,  5.6910e-02,  1.6920e+01,  5.6907e-02,  5.6911e-02,
         -1.2295e+01, -1.7740e+01,  5.6910e-02,  4.4796e+00,  5.6907e-02],
        [-5.8484e-03, -5.8551e-03, -7.6404e-01, -5.8443e-03, -5.8528e-03,
         -1.4219e-03, -1.1432e-01, -5.8489e-03, -1.5264e+00, -5.8433e-03],
        [-5.8484e-03, -5.8550e-03, -7.6404e-01, -5.8443e-03, -5.8527e-03,
         -1.4219e-03, -1.1432e-01, -5.8489e-03, -1.5264e+00, -5.8433e-03]],
       device='cuda:0')), ('0.model.hidden_layer_2.bias', tensor([-2.2693, -2.2693, 12.4049, -2.2693, -2.2693, -2.2693, -6.7425, -8.2173,
        -2.2693, -2.2693], device='cuda:0')), ('0.model.output_layer_3.weight', tensor([[ -0.1030,  -0.1030, -20.5494,  -0.1030,  -0.1030,  -0.1030,   2.5710,
           9.2263,  -0.1030,  -0.1030]], device='cuda:0')), ('1.model.hidden_layer_1.weight', tensor([[ -4.3861,   5.4599],
        [ -4.2397,  -0.0905],
        [ -4.4164,   0.2492],
        [ -2.9491,   7.3178],
        [-13.3487,  -8.8484],
        [-25.0765,  -1.7542],
        [-17.2729,   1.0833],
        [-27.6241,   1.9478],
        [-20.7803,  -4.1706],
        [ 25.9706,  10.3564]], device='cuda:0')), ('1.model.hidden_layer_1.bias', tensor([-8.0550, -5.5325, -5.9800,  6.6908, -5.8258, 18.0276, -1.6679,  2.1162,
         9.0046,  8.0369], device='cuda:0')), ('1.model.hidden_layer_2.weight', tensor([[-2.3907e-01, -3.8374e-03, -6.7160e-03, -8.6798e-01, -2.9549e-01,
         -4.5666e-01, -8.2811e-05, -1.3701e-02, -1.9292e-01, -4.2104e+00],
        [ 1.0908e-01,  1.0019e+00,  8.6860e-01,  7.9105e-01,  3.0713e+00,
         -7.6095e-01,  2.2625e+00,  3.1546e+00,  2.0174e+00, -3.6993e+00],
        [ 1.3899e-01,  1.1567e+00,  8.7849e-01, -5.9272e+00,  4.7075e+00,
          5.2993e+00,  3.1115e-01, -4.7893e+00,  1.6887e+00, -2.9310e+00],
        [ 7.6994e-03,  6.6736e-01,  2.2744e-01, -1.7566e+01,  1.8967e+00,
         -1.1864e+01,  3.1853e-03, -5.2087e-04,  1.3813e+00, -2.2098e+00],
        [-2.7124e+00,  2.6005e+00,  3.1840e+00,  1.3198e+01, -1.8426e+00,
          2.2916e+00,  6.2049e+00,  7.3561e+00,  5.2305e+00, -1.0786e+01],
        [-9.9072e-03, -3.2262e+00, -2.0612e+00, -5.9365e+01,  7.4415e+00,
          1.9468e+00,  1.7966e+00, -3.3964e+00,  2.5606e+01, -2.8539e+01],
        [-3.9568e-03,  5.4525e-02,  1.9926e-01, -1.6575e+01,  9.2899e+00,
          1.6442e+01, -4.5718e+00, -7.6150e+00,  9.2888e+00, -1.4181e+01],
        [-9.6519e-01, -1.3308e-01, -1.2591e-01, -3.4696e+00,  1.5107e+00,
          2.9861e+00, -3.3982e+00, -6.5947e+00,  1.6287e+00, -3.8129e+00],
        [-5.1323e-03,  1.7279e-01, -2.3867e-01, -2.1756e+01,  4.6246e+00,
         -1.3993e+01,  5.5183e-03, -5.8642e-05, -1.5793e+01, -6.3953e-01],
        [ 1.4124e-02, -4.2348e-01,  5.5579e-01,  2.3239e+01, -2.0920e+01,
         -7.1915e-01,  7.2353e+00,  2.0006e+01, -8.4537e-01,  6.5294e-01]],
       device='cuda:0')), ('1.model.hidden_layer_2.bias', tensor([ -6.3065,  -5.4810,  -8.4394,  -2.2189,  -2.9406, -16.0554, -15.9864,
         -3.8750,  -2.9321,  -0.1042], device='cuda:0')), ('1.model.output_layer_3.weight', tensor([[ -0.0503,   1.2914,  -4.0073,   6.8973,   0.2758,  19.5233,  -7.0728,
          -2.3013,   7.2728,   0.3457],
        [  0.0342,  -1.2913,   4.0074,  -6.8973,  -0.2753, -19.5148,   7.0846,
           2.3013,  -7.2726,  -0.4738]], device='cuda:0'))])
xi:  [247.51015]
-----------------------------------------------
Selected results: NN-strategy-on-TRAINING dataset (temp implementation
W_T_mean: 734.9379170261943
W_T_median: 549.9076105539781
W_T_pctile_5: 249.49869785371112
W_T_CVAR_5_pct: 60.22195247742417
Average q (qsum/M+1):  45.70418031754032
Optimal xi:  [247.51015]
Observed VAR:  549.9076105539781
Expected(across Rb) median(across samples) p_equity:  0.22945809736847878
obj fun:  tensor(-4427.6342, device='cuda:0', dtype=torch.float64)
-----------------------------------------------
Need to implement this
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_FunctionHeatmaps.py:262: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
bond graph is mislabeled!
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_Plot_NN_control_DataHeatmaps.py:161: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format=save_Figures_format, bbox_inches="tight")
/home/marcchen/Documents/testing_pyt_decum/researchcode/fun_output_results_Pctiles.py:176: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.savefig(fig_filename, format = save_Figures_format, bbox_inches = "tight")
-----------------------------------------------
Just FINISHED: 
Asset basket ID: B10_and_VWD
Objective function: mean_cvar_single_level
Tracing param: 50.0
-----------------------------------------------
